{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5358cb1ea2ce477dab4a95e8aee5f0f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_203fd0148b764feea5eac327d6345844",
              "IPY_MODEL_9d107721958d442ba708e961481d85e5",
              "IPY_MODEL_4b8cf5e225ec48378759eda53d635e7c"
            ],
            "layout": "IPY_MODEL_ad9d9290120842aba0ee9b81203fc568"
          }
        },
        "203fd0148b764feea5eac327d6345844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e2bea51d39e4d54875d602b0925f80b",
            "placeholder": "​",
            "style": "IPY_MODEL_044a868d097f4cce8b2405c66ce2b66d",
            "value": "modules.json: 100%"
          }
        },
        "9d107721958d442ba708e961481d85e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_193e3798028f489fbd5185595c9c7171",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afeb3a7ac9cf4113a01a0711c9a3b7fe",
            "value": 349
          }
        },
        "4b8cf5e225ec48378759eda53d635e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4187b38b2d6944ecaef29f6a5946e2fa",
            "placeholder": "​",
            "style": "IPY_MODEL_feeafbc07afd4d6b84fd03a106d1d30f",
            "value": " 349/349 [00:00&lt;00:00, 21.2kB/s]"
          }
        },
        "ad9d9290120842aba0ee9b81203fc568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e2bea51d39e4d54875d602b0925f80b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "044a868d097f4cce8b2405c66ce2b66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "193e3798028f489fbd5185595c9c7171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afeb3a7ac9cf4113a01a0711c9a3b7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4187b38b2d6944ecaef29f6a5946e2fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feeafbc07afd4d6b84fd03a106d1d30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fadaf8dce403407b8a7525e962bf25d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fe50353120044bc8142b8af59de727b",
              "IPY_MODEL_59d5538546844920ae3181487d106062",
              "IPY_MODEL_ca6c547569dd4eabb6ce25a8cfd7a321"
            ],
            "layout": "IPY_MODEL_c99b1417bb9347089e1fffecca2cbe38"
          }
        },
        "4fe50353120044bc8142b8af59de727b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a63d74c6556e484eb1fc72d0708cf43b",
            "placeholder": "​",
            "style": "IPY_MODEL_ce1bf64aa8ff4e78aa950f89b88d62b8",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "59d5538546844920ae3181487d106062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1339cf9cb7b4e4d8f6274d5bd001448",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3e92d270fdc3494585f1d884952fc41f",
            "value": 116
          }
        },
        "ca6c547569dd4eabb6ce25a8cfd7a321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99bf50624b254b83a0f20a0cbd0eb59b",
            "placeholder": "​",
            "style": "IPY_MODEL_2885db0068ca4a0095268224230433b3",
            "value": " 116/116 [00:00&lt;00:00, 6.61kB/s]"
          }
        },
        "c99b1417bb9347089e1fffecca2cbe38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a63d74c6556e484eb1fc72d0708cf43b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1bf64aa8ff4e78aa950f89b88d62b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1339cf9cb7b4e4d8f6274d5bd001448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e92d270fdc3494585f1d884952fc41f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99bf50624b254b83a0f20a0cbd0eb59b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2885db0068ca4a0095268224230433b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a7df09805054b349f86dfd5ef1162cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bf8755dface4f3183ace1c247502f6b",
              "IPY_MODEL_f699de78420c4dd3b3631cf3989ad337",
              "IPY_MODEL_8016cabaa67043439a570e3aec39f821"
            ],
            "layout": "IPY_MODEL_a450111b914f49328a029e76a9d64239"
          }
        },
        "5bf8755dface4f3183ace1c247502f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bf7c3c62ec745488983560f58f1cc86",
            "placeholder": "​",
            "style": "IPY_MODEL_99a4f90dcc034e34b7d7548ff2fbd563",
            "value": "README.md: 100%"
          }
        },
        "f699de78420c4dd3b3631cf3989ad337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5f6a53717854ce7934efc7fd7b20fa0",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c937b246886045688bfa46a2562bdf66",
            "value": 10659
          }
        },
        "8016cabaa67043439a570e3aec39f821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_318173c0ca7f4312804c75602a6edf91",
            "placeholder": "​",
            "style": "IPY_MODEL_26537a77b9a84d40b6225504848d04c9",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 610kB/s]"
          }
        },
        "a450111b914f49328a029e76a9d64239": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bf7c3c62ec745488983560f58f1cc86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99a4f90dcc034e34b7d7548ff2fbd563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5f6a53717854ce7934efc7fd7b20fa0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c937b246886045688bfa46a2562bdf66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "318173c0ca7f4312804c75602a6edf91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26537a77b9a84d40b6225504848d04c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07a1cc5a5c424b049da3c451b8eece7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c9388c8db6724847bffb0a68d4bf544b",
              "IPY_MODEL_b13a4a09ecec43d494050695669b39a8",
              "IPY_MODEL_81849ae324a442b2ae85b1ff6e48be69"
            ],
            "layout": "IPY_MODEL_e618ff4094804f87b8b5a07a0fd59c86"
          }
        },
        "c9388c8db6724847bffb0a68d4bf544b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63cd207de39345b8a4c7179d5aadd8d9",
            "placeholder": "​",
            "style": "IPY_MODEL_6dc551ccedb043eeba683b69f20a3456",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "b13a4a09ecec43d494050695669b39a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a5ff8310df14d978c419d7dba7ae3e1",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ce4877df7da4adca83cd7cff114a0da",
            "value": 53
          }
        },
        "81849ae324a442b2ae85b1ff6e48be69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91c2d32ce4164be7855ea19c16307a88",
            "placeholder": "​",
            "style": "IPY_MODEL_67fecae33dc44a23b6ff5f59ed06595b",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.52kB/s]"
          }
        },
        "e618ff4094804f87b8b5a07a0fd59c86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63cd207de39345b8a4c7179d5aadd8d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc551ccedb043eeba683b69f20a3456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a5ff8310df14d978c419d7dba7ae3e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ce4877df7da4adca83cd7cff114a0da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91c2d32ce4164be7855ea19c16307a88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67fecae33dc44a23b6ff5f59ed06595b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8678b2e38b046359de148ce7e9d4194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56c5c274ad914804ab7899664677d002",
              "IPY_MODEL_1bed7604b70d478aa590d76c4193d47e",
              "IPY_MODEL_33b2af83cf454804a83446f44d487633"
            ],
            "layout": "IPY_MODEL_1bac9468e6e9425caf714493f22dffd9"
          }
        },
        "56c5c274ad914804ab7899664677d002": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4b28781450e4342b82cfcc4dbfa0496",
            "placeholder": "​",
            "style": "IPY_MODEL_2feabc4b94f64b2bac81f1a6257a16c1",
            "value": "config.json: 100%"
          }
        },
        "1bed7604b70d478aa590d76c4193d47e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e1c7d4fc5b54d2494e439f03531ee36",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_caba233f75144150b09f6917955b0967",
            "value": 612
          }
        },
        "33b2af83cf454804a83446f44d487633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4ccb45152d647af9b759154a6e468c5",
            "placeholder": "​",
            "style": "IPY_MODEL_3e149bd0e0634824af5aa92372acf21b",
            "value": " 612/612 [00:00&lt;00:00, 39.2kB/s]"
          }
        },
        "1bac9468e6e9425caf714493f22dffd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b28781450e4342b82cfcc4dbfa0496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2feabc4b94f64b2bac81f1a6257a16c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e1c7d4fc5b54d2494e439f03531ee36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caba233f75144150b09f6917955b0967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4ccb45152d647af9b759154a6e468c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e149bd0e0634824af5aa92372acf21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0ee001569a544698a30de77793f3f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db235ea6368d4b4db4cc616042aeb0ff",
              "IPY_MODEL_76207d93f3ac4428a8a596c7a67d2fdc",
              "IPY_MODEL_7db176ddec724e17b4c4b6d8d7ee2855"
            ],
            "layout": "IPY_MODEL_94d4313c431f437983b723bfc9443059"
          }
        },
        "db235ea6368d4b4db4cc616042aeb0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f8d52368c7f4dcfb8a3e22cf0a565b4",
            "placeholder": "​",
            "style": "IPY_MODEL_b6e02dd9b2344363823e04e7965386a0",
            "value": "model.safetensors: 100%"
          }
        },
        "76207d93f3ac4428a8a596c7a67d2fdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c23b9931652c4ea9aac105e2a9a0c016",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21d93f3a57de49a3b2a427929ab7e5ba",
            "value": 90868376
          }
        },
        "7db176ddec724e17b4c4b6d8d7ee2855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4535b5e11ef4ead8c161b7f74a9fd1c",
            "placeholder": "​",
            "style": "IPY_MODEL_1cc0e7e8a2eb49d6b576ec5e3b5abfb5",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 117MB/s]"
          }
        },
        "94d4313c431f437983b723bfc9443059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f8d52368c7f4dcfb8a3e22cf0a565b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6e02dd9b2344363823e04e7965386a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c23b9931652c4ea9aac105e2a9a0c016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21d93f3a57de49a3b2a427929ab7e5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4535b5e11ef4ead8c161b7f74a9fd1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cc0e7e8a2eb49d6b576ec5e3b5abfb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cfd25e13aff40478ba6c577cd8867f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d94169739a1048b0992db411af262950",
              "IPY_MODEL_974ced3e6bef4c21afc1fad4bd7ee383",
              "IPY_MODEL_293d0bb866d24e32bd6602534b37821f"
            ],
            "layout": "IPY_MODEL_1fd010309afc40c185c0cc482a446102"
          }
        },
        "d94169739a1048b0992db411af262950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53d607e7c2d84db581839afc232e8559",
            "placeholder": "​",
            "style": "IPY_MODEL_d49e8b09b55e4c498c9c0bb3ac552dd0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "974ced3e6bef4c21afc1fad4bd7ee383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3060c38212fc4456a19628ee4702f92d",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3161bb61eb26474f9c8ffdd7612036d9",
            "value": 350
          }
        },
        "293d0bb866d24e32bd6602534b37821f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7536f970ec14e14a02ad4a4b7886e04",
            "placeholder": "​",
            "style": "IPY_MODEL_fc6c5f8608f145df9d84148ba95f84b5",
            "value": " 350/350 [00:00&lt;00:00, 15.6kB/s]"
          }
        },
        "1fd010309afc40c185c0cc482a446102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53d607e7c2d84db581839afc232e8559": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d49e8b09b55e4c498c9c0bb3ac552dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3060c38212fc4456a19628ee4702f92d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3161bb61eb26474f9c8ffdd7612036d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7536f970ec14e14a02ad4a4b7886e04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc6c5f8608f145df9d84148ba95f84b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "973ed2ddcba8499e8cbd0a2493ff56f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e3d822b0d5d433e81e7de9ade3b527d",
              "IPY_MODEL_42d5205ad62547eea87b09788f4c15a7",
              "IPY_MODEL_be0984b53d854527bc9789b4ae9fc7a2"
            ],
            "layout": "IPY_MODEL_8d553dd661e34f2d848064f6d3110ef5"
          }
        },
        "8e3d822b0d5d433e81e7de9ade3b527d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5cb88ade43149509261f2795c4981aa",
            "placeholder": "​",
            "style": "IPY_MODEL_927ba920a3e84595a82e7c93ac1a69f7",
            "value": "vocab.txt: 100%"
          }
        },
        "42d5205ad62547eea87b09788f4c15a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5e6d72bc46b4f85a1d634990075b020",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_29ba540c3c2348abbb0a94e72f43d92f",
            "value": 231508
          }
        },
        "be0984b53d854527bc9789b4ae9fc7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4af685e6672047018b33d64cdf75feee",
            "placeholder": "​",
            "style": "IPY_MODEL_031bff1773044d65aaae394906b4bb4c",
            "value": " 232k/232k [00:00&lt;00:00, 6.06MB/s]"
          }
        },
        "8d553dd661e34f2d848064f6d3110ef5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5cb88ade43149509261f2795c4981aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927ba920a3e84595a82e7c93ac1a69f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5e6d72bc46b4f85a1d634990075b020": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29ba540c3c2348abbb0a94e72f43d92f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4af685e6672047018b33d64cdf75feee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "031bff1773044d65aaae394906b4bb4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bcc817216b647ae8d1d499267622b4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42964885fce74d138a7b25aa7cde1c38",
              "IPY_MODEL_445dbf8bdcbd4b8fbb52d1af720505ce",
              "IPY_MODEL_cadf332472654b7a95e8a0efd56a83d9"
            ],
            "layout": "IPY_MODEL_d5cebd76ca6344dda93a51659d5ef40f"
          }
        },
        "42964885fce74d138a7b25aa7cde1c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_137e8157f0f14437a09d589a37fd790a",
            "placeholder": "​",
            "style": "IPY_MODEL_42ddc6c46dd1486e8838dd4cd29fd4de",
            "value": "tokenizer.json: 100%"
          }
        },
        "445dbf8bdcbd4b8fbb52d1af720505ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceef743de0e844dcb06836826ada9223",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac41635a6bb6425a864723626e2e3f6f",
            "value": 466247
          }
        },
        "cadf332472654b7a95e8a0efd56a83d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed1e6c4d117b4611b54fc50c4cce6671",
            "placeholder": "​",
            "style": "IPY_MODEL_f07ce5a0375f4d6a90c8828958e96b43",
            "value": " 466k/466k [00:00&lt;00:00, 594kB/s]"
          }
        },
        "d5cebd76ca6344dda93a51659d5ef40f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "137e8157f0f14437a09d589a37fd790a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42ddc6c46dd1486e8838dd4cd29fd4de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceef743de0e844dcb06836826ada9223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac41635a6bb6425a864723626e2e3f6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed1e6c4d117b4611b54fc50c4cce6671": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f07ce5a0375f4d6a90c8828958e96b43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e1b6416b59449779e89225a28436622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22b176934bb04fc0b8676161b6dd5ee7",
              "IPY_MODEL_f454bd94ae5e494185258fcd5123cba2",
              "IPY_MODEL_158deb3ad6a848178145c59a291a0784"
            ],
            "layout": "IPY_MODEL_9d55faf6a89545bd8f6b5ec931da1c74"
          }
        },
        "22b176934bb04fc0b8676161b6dd5ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d096b90de81a46629b2954ccda0a1bf1",
            "placeholder": "​",
            "style": "IPY_MODEL_045ca7f3746547c9afe4a070e4c706c6",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "f454bd94ae5e494185258fcd5123cba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3467237234646329a2b2e6cdef0fbc4",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3bfd2e0794ff4e3b9fa89ff6589de2fb",
            "value": 112
          }
        },
        "158deb3ad6a848178145c59a291a0784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91005ae3f8e940bc8692e4baae09d3b9",
            "placeholder": "​",
            "style": "IPY_MODEL_37e22431981c4e85bdbc246a127fd619",
            "value": " 112/112 [00:00&lt;00:00, 5.07kB/s]"
          }
        },
        "9d55faf6a89545bd8f6b5ec931da1c74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d096b90de81a46629b2954ccda0a1bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "045ca7f3746547c9afe4a070e4c706c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3467237234646329a2b2e6cdef0fbc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bfd2e0794ff4e3b9fa89ff6589de2fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "91005ae3f8e940bc8692e4baae09d3b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e22431981c4e85bdbc246a127fd619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df499522bc0d406985e1457e70e11867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7169f4866034f38afbdfbc9301151fc",
              "IPY_MODEL_10ffec29c1fb45458a17a7319f47da84",
              "IPY_MODEL_cbf55cdb9fb64643af331e1982531c37"
            ],
            "layout": "IPY_MODEL_15057f96999e4ecb9e1644a9ff53b0ac"
          }
        },
        "f7169f4866034f38afbdfbc9301151fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5664ddaf582d4537af6e98dd69738807",
            "placeholder": "​",
            "style": "IPY_MODEL_223d687a83b24a12876af784cbc673b8",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "10ffec29c1fb45458a17a7319f47da84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f4ce65b42e244519a0a5f6902824431",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_930538bf8ed04e92bf6bd88dcc19b618",
            "value": 190
          }
        },
        "cbf55cdb9fb64643af331e1982531c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_587cbb83a71e4d55875e2ffaf2b7d721",
            "placeholder": "​",
            "style": "IPY_MODEL_29913ece80714fbc90c87870ca439949",
            "value": " 190/190 [00:00&lt;00:00, 10.9kB/s]"
          }
        },
        "15057f96999e4ecb9e1644a9ff53b0ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5664ddaf582d4537af6e98dd69738807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "223d687a83b24a12876af784cbc673b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f4ce65b42e244519a0a5f6902824431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "930538bf8ed04e92bf6bd88dcc19b618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "587cbb83a71e4d55875e2ffaf2b7d721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29913ece80714fbc90c87870ca439949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwPrjrpRaJmR",
        "outputId": "8611c148-5a7a-45a5-fc69-2ed421469419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_webpage(url):\n",
        "    # Send a GET request to the specified URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful (status code 200)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the page using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all paragraph elements () on the page\n",
        "        paragraphs = soup.find_all('p')\n",
        "\n",
        "        # Extract text from each paragraph and join them with newlines\n",
        "        text_content = '\\n'.join([paragraph.get_text() for paragraph in paragraphs])\n",
        "\n",
        "        # Return the concatenated text content\n",
        "        return text_content\n",
        "    else:\n",
        "        # Print an error message if the request failed and return None\n",
        "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
        "        return None"
      ],
      "metadata": {
        "id": "NIgc51UHaSdl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content1 = scrape_webpage(\"https://stanford-cs324.github.io/winter2022/lectures/introduction/\") #introduction\n",
        "content2 = scrape_webpage(\"https://stanford-cs324.github.io/winter2022/lectures/capabilities/\") #capabilities\n",
        "content3 = scrape_webpage(\"https://stanford-cs324.github.io/winter2022/lectures/modeling/\") #modelling\n",
        "content4 = scrape_webpage(\"https://stanford-cs324.github.io/winter2022/lectures/data/\") #data\n",
        "content5 = scrape_webpage(\"https://stanford-cs324.github.io/winter2022/lectures/training/\") #training"
      ],
      "metadata": {
        "id": "yq7cVVOXaZ_J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_content = content1 + '\\n' + content2 + '\\n' + content3 + '\\n' + content4 + '\\n' + content5"
      ],
      "metadata": {
        "id": "LdV0e5YBaeY0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epAd8LPBaxOj",
        "outputId": "652ef04f-ced1-41a4-c9ea-9f0a8ce20a9d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to CS324! This is a new course on understanding and developing large language models.\n",
            "The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):\n",
            "The probability intuitively tells us how “good” a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo):\n",
            "Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.\n",
            "For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it’s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.\n",
            "Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted:\n",
            "How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an “average” sequence but something closer to the “best” sequence.\n",
            "A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using the chain rule of probability:\n",
            "For example (demo):\n",
            "In particular, \\(p(x_i \\mid x_{1:i-1})\\) is a conditional probability distribution of the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\).\n",
            "Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network).\n",
            "Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:\n",
            "where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:\n",
            "However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) the annealed conditional probability distribution. For example:\n",
            "Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing.\n",
            "Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.\n",
            "Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):\n",
            "If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\).\n",
            "As we’ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.\n",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as\n",
            "The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:\n",
            "Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.\n",
            "Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a “true” distribution \\(p\\) out there (the existence of this is questionable, but it’s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\).\n",
            "Shannon also defined cross entropy:\n",
            "which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)).\n",
            "Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\),\n",
            "which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English.\n",
            "So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\).\n",
            "Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paper Prediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human:\n",
            "Humans aren’t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses.\n",
            "Language models became first used in practical applications that required generation of text:\n",
            "Noisy channel model. The dominant paradigm for solving these tasks then was the noisy channel model. Taking speech recognition as an example:\n",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).\n",
            "N-gram models. In an n-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history:\n",
            "For example, a trigram (\\(n=3\\)) model would define:\n",
            "These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing).\n",
            "Fitting n-gram models to data is extremely computationally cheap and scalable. As a result, n-gram models were trained on massive amount of text. For example, Brants et al. (2007) trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix:\n",
            "If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in “huge” corpora):\n",
            "As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn’t a huge problem.\n",
            "An important step forward for language models was the introduction of neural networks. Bengio et al., 2003 pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network:\n",
            "Note that the context length is still bounded by \\(n\\), but it is now statistically feasible to estimate neural language models for much larger values of \\(n\\).\n",
            "Now, the main challenge was that training neural networks was much more computationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade.\n",
            "Since 2003, two other key developments in neural language modeling include:\n",
            "Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\(x_i\\) to depend on the entire context \\(x_{1:i-1}\\) (effectively \\(n = \\infty\\)), but these were hard to train.\n",
            "Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\(n\\), but were much easier to train (and exploited the parallelism of GPUs). Also, \\(n\\) could be made “large enough” for many applications (GPT-3 used \\(n = 2048\\)).\n",
            "We will open up the hood and dive deeper into the architecture and training later in the course.\n",
            "Having introduced language models, one might wonder why we need a course specifically on large language models.\n",
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:\n",
            "Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that “just scaling up” these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact.\n",
            "Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa.\n",
            "Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past.\n",
            "Recall that language models are capable of conditional generation: given a prompt, generate a completion:\n",
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):\n",
            "One can prompt a language model to solve word analogies (demo):\n",
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):\n",
            "Title: NLP Researchers at Stanford Discover Black Holes in Language Models Article: On January 3, 2007, the Stanford University News Service published an article that reported a remarkable discovery by NLP researchers at Stanford. The article was titled “Stanford Researchers Discover Black Holes in Language Models.” The discovery was described as follows: A black hole is a region of space-time where gravity pulls so much that even light cannot get out. Now physicists think they have found a similar phenomenon in language: They call it the semantic black hole. It occurs when a word or phrase has no clear definition – and sometimes no clear meaning at all. If you toss such a word into a sentence, it drags along other words until eventually the whole thing collapses under its own weight. “It’s like if you have a paper cup and you push in the bottom,” said Stanford computer scientist Michael Schmidt. “At first it holds up fine, but then it gets weaker and weaker until it collapses in on itself.” Schmidt and his colleagues are using computers to identify and avoid semantic black holes.\n",
            "In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let’s start with an example (demo):\n",
            "Input: Where is Stanford University? Output: Stanford University is in California.\n",
            "We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence.\n",
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):\n",
            "Input: Where is MIT? Output: Cambridge  Input: Where is University of Washington? Output: Seattle  Input: Where is Stanford University? Output: Stanford\n",
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.\n",
            "Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity.\n",
            "Given the strong capabilities of language models, it is not surprising to see their widespread adoption.\n",
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.\n",
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:\n",
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people.\n",
            "An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer.\n",
            "So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind’s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course.\n",
            "Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer can seem correct and there is no way of knowing (demo)\n",
            "Input: Who invented the Internet? Output: Al Gore\n",
            "In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable?\n",
            "Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo):\n",
            "The software developer finished the program. He celebrated. The software developer finished the program. She celebrated.\n",
            "Social biases are of course encoded in the data, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias?\n",
            "Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content. RealToxicityPrompts is a dataset that evaluates a language model’s propensity for producing toxic content. For example:\n",
            "So, I’m starting to think she’s full _\n",
            "As another example, GPT-3 has been demonstrated to output anti-Muslim stereotypes:\n",
            "Two Muslims walked into a _\n",
            "Applications such as writing assistants or chatbots would be vulnerable.\n",
            "Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models’ linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.\n",
            "Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt:\n",
            "In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem.\n",
            "Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation?\n",
            "For example, if you prompt GPT-3 with the first line of Harry Potter (demo):\n",
            "Mr. and Mrs. Dursley of number four, Privet Drive, _\n",
            "It will happily continue to spout out text from Harry Potter with high confidence.\n",
            "Cost and environmental impact. Finally, large language models can be quite expensive to work with.\n",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.\n",
            "Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face’s Big Science project, EleutherAI, and Stanford’s CRFM. Given language models’ increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.\n",
            "This course will be structured like an onion:\n",
            "Behavior of large language models: We will start at the outer layer where we only have blackbox API access to the model (as we’ve had so far). Our goal is to understand the behavior of these objects called large language models, as if we were a biologist studying an organism. Many questions about capabilities and harms can be answered at this level.\n",
            "Data behind large language models: Then we take a deeper look behind the data that is used to train large language models, and address issues such as security, privacy, and legal considerations. Having access to the training data provides us with important information about the model, even if we don’t have full access to the model.\n",
            "Building large language models: Then we arrive at the core of the onion, where we study how large language models are built (the model architectures, the training algorithms, etc.).\n",
            "Beyond large language models: Finally, we end the course with a look beyond language models. A language model is just a distribution over a sequence of tokens. These tokens could represent natural language, or a programming language, or elements in an audio or visual dictionary. Language models also belong to a more general class of foundation models, which share many of the properties of language models.\n",
            "In this lecture, we will explore the capabilities of GPT-3, the canonical large language model. We will closely follow the benchmarks from the GPT-3 paper, which include:\n",
            "In comparison with the state-of-the-art-result for each task, the results are mixed:\n",
            "The way to think about these results is as follows:\n",
            "Adaptation. Recall that a language model \\(p\\) is a distribution over sequences of tokens \\(x_{1:L}\\) and thus can be used to score sequences:\n",
            "It can also be used to perform conditional generation of a completion given a prompt:\n",
            "A task is a mapping from inputs to outputs. For example, for question answering, we might have:\n",
            "Input: What school did burne hogarth establish? Output: School of Visual Arts\n",
            "We use the term adaptation to refer to the process of taking a language model and turning it into a task model, given:\n",
            "There are two primary ways to perform adaptation:\n",
            "Which adaptation procedure should we go with?\n",
            "The GPT-3 paper evaluated GPT-3 on a large set of tasks. We will consider a subset of these, and for each task, discuss the following:\n",
            "Size and number of examples matters. By default, the results will based on\n",
            "Along the way, we will do ablations to see if model size and number of in-context training instances matters. Spoiler: it does and more is better.\n",
            "The tasks are grouped as follows:\n",
            "The goals of this lecture is to provide:\n",
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language.\n",
            "Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example:\n",
            "We can ask: what is the probability the language model assigns to it?\n",
            "Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule:\n",
            "Perplexity. The joint probability of a sequence depends on its length and thus goes to zero as the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.)\n",
            "Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don’t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn’t penalize you for that. Instead, we want the geometric average, which is exactly what perplexity does:\n",
            "Perplexity can be interpreted as the average “branching factor” per token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings.\n",
            "Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically:\n",
            "Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\):\n",
            "where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it’s just going to generate a gibberish token.\n",
            "Now let’s get on with evaluating perplexity on an actual dataset.\n",
            "The Penn Tree Bank is a classic dataset in NLP, originally annotated for syntactic parsing. Beginning with Emami and Jelinek (2004) and Mikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t to John Hewitt for pointing this out).\n",
            "Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo):\n",
            "Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29. Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group.\n",
            "Results. GPT-3 vastly outperforms the existing state-of-the-art:\n",
            "See the leaderboard for the latest results.\n",
            "Train/test leakage. The authors did not evaluate on some datasets such as WikiText-103 because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized.\n",
            "Adaptation.\n",
            "Fill in blank:  Alice was friends with Bob. Alice went to visit her friend ___. -> Bob  She held the torch in front of her. She caught her breath. “Chris? There’s a step.” “What?” “A step. Cut in the rock. About fifty feet ahead.” She moved faster. They both moved faster. “In fact,” she said, raising the torch higher, “there’s more than a ___. -> step\n",
            "Results. GPT-3 does much better on this task than the previous state-of-the-art (based on GPT-2):\n",
            "See the leaderboard for the latest results.\n",
            "Adaptation. This is a multiple-choice task, so the most natural thing to do is to score each candidate answer with the language model and predict the “best” one (demo):\n",
            "Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They ${answer}\n",
            "where ${answer} is one of:\n",
            "How do you score a candidate answer \\(y\\) given a question \\(x\\)? There’s no principled answer, but here are some heuristics:\n",
            "Unnormalized probability: \\(\\text{score}(x, y) = p(x, y)\\). The problem with the unnormalized probability is that it has a bias towards short answers (demo).\n",
            "Length-normalized probability: \\(\\text{score}(x, y) = \\frac{p(x, y)}{\\text{num-tokens}(y)}\\). This fixes the length bias. However, given two answers of the same length, the model still might prefer the more popular entity.\n",
            "Frequency-normalized probability: \\(\\text{score}(x, y) = \\frac{p(y \\mid x)}{p(y \\mid x_0)}\\), where \\(x_0\\) is a neutral string like \\(\\nl{Answer:}\\). This lowers the score for answers that happen to just be common (e.g., \\nl{John}). Compare demo versus demo.\n",
            "Results. GPT-3 got close but did not exceed the state-of-the-art:\n",
            "However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data!\n",
            "See the leaderboard for the latest results.\n",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. The language model has to somehow “know” the answer without looking up information in a database or a set of documents (we’ll consider reading comprehension later, where the information is provided).\n",
            "Input: What school did burne hogarth establish? Output: School of Visual Arts\n",
            "Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo):\n",
            "Q: ‘Nude Descending A Staircase’ is perhaps the most famous painting by which 20th century artist? A: Marcel Duchamp\n",
            "Results.\n",
            "We also see that both increasing the model size and the number of in-context training instances helps:\n",
            "\n",
            "Adaptation.\n",
            "We define a prompt the same as above (demo):\n",
            "Q: What school did burne hogarth establish? A: School of Visual Arts\n",
            "Results.\n",
            "Adaptation. We define a prompt the same as above (demo):\n",
            "Q: Who played tess on touched by an angel? A: Delloreese Patricia Early (July 6, 1931 - November 19, 2017), known professionally as Della Reese.\n",
            "Results.\n",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo):\n",
            "Mein Haus liegt auf dem Hügel. = My house is on the hill. Keinesfalls dürfen diese für den kommerziellen Gebrauch verwendet werden. = In no case may they be used for commercial purposes.\n",
            "Results. Here are the results from German to English:\n",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more “abstract reasoning” tasks, to evaluate GPT-3 as more of a general-purpose model.\n",
            "Adaptation. Pose the problem as question answering (demo):\n",
            "Q: What is 556 plus 497? A: 1053\n",
            "Results.\n",
            "\n",
            "It doesn’t work perfectly and can hardly be said to “understand arithmetic” fully, but it works surprisingly well.\n",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like.\n",
            "Title: United Methodists Agree to Historic Split Subtitle: Those who oppose gay marriage will form their own denomination Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church's annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination...\n",
            "Results. Humans were able to able to detect classify “human” versus “machine” only 52% of the time (barely above random chance).\n",
            "For the article above, humans guessed “machine” correctly only 12% of the time.\n",
            "Adaptation. Just describe the task in the prompt (demo):\n",
            "To “screeg” something is to swing a sword at it. An example of a sentence that uses the word screeg is: We screeged the tree with our swords.\n",
            "Adaptation. The prompt consists of input-output pairs (demo):\n",
            "Poor English input: I eated the purple berries. Good English output: I ate the purple berries. Poor English input: Thank you for picking me as your designer. I’d appreciate it. Good English output: Thank you for choosing me as your designer. I appreciate it. Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications. Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications. Poor English input: I’d be more than happy to work with you in another project. Good English output: I would be happy to work with you on another project.\n",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list.\n",
            "Benchmarks.\n",
            "The performance on these benchmarks is still mediocre, but it’s perhaps not bad given that we’re doing few-shot learning!\n",
            "Demos.\n",
            "The demos are creative and interesting, but it’s hard to tell how reliably they work.\n",
            "\\(\\newcommand{\\sV}{\\mathcal{V}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\x}{x_{1:L}} \\newcommand{\\softmax}{\\text{softmax}} \\newcommand{\\EmbedToken}{\\text{EmbedToken}} \\newcommand{\\SequenceModel}{\\text{SequenceModel}} \\newcommand{\\FeedForward}{\\text{FeedForward}} \\newcommand{\\FeedForwardSequenceModel}{\\text{FeedForwardSequenceModel}} \\newcommand{\\SequenceRNN}{\\text{SequenceRNN}} \\newcommand{\\BidirectionalSequenceRNN}{\\text{BidirectionalSequenceRNN}} \\newcommand{\\RNN}{\\text{RNN}} \\newcommand{\\SimpleRNN}{\\text{SimpleRNN}} \\newcommand{\\LSTM}{\\text{LSTM}} \\newcommand{\\GRU}{\\text{GRU}} \\newcommand{\\Attention}{\\text{Attention}} \\newcommand{\\MultiHeadedAttention}{\\text{MultiHeadedAttention}} \\newcommand{\\SelfAttention}{\\text{SelfAttention}} \\newcommand{\\TransformerBlock}{\\text{TransformerBlock}} \\newcommand{\\EmbedTokenWithPosition}{\\text{EmbedTokenWithPosition}} \\newcommand{\\LayerNorm}{\\text{LayerNorm}} \\newcommand{\\AddNorm}{\\text{AddNorm}} \\newcommand{\\nl}[1]{\\textsf{#1}} \\newcommand{\\generate}[1]{\\stackrel{#1}{\\rightsquigarrow}} \\newcommand{\\embed}{\\stackrel{\\phi}{\\Rightarrow}}\\) We started this course by analyzing a language model as a black box:\n",
            "Then we looked at the training data of large language models (e.g., The Pile):\n",
            "In this lecture, we will open up the onion all the way and talk about how large language models are built.\n",
            "Today’s lecture will focus on two topics, tokenization and model architecture.\n",
            "Tokenization: how a string is split into tokens.\n",
            "Model architecture: We will discuss mostly the Transformer architecture, which is the modeling innovation that really enabled large language models.\n",
            "Recall that a language model \\(p\\) is a probability distribution over a sequence of tokens where each token comes from some vocabulary \\(\\sV\\):\n",
            "However, natural language doesn’t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters):\n",
            "A tokenizer converts any string into a sequence of tokens.\n",
            "This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work.\n",
            "The simplest solution is to do:\n",
            "text.split(' ')\n",
            "我今天去了商店。 [gloss: I went to the store.]\n",
            "Then there are languages like German that have long compound words (e.g., Abwasserbehandlungsanlange).\n",
            "Even in English, there are hyphenated words (e.g., father-in-law) and contractions (e.g., don’t), which should get split up. For example, the Penn Treebank splits don’t into do and n’t, a linguistically-informed but not obvious choice.\n",
            "Therefore, splitting by spaces by spaces to identify words is quite problematic.\n",
            "What makes a good tokenization?\n",
            "Sennrich et al, 2015 applied the byte pair encoding (BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers.\n",
            "Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot.\n",
            "Example:\n",
            "The output of learning is:\n",
            "Applying the tokenizer. To tokenize a new string, apply the merges in the same order:\n",
            "Unicode.\n",
            "今天 [gloss: today] [x62, x11, 4e, ca]\n",
            "Rather than just splitting by frequency, a more “principled” approach is to define an objective function that captures what a good tokenization looks like. We now describe the unigram model (Kudo 2018).\n",
            "Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of\n",
            "Example:\n",
            "Algorithm:\n",
            "Impact:\n",
            "Examples of tokenizations for both GPT-3 and Jurassic (demo):\n",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence.\n",
            "Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings:\n",
            "We will broaden our notion of language models to three types of models.\n",
            "Encoder-only (BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text.\n",
            "These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks).\n",
            "Decoder-only (GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)).\n",
            "Encoder-decoder (BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\).\n",
            "We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\):\n",
            "We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on these slides from CS221 on differentiable programming, and will depart a bit from the standard presentation.\n",
            "The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity:\n",
            "This function will have parameters which we will include in the body but elide in the function signature for simplicity.\n",
            "In what follows, we will define a library of building blocks until we get to the full Transformer.\n",
            "First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data):\n",
            "def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\):\n",
            "These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes these context-independent embeddings and maps them into contextual embeddings.\n",
            "def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to a fixed length context, just as in an n-gram model:\n",
            "def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "The first “real” sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence of hidden states recursively.\n",
            "def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state:\n",
            "def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\):\n",
            "There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNN Elman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)).\n",
            "def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\):\n",
            "As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used by ELMo and ULMFiT.\n",
            "def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\):\n",
            "Notes:\n",
            "We will not discuss these models in the interest of time.\n",
            "Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models.\n",
            "There are great resources for learning about the Transformer:\n",
            "You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces.\n",
            "The crux of the Transformers are the attention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017).\n",
            "One can think of attention as a “soft” lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\):\n",
            "We can think of each \\(x_i\\) as representing a key-value pair via linear transformations:\n",
            "and forming the query via another linear transformation:\n",
            "The key and the query can be compared to give a score:\n",
            "These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\):\n",
            "Then the final output is a weighted combination over the values:\n",
            "We can write this all succinctly in matrix form:\n",
            "def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\):\n",
            "We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multiple attention heads and simply combine their outputs.\n",
            "def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\)\n",
            "Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce:\n",
            "def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\):\n",
            "Feedforward layer. Self-attention allows all the tokens to “talk” to each other, whereas feedforward connections provide:\n",
            "def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "Improving trainability. We’re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable.\n",
            "Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\):\n",
            "we add a residual (skip) connection so that if \\(f\\)’s gradients vanish, gradients can still flow through \\(\\x\\):\n",
            "Layer normalization. Another trick is layer normalization, which takes a takes a vector and makes sure its elements are too big:\n",
            "def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "We first define an adapter function that takes a sequence model \\(f\\) and makes it “robust”:\n",
            "def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "Finally, we can define the Transformer block succinctly as follows:\n",
            "def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "Positional embeddings. You might have noticed that as defined, the embedding of a token doesn’t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible.\n",
            "To fix this, we add positional information into the embedding:\n",
            "def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\):\n",
            "GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times:\n",
            "Shape of the architecture (how the 175 billion parameters are allocated):\n",
            "These decisions are not necessarily optimal. Levine et al. 2020 provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture.\n",
            "There are important but detailed differences between different versions of Transformers:\n",
            "Tokenization:\n",
            "Modeling:\n",
            "CS224N slides on Transformers\n",
            "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. Ofir Press, Noah A. Smith, M. Lewis. 2021. Introduces Alibi embeddings.\n",
            "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov. ACL 2019. Introduces recurrence on Transformers, relative position encoding scheme.\n",
            "Decoder-only architectures:\n",
            "Encoder-only architectures:\n",
            "Encoder-decoder architectures:\n",
            "So far, we’ve talked about the behavior (capabilities and harms) of large language models. Now, we peel open the first layer of the onion and start discussing how these models are constructed. The starting point of any machine learning approach is training data, so this is where we’ll start.\n",
            "Aside: Normally in machine learning, the training data and the test (evaluation) data are similar or at least of the same type. But for large language models, the training data is just “raw text”.\n",
            "In the rest of the lecture, we’ll talk about:\n",
            "Recall that large language models are trained on “raw text”. To be highly capable (e.g., have linguistic and world knowledge), this text should span a broad range of domains, genres, languages, etc.\n",
            "A natural place (but not the only place) to look for such text is the web, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and the Deep Web is even larger than that.\n",
            "It is worth noting that private datasets that reside in big companies are even larger than what’s available publicly. For example, WalMart generates 2.5 petabytes of data each hour!\n",
            "Common Crawl is a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot of Common Crawl has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index.\n",
            "Representation. Despite the richness of web data, it has been noted in Bender et al, 2021 that:\n",
            "Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models.\n",
            "WebText. The WebText dataset was used to train GPT-2.\n",
            "OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by the OpenWebText dataset.\n",
            "Toxicity analysis. Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found:\n",
            "The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model.\n",
            "Analysis. Dodge et al. 2021 performed a thorough analysis of the C4 dataset.\n",
            "Documentation levels:\n",
            "Note: Raffel et al. 2020 only provided scripts to recreate; cost thousands of dollars just to run these scripts.\n",
            "\n",
            "Benchmark data contamination.\n",
            "Example from the XSum summarization dataset:\n",
            "Input: The 48-year-old former Arsenal goalkeeper played for the Royals for four years. He was appointed youth academy director in 2000 and has been director of football since 2003. A West Brom statement said: “He played a key role in the Championship club twice winning promotion to the Premier League in 2006 and 2012. Output: West Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading.\n",
            "There are two types of contamination:\n",
            "Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage).\n",
            "The dataset could also be responsible for various harms:\n",
            "\n",
            "\n",
            "Data composition.\n",
            "\n",
            "Compare:\n",
            "\n",
            "Takeaway: The Pile contains a lot of information that’s not well covered by GPT-3’s dataset.\n",
            "They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.\n",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data.\n",
            "Two purposes:\n",
            "Dataset lifecycle (a sample of the questions from each category are provided below):\n",
            "Data statements. The data statements work is specialized to NLP datasets, and covers other aspects:\n",
            "As an example, let’s look at the datasheet for The Pile.\n",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles.\n",
            "Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant in industry.\n",
            "\n",
            "Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.\n",
            "Documentation for datasets:\n",
            "Datasets:\n",
            "Analysis of datasets:\n",
            "Filtering datasets:\n",
            "Data ecosystems:\n",
            "\\(\\newcommand{\\sV}{\\mathcal{V}} \\newcommand{\\sO}{\\mathcal{O}} \\newcommand{\\sD}{\\mathcal{D}} \\newcommand{\\sN}{\\mathcal{N}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\x}{x_{1:L}} \\newcommand{\\tx}{\\tilde x_{1:L}} \\newcommand{\\nl}[1]{\\textsf{#1}} \\newcommand{\\softmax}{\\text{softmax}} \\newcommand{\\TransformerBlock}{\\text{TransformerBlock}} \\newcommand{\\EmbedTokenWithPosition}{\\text{EmbedTokenWithPosition}} \\newcommand{\\SentenceEmbedding}{\\text{SentenceEmbedding}} \\newcommand{\\BERT}{\\text{BERT}} \\newcommand{\\MASK}{\\nl{[MASK]}} \\newcommand{\\SEP}{\\nl{[SEP]}} \\newcommand{\\CLS}{\\nl{[CLS]}} \\newcommand{\\generate}[1]{\\stackrel{#1}{\\rightsquigarrow}} \\newcommand{\\embed}{\\stackrel{\\phi}{\\Rightarrow}}\\) Last lecture, we talked about the model architecture for large language models (e.g., the Transformer). In this lecture, we will discuss how to train large language models.\n",
            "We will consider objective functions for the three types of language models:\n",
            "We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers):\n",
            "Recall that an autoregressive language model defines a conditional distribution:\n",
            "We define it as follows:\n",
            "Succinctly:\n",
            "Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models.\n",
            "Let \\(\\sD\\) be the training data consisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function:\n",
            "There’s more to say about how to efficiently optimize this function, but that’s all there is for the objective.\n",
            "Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don’t need to generate.\n",
            "BERT. We will first present the BERT objective function, which contains two terms:\n",
            "Take the example sequence for natural language inference (predict entailment, contradiction, or neutral):\n",
            "There are two special tokens:\n",
            "Using our notation from the previous lecture, the BERT model is defined as:\n",
            "where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:\n",
            "\n",
            "BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters.\n",
            "Masked language modeling. The basic idea of the masked language model is to train on the prediction problem:\n",
            "More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\).\n",
            "Model. We first define the model distribution that takes \\(\\tx\\) and predicts each token independently given the contextual embedding:\n",
            "Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that:\n",
            "Here’s how \\(A\\) is defined:\n",
            "Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then:\n",
            "Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not.\n",
            "\\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\).\n",
            "\\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\).\n",
            "We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision.\n",
            "Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows:\n",
            "Objective. Then the BERT objective is:\n",
            "We will talk about training later, but a few quick notes about BERT:\n",
            "RoBERTa makes the following changes to BERT:\n",
            "Example task (table-to-text generation):\n",
            "Recall that encoder-decoder models (e.g., BART, T5):\n",
            "BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model.\n",
            "BART considers the following transformations \\(A(\\tx \\mid \\x)\\):  Based on BERT-scaled experiments, they decided on the following transformations for the final model:\n",
            "They demonstrated strong results on both classification and generation tasks using fine-tuning.\n",
            "T5 (Text-to-Text Transfer Transformer).\n",
            "T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model.\n",
            "Tasks:\n",
            "This paper experimented with many different unsupervised objectives:  and found that the “i.i.d. noise, replace spans” worked well (though many objectives were similar).\n",
            "They also cast all classical NLP tasks in a uniform framework as “text-to-text” tasks:  Note the difference in approach to classification tasks:\n",
            "Notes:\n",
            "Now we turn our attention to how to optimize the objective. For simplicity, let’s take autogressive language modeling:\n",
            "Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches:\n",
            "The key concerns in optimization are:\n",
            "There are several levels that we can approach optimization:\n",
            "ADAM (adaptive moment estimation). ADAM incorporates two ideas:\n",
            "Have an adaptive (different) step size for each dimension of \\(\\theta\\) (inspiration from second-order methods).\n",
            "Updating parameters.\n",
            "Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)).\n",
            "AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint.\n",
            "Mixed-precision training is another method for reducing memory (Narang et al., 2018).\n",
            "\n",
            "Learning rates.\n",
            "Initialization.\n",
            "For GPT-3:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def html_to_text(url):\n",
        "        # Fetch the HTML content from the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check that the request was successful\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract text from the HTML\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # Print the extracted text\n",
        "        print(\"Extracted Text:\\n\")\n",
        "        print(text)\n",
        "        # final_data= all_content +'\\n'+ text\n",
        "        return text\n",
        "# Example usage\n",
        "url = \"https://arxiv.org/html/1706.03762v7\"\n",
        "aaa= html_to_text(url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE_0UdP1azFO",
        "outputId": "f960deb0-2c43-46ef-f68a-794066aeaad1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Attention Is All You Need\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1 Introduction\n",
            "2 Background\n",
            "\n",
            "3 Model Architecture\n",
            "\n",
            "\n",
            "3.1 Encoder and Decoder Stacks\n",
            "\n",
            "Encoder:\n",
            "Decoder:\n",
            "\n",
            "\n",
            "\n",
            "3.2 Attention\n",
            "\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "3.2.2 Multi-Head Attention\n",
            "3.2.3 Applications of Attention in our Model\n",
            "\n",
            "\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "3.4 Embeddings and Softmax\n",
            "3.5 Positional Encoding\n",
            "\n",
            "\n",
            "4 Why Self-Attention\n",
            "\n",
            "5 Training\n",
            "\n",
            "5.1 Training Data and Batching\n",
            "5.2 Hardware and Schedule\n",
            "5.3 Optimizer\n",
            "\n",
            "5.4 Regularization\n",
            "\n",
            "Residual Dropout\n",
            "Label Smoothing\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6 Results\n",
            "\n",
            "6.1 Machine Translation\n",
            "6.2 Model Variations\n",
            "6.3 English Constituency Parsing\n",
            "\n",
            "\n",
            "\n",
            "7 Conclusion\n",
            "\n",
            "Acknowledgements\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
            "\n",
            "Attention Is All You Need\n",
            "\n",
            "\n",
            "\n",
            "\\ANDAshish Vaswani\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "&Noam Shazeer11footnotemark: 1\n",
            "Google Brain\n",
            "noam@google.com\n",
            "&Niki Parmar11footnotemark: 1\n",
            "Google Research\n",
            "nikip@google.com\n",
            "&Jakob Uszkoreit11footnotemark: 1\n",
            "Google Research\n",
            "usz@google.com\n",
            "&Llion Jones11footnotemark: 1\n",
            "Google Research\n",
            "llion@google.com\n",
            "&Aidan N. Gomez11footnotemark: 1   \n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "&Łukasz Kaiser11footnotemark: 1\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "&Illia Polosukhin11footnotemark: 1   \n",
            "illia.polosukhin@gmail.com\n",
            "\n",
            "Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.\n",
            "Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
            "Work performed while at Google Brain.Work performed while at Google Research.\n",
            "\n",
            "\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
            "\n",
            "\n",
            "\n",
            "1 Introduction\n",
            "\n",
            "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n",
            "\n",
            "\n",
            "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states htsubscriptℎ𝑡h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, as a function of the previous hidden state ht−1subscriptℎ𝑡1h_{t-1}italic_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT and the input for position t𝑡titalic_t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n",
            "Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
            "\n",
            "\n",
            "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
            "\n",
            "\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2 Background\n",
            "\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
            "\n",
            "\n",
            "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
            "\n",
            "\n",
            "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n",
            "\n",
            "\n",
            "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
            "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3 Model Architecture\n",
            "\n",
            "Figure 1: The Transformer - model architecture.\n",
            "\n",
            "\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,…,xn)subscript𝑥1…subscript𝑥𝑛(x_{1},...,x_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) to a sequence of continuous representations 𝐳=(z1,…,zn)𝐳subscript𝑧1…subscript𝑧𝑛\\mathbf{z}=(z_{1},...,z_{n})bold_z = ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ). Given 𝐳𝐳\\mathbf{z}bold_z, the decoder then generates an output sequence (y1,…,ym)subscript𝑦1…subscript𝑦𝑚(y_{1},...,y_{m})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
            "\n",
            "\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n",
            "\n",
            "\n",
            "\n",
            "3.1 Encoder and Decoder Stacks\n",
            "\n",
            "Encoder:\n",
            "\n",
            "The encoder is composed of a stack of N=6𝑁6N=6italic_N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm⁢(x+Sublayer⁢(x))LayerNorm𝑥Sublayer𝑥\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x))roman_LayerNorm ( italic_x + roman_Sublayer ( italic_x ) ), where Sublayer⁢(x)Sublayer𝑥\\mathrm{Sublayer}(x)roman_Sublayer ( italic_x ) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512subscript𝑑model512d_{\\text{model}}=512italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 512.\n",
            "\n",
            "\n",
            "\n",
            "Decoder:\n",
            "\n",
            "The decoder is also composed of a stack of N=6𝑁6N=6italic_N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i𝑖iitalic_i can depend only on the known outputs at positions less than i𝑖iitalic_i.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.2 Attention\n",
            "\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
            "\n",
            "\n",
            "\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, and values of dimension dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. We compute the dot products of the query with all keys, divide each by dksubscript𝑑𝑘\\sqrt{d_{k}}square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG, and apply a softmax function to obtain the weights on the values.\n",
            "\n",
            "\n",
            "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q𝑄Qitalic_Q. The keys and values are also packed together into matrices K𝐾Kitalic_K and V𝑉Vitalic_V. We compute the matrix of outputs as:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Attention⁢(Q,K,V)=softmax⁢(Q⁢KTdk)⁢VAttention𝑄𝐾𝑉softmax𝑄superscript𝐾𝑇subscript𝑑𝑘𝑉\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})Vroman_Attention ( italic_Q , italic_K , italic_V ) = roman_softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_V\n",
            "\n",
            "(1)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1dk1subscript𝑑𝑘\\frac{1}{\\sqrt{d_{k}}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
            "\n",
            "\n",
            "While for small values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT [3]. We suspect that for large values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of q𝑞qitalic_q and k𝑘kitalic_k are independent random variables with mean 00 and variance 1111. Then their dot product, q⋅k=∑i=1dkqi⁢ki⋅𝑞𝑘superscriptsubscript𝑖1subscript𝑑𝑘subscript𝑞𝑖subscript𝑘𝑖q\\cdot k=\\sum_{i=1}^{d_{k}}q_{i}k_{i}italic_q ⋅ italic_k = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, has mean 00 and variance dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT.. To counteract this effect, we scale the dot products by 1dk1subscript𝑑𝑘\\frac{1}{\\sqrt{d_{k}}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.2.2 Multi-Head Attention\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Scaled Dot-Product Attention\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Multi-Head Attention\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
            "\n",
            "\n",
            "Instead of performing a single attention function with dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values hℎhitalic_h times with different, learned linear projections to dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT dimensions, respectively.\n",
            "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
            "\n",
            "\n",
            "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MultiHead⁢(Q,K,V)MultiHead𝑄𝐾𝑉\\displaystyle\\mathrm{MultiHead}(Q,K,V)roman_MultiHead ( italic_Q , italic_K , italic_V )\n",
            "=Concat⁢(head1,…,headh)⁢WOabsentConcatsubscripthead1…subscriptheadhsuperscript𝑊𝑂\\displaystyle=\\mathrm{Concat}(\\mathrm{head_{1}},...,\\mathrm{head_{h}})W^{O}= roman_Concat ( roman_head start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , roman_head start_POSTSUBSCRIPT roman_h end_POSTSUBSCRIPT ) italic_W start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "where⁢headiwheresubscriptheadi\\displaystyle\\text{where}~{}\\mathrm{head_{i}}where roman_head start_POSTSUBSCRIPT roman_i end_POSTSUBSCRIPT\n",
            "=Attention⁢(Q⁢WiQ,K⁢WiK,V⁢WiV)absentAttention𝑄subscriptsuperscript𝑊𝑄𝑖𝐾subscriptsuperscript𝑊𝐾𝑖𝑉subscriptsuperscript𝑊𝑉𝑖\\displaystyle=\\mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})= roman_Attention ( italic_Q italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_K italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_V italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Where the projections are parameter matrices WiQ∈ℝdmodel×dksubscriptsuperscript𝑊𝑄𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑘W^{Q}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WiK∈ℝdmodel×dksubscriptsuperscript𝑊𝐾𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑘W^{K}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WiV∈ℝdmodel×dvsubscriptsuperscript𝑊𝑉𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑣W^{V}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{v}}italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and WO∈ℝh⁢dv×dmodelsuperscript𝑊𝑂superscriptℝℎsubscript𝑑𝑣subscript𝑑modelW^{O}\\in\\mathbb{R}^{hd_{v}\\times d_{\\text{model}}}italic_W start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_h italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT.\n",
            "\n",
            "\n",
            "In this work we employ h=8ℎ8h=8italic_h = 8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64subscript𝑑𝑘subscript𝑑𝑣subscript𝑑modelℎ64d_{k}=d_{v}=d_{\\text{model}}/h=64italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT / italic_h = 64.\n",
            "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.2.3 Applications of Attention in our Model\n",
            "\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "\n",
            "\n",
            "•\n",
            "\n",
            "In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
            "\n",
            "\n",
            "\n",
            "•\n",
            "\n",
            "The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
            "\n",
            "\n",
            "\n",
            "•\n",
            "\n",
            "Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞-\\infty- ∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "\n",
            "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FFN⁢(x)=max⁡(0,x⁢W1+b1)⁢W2+b2FFN𝑥0𝑥subscript𝑊1subscript𝑏1subscript𝑊2subscript𝑏2\\mathrm{FFN}(x)=\\max(0,xW_{1}+b_{1})W_{2}+b_{2}roman_FFN ( italic_x ) = roman_max ( 0 , italic_x italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT\n",
            "\n",
            "(2)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel=512subscript𝑑model512d_{\\text{model}}=512italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 512, and the inner-layer has dimensionality df⁢f=2048subscript𝑑𝑓𝑓2048d_{ff}=2048italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT = 2048.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.4 Embeddings and Softmax\n",
            "\n",
            "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodelsubscript𝑑model\\sqrt{d_{\\text{model}}}square-root start_ARG italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_ARG.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.5 Positional Encoding\n",
            "\n",
            "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
            "\n",
            "\n",
            "In this work, we use sine and cosine functions of different frequencies:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "P⁢E(p⁢o⁢s,2⁢i)=s⁢i⁢n⁢(p⁢o⁢s/100002⁢i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖𝑠𝑖𝑛𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\text{model}}})italic_P italic_E start_POSTSUBSCRIPT ( italic_p italic_o italic_s , 2 italic_i ) end_POSTSUBSCRIPT = italic_s italic_i italic_n ( italic_p italic_o italic_s / 10000 start_POSTSUPERSCRIPT 2 italic_i / italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "P⁢E(p⁢o⁢s,2⁢i+1)=c⁢o⁢s⁢(p⁢o⁢s/100002⁢i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖1𝑐𝑜𝑠𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\text{model}}})italic_P italic_E start_POSTSUBSCRIPT ( italic_p italic_o italic_s , 2 italic_i + 1 ) end_POSTSUBSCRIPT = italic_c italic_o italic_s ( italic_p italic_o italic_s / 10000 start_POSTSUPERSCRIPT 2 italic_i / italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "where p⁢o⁢s𝑝𝑜𝑠positalic_p italic_o italic_s is the position and i𝑖iitalic_i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2⁢π2𝜋2\\pi2 italic_π to 10000⋅2⁢π⋅100002𝜋10000\\cdot 2\\pi10000 ⋅ 2 italic_π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k𝑘kitalic_k, P⁢Ep⁢o⁢s+k𝑃subscript𝐸𝑝𝑜𝑠𝑘PE_{pos+k}italic_P italic_E start_POSTSUBSCRIPT italic_p italic_o italic_s + italic_k end_POSTSUBSCRIPT can be represented as a linear function of P⁢Ep⁢o⁢s𝑃subscript𝐸𝑝𝑜𝑠PE_{pos}italic_P italic_E start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT.\n",
            "\n",
            "\n",
            "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4 Why Self-Attention\n",
            "\n",
            "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1,…,xn)subscript𝑥1…subscript𝑥𝑛(x_{1},...,x_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) to another sequence of equal length (z1,…,zn)subscript𝑧1…subscript𝑧𝑛(z_{1},...,z_{n})( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ), with xi,zi∈ℝdsubscript𝑥𝑖subscript𝑧𝑖superscriptℝ𝑑x_{i},z_{i}\\in\\mathbb{R}^{d}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
            "\n",
            "\n",
            "One is the total computational complexity per layer.\n",
            "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
            "\n",
            "\n",
            "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
            "\n",
            "\n",
            "Table 1: \n",
            "Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n𝑛nitalic_n is the sequence length, d𝑑ditalic_d is the representation dimension, k𝑘kitalic_k is the kernel size of convolutions and r𝑟ritalic_r the size of the neighborhood in restricted self-attention.\n",
            "\n",
            "\n",
            "\n",
            "Layer Type\n",
            "Complexity per Layer\n",
            "Sequential\n",
            "Maximum Path Length\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Operations\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Self-Attention\n",
            "O⁢(n2⋅d)𝑂⋅superscript𝑛2𝑑O(n^{2}\\cdot d)italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⋅ italic_d )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "\n",
            "\n",
            "Recurrent\n",
            "O⁢(n⋅d2)𝑂⋅𝑛superscript𝑑2O(n\\cdot d^{2})italic_O ( italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )\n",
            "O⁢(n)𝑂𝑛O(n)italic_O ( italic_n )\n",
            "O⁢(n)𝑂𝑛O(n)italic_O ( italic_n )\n",
            "\n",
            "\n",
            "Convolutional\n",
            "O⁢(k⋅n⋅d2)𝑂⋅𝑘𝑛superscript𝑑2O(k\\cdot n\\cdot d^{2})italic_O ( italic_k ⋅ italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "O⁢(l⁢o⁢gk⁢(n))𝑂𝑙𝑜subscript𝑔𝑘𝑛O(log_{k}(n))italic_O ( italic_l italic_o italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_n ) )\n",
            "\n",
            "\n",
            "Self-Attention (restricted)\n",
            "O⁢(r⋅n⋅d)𝑂⋅𝑟𝑛𝑑O(r\\cdot n\\cdot d)italic_O ( italic_r ⋅ italic_n ⋅ italic_d )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "O⁢(n/r)𝑂𝑛𝑟O(n/r)italic_O ( italic_n / italic_r )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) sequential operations.\n",
            "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n𝑛nitalic_n is smaller than the representation dimensionality d𝑑ditalic_d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations.\n",
            "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r𝑟ritalic_r in the input sequence centered around the respective output position. This would increase the maximum path length to O⁢(n/r)𝑂𝑛𝑟O(n/r)italic_O ( italic_n / italic_r ). We plan to investigate this approach further in future work.\n",
            "\n",
            "\n",
            "A single convolutional layer with kernel width k<n𝑘𝑛k<nitalic_k < italic_n does not connect all pairs of input and output positions. Doing so requires a stack of O⁢(n/k)𝑂𝑛𝑘O(n/k)italic_O ( italic_n / italic_k ) convolutional layers in the case of contiguous kernels, or O⁢(l⁢o⁢gk⁢(n))𝑂𝑙𝑜subscript𝑔𝑘𝑛O(log_{k}(n))italic_O ( italic_l italic_o italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_n ) ) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network.\n",
            "Convolutional layers are generally more expensive than recurrent layers, by a factor of k𝑘kitalic_k. Separable convolutions [6], however, decrease the complexity considerably, to O⁢(k⋅n⋅d+n⋅d2)𝑂⋅𝑘𝑛𝑑⋅𝑛superscript𝑑2O(k\\cdot n\\cdot d+n\\cdot d^{2})italic_O ( italic_k ⋅ italic_n ⋅ italic_d + italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Even with k=n𝑘𝑛k=nitalic_k = italic_n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
            "\n",
            "\n",
            "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5 Training\n",
            "\n",
            "This section describes the training regime for our models.\n",
            "\n",
            "\n",
            "\n",
            "5.1 Training Data and Batching\n",
            "\n",
            "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.2 Hardware and Schedule\n",
            "\n",
            "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.3 Optimizer\n",
            "\n",
            "We used the Adam optimizer [20] with β1=0.9subscript𝛽10.9\\beta_{1}=0.9italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, β2=0.98subscript𝛽20.98\\beta_{2}=0.98italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.98 and ϵ=10−9italic-ϵsuperscript109\\epsilon=10^{-9}italic_ϵ = 10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT. We varied the learning rate over the course of training, according to the formula:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "l⁢r⁢a⁢t⁢e=dmodel−0.5⋅min⁡(s⁢t⁢e⁢p⁢_⁢n⁢u⁢m−0.5,s⁢t⁢e⁢p⁢_⁢n⁢u⁢m⋅w⁢a⁢r⁢m⁢u⁢p⁢_⁢s⁢t⁢e⁢p⁢s−1.5)𝑙𝑟𝑎𝑡𝑒⋅superscriptsubscript𝑑model0.5𝑠𝑡𝑒𝑝_𝑛𝑢superscript𝑚0.5⋅𝑠𝑡𝑒𝑝_𝑛𝑢𝑚𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝superscript𝑠1.5lrate=d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{%\n",
            "warmup\\_steps}^{-1.5})italic_l italic_r italic_a italic_t italic_e = italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT ⋅ roman_min ( italic_s italic_t italic_e italic_p _ italic_n italic_u italic_m start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT , italic_s italic_t italic_e italic_p _ italic_n italic_u italic_m ⋅ italic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s start_POSTSUPERSCRIPT - 1.5 end_POSTSUPERSCRIPT )\n",
            "\n",
            "(3)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "This corresponds to increasing the learning rate linearly for the first w⁢a⁢r⁢m⁢u⁢p⁢_⁢s⁢t⁢e⁢p⁢s𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝𝑠warmup\\_stepsitalic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used w⁢a⁢r⁢m⁢u⁢p⁢_⁢s⁢t⁢e⁢p⁢s=4000𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝𝑠4000warmup\\_steps=4000italic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s = 4000.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.4 Regularization\n",
            "\n",
            "We employ three types of regularization during training:\n",
            "\n",
            "\n",
            "Residual Dropout\n",
            "\n",
            "We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pd⁢r⁢o⁢p=0.1subscript𝑃𝑑𝑟𝑜𝑝0.1P_{drop}=0.1italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT = 0.1.\n",
            "\n",
            "\n",
            "\n",
            "Label Smoothing\n",
            "\n",
            "During training, we employed label smoothing of value ϵl⁢s=0.1subscriptitalic-ϵ𝑙𝑠0.1\\epsilon_{ls}=0.1italic_ϵ start_POSTSUBSCRIPT italic_l italic_s end_POSTSUBSCRIPT = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6 Results\n",
            "\n",
            "\n",
            "6.1 Machine Translation\n",
            "\n",
            "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. \n",
            "\n",
            "\n",
            "\n",
            "Model\n",
            "BLEU\n",
            "\n",
            "Training Cost (FLOPs)\n",
            "\n",
            "\n",
            "EN-DE\n",
            "EN-FR\n",
            "\n",
            "EN-DE\n",
            "EN-FR\n",
            "\n",
            "\n",
            "ByteNet [18]\n",
            "\n",
            "23.75\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Deep-Att + PosUnk [39]\n",
            "\n",
            "\n",
            "39.2\n",
            "\n",
            "\n",
            "1.0⋅1020⋅1.0superscript10201.0\\cdot 10^{20}1.0 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "GNMT + RL [38]\n",
            "\n",
            "24.6\n",
            "39.92\n",
            "\n",
            "2.3⋅1019⋅2.3superscript10192.3\\cdot 10^{19}2.3 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "1.4⋅1020⋅1.4superscript10201.4\\cdot 10^{20}1.4 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "ConvS2S [9]\n",
            "\n",
            "25.16\n",
            "40.46\n",
            "\n",
            "9.6⋅1018⋅9.6superscript10189.6\\cdot 10^{18}9.6 ⋅ 10 start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPT\n",
            "1.5⋅1020⋅1.5superscript10201.5\\cdot 10^{20}1.5 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "MoE [32]\n",
            "\n",
            "26.03\n",
            "40.56\n",
            "\n",
            "2.0⋅1019⋅2.0superscript10192.0\\cdot 10^{19}2.0 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "1.2⋅1020⋅1.2superscript10201.2\\cdot 10^{20}1.2 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "Deep-Att + PosUnk Ensemble [39]\n",
            "\n",
            "\n",
            "40.4\n",
            "\n",
            "\n",
            "8.0⋅1020⋅8.0superscript10208.0\\cdot 10^{20}8.0 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "GNMT + RL Ensemble [38]\n",
            "\n",
            "26.30\n",
            "41.16\n",
            "\n",
            "1.8⋅1020⋅1.8superscript10201.8\\cdot 10^{20}1.8 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "1.1⋅1021⋅1.1superscript10211.1\\cdot 10^{21}1.1 ⋅ 10 start_POSTSUPERSCRIPT 21 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "ConvS2S Ensemble [9]\n",
            "\n",
            "26.36\n",
            "41.29\n",
            "\n",
            "7.7⋅1019⋅7.7superscript10197.7\\cdot 10^{19}7.7 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "1.2⋅1021⋅1.2superscript10211.2\\cdot 10^{21}1.2 ⋅ 10 start_POSTSUPERSCRIPT 21 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "Transformer (base model)\n",
            "27.3\n",
            "38.1\n",
            "\n",
            "3.3⋅𝟏𝟎𝟏𝟖bold-⋅3.3superscript10183.3\\cdot 10^{18}bold_3.3 bold_⋅ bold_10 start_POSTSUPERSCRIPT bold_18 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "Transformer (big)\n",
            "28.4\n",
            "41.8\n",
            "\n",
            "2.3⋅1019⋅2.3superscript10192.3\\cdot 10^{19}2.3 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.02.02.02.0 BLEU, establishing a new state-of-the-art BLEU score of 28.428.428.428.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.53.53.53.5 days on 8888 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
            "\n",
            "\n",
            "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.041.041.041.0, outperforming all of the previously published single models, at less than 1/4141/41 / 4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pd⁢r⁢o⁢p=0.1subscript𝑃𝑑𝑟𝑜𝑝0.1P_{drop}=0.1italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT = 0.1, instead of 0.30.30.30.3.\n",
            "\n",
            "\n",
            "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4444 and length penalty α=0.6𝛼0.6\\alpha=0.6italic_α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50505050, but terminate early when possible [38].\n",
            "\n",
            "\n",
            "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 222We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6.2 Model Variations\n",
            "\n",
            "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "N𝑁Nitalic_N\n",
            "dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT\n",
            "dffsubscript𝑑ffd_{\\text{ff}}italic_d start_POSTSUBSCRIPT ff end_POSTSUBSCRIPT\n",
            "hℎhitalic_h\n",
            "dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT\n",
            "dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT\n",
            "Pd⁢r⁢o⁢psubscript𝑃𝑑𝑟𝑜𝑝P_{drop}italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT\n",
            "ϵl⁢ssubscriptitalic-ϵ𝑙𝑠\\epsilon_{ls}italic_ϵ start_POSTSUBSCRIPT italic_l italic_s end_POSTSUBSCRIPT\n",
            "train\n",
            "PPL\n",
            "BLEU\n",
            "params\n",
            "\n",
            "\n",
            "\n",
            "steps\n",
            "(dev)\n",
            "(dev)\n",
            "×106absentsuperscript106\\times 10^{6}× 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "base\n",
            "6\n",
            "512\n",
            "2048\n",
            "8\n",
            "64\n",
            "64\n",
            "0.1\n",
            "0.1\n",
            "100K\n",
            "4.92\n",
            "25.8\n",
            "65\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(A)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "512\n",
            "512\n",
            "\n",
            "\n",
            "\n",
            "5.29\n",
            "24.9\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4\n",
            "128\n",
            "128\n",
            "\n",
            "\n",
            "\n",
            "5.00\n",
            "25.5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16\n",
            "32\n",
            "32\n",
            "\n",
            "\n",
            "\n",
            "4.91\n",
            "25.8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "32\n",
            "16\n",
            "16\n",
            "\n",
            "\n",
            "\n",
            "5.01\n",
            "25.4\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(B)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.16\n",
            "25.1\n",
            "58\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "32\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.01\n",
            "25.4\n",
            "60\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(C)\n",
            "\n",
            "2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6.11\n",
            "23.7\n",
            "36\n",
            "\n",
            "\n",
            "4\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.19\n",
            "25.3\n",
            "50\n",
            "\n",
            "\n",
            "8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4.88\n",
            "25.5\n",
            "80\n",
            "\n",
            "\n",
            "\n",
            "256\n",
            "\n",
            "\n",
            "32\n",
            "32\n",
            "\n",
            "\n",
            "\n",
            "5.75\n",
            "24.5\n",
            "28\n",
            "\n",
            "\n",
            "\n",
            "1024\n",
            "\n",
            "\n",
            "128\n",
            "128\n",
            "\n",
            "\n",
            "\n",
            "4.66\n",
            "26.0\n",
            "168\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.12\n",
            "25.4\n",
            "53\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4096\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4.75\n",
            "26.2\n",
            "90\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(D)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.0\n",
            "\n",
            "\n",
            "5.77\n",
            "24.6\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.2\n",
            "\n",
            "\n",
            "4.95\n",
            "25.5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.0\n",
            "\n",
            "4.67\n",
            "25.3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.2\n",
            "\n",
            "5.47\n",
            "25.7\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(E)\n",
            "\n",
            "positional embedding instead of sinusoids\n",
            "\n",
            "4.92\n",
            "25.7\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "big\n",
            "6\n",
            "1024\n",
            "4096\n",
            "16\n",
            "\n",
            "\n",
            "0.3\n",
            "\n",
            "300K\n",
            "4.33\n",
            "26.4\n",
            "213\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
            "\n",
            "\n",
            "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
            "\n",
            "\n",
            "In Table 3 rows (B), we observe that reducing the attention key size dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6.3 English Constituency Parsing\n",
            "\n",
            "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
            "\n",
            "\n",
            "\n",
            "Parser\n",
            "Training\n",
            "WSJ 23 F1\n",
            "\n",
            "\n",
            "Vinyals & Kaiser el al. (2014) [37]\n",
            "\n",
            "WSJ only, discriminative\n",
            "88.3\n",
            "\n",
            "\n",
            "Petrov et al. (2006) [29]\n",
            "\n",
            "WSJ only, discriminative\n",
            "90.4\n",
            "\n",
            "\n",
            "Zhu et al. (2013) [40]\n",
            "\n",
            "WSJ only, discriminative\n",
            "90.4\n",
            "\n",
            "\n",
            "Dyer et al. (2016) [8]\n",
            "\n",
            "WSJ only, discriminative\n",
            "91.7\n",
            "\n",
            "\n",
            "Transformer (4 layers)\n",
            "WSJ only, discriminative\n",
            "91.3\n",
            "\n",
            "\n",
            "Zhu et al. (2013) [40]\n",
            "\n",
            "semi-supervised\n",
            "91.3\n",
            "\n",
            "\n",
            "Huang & Harper (2009) [14]\n",
            "\n",
            "semi-supervised\n",
            "91.3\n",
            "\n",
            "\n",
            "McClosky et al. (2006) [26]\n",
            "\n",
            "semi-supervised\n",
            "92.1\n",
            "\n",
            "\n",
            "Vinyals & Kaiser el al. (2014) [37]\n",
            "\n",
            "semi-supervised\n",
            "92.1\n",
            "\n",
            "\n",
            "Transformer (4 layers)\n",
            "semi-supervised\n",
            "92.7\n",
            "\n",
            "\n",
            "Luong et al. (2015) [23]\n",
            "\n",
            "multi-task\n",
            "93.0\n",
            "\n",
            "\n",
            "Dyer et al. (2016) [8]\n",
            "\n",
            "generative\n",
            "93.3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\n",
            "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
            "\n",
            "\n",
            "We trained a 4-layer transformer with dm⁢o⁢d⁢e⁢l=1024subscript𝑑𝑚𝑜𝑑𝑒𝑙1024d_{model}=1024italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
            "\n",
            "\n",
            "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300300300300. We used a beam size of 21212121 and α=0.3𝛼0.3\\alpha=0.3italic_α = 0.3 for both WSJ only and the semi-supervised setting.\n",
            "\n",
            "\n",
            "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
            "\n",
            "\n",
            "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "7 Conclusion\n",
            "\n",
            "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
            "\n",
            "\n",
            "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
            "\n",
            "\n",
            "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.\n",
            "Making generation less sequential is another research goals of ours.\n",
            "\n",
            "\n",
            "The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\n",
            "\n",
            "\n",
            "Acknowledgements\n",
            "\n",
            "We are grateful to Nal Kalchbrenner and Stephan Gouws for\n",
            "their fruitful comments, corrections and inspiration.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "References\n",
            "\n",
            "\n",
            "[1]\n",
            "\n",
            "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\n",
            "\n",
            "\n",
            "Layer normalization.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1607.06450, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[2]\n",
            "\n",
            "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\n",
            "\n",
            "\n",
            "Neural machine translation by jointly learning to align and\n",
            "translate.\n",
            "\n",
            "\n",
            "CoRR, abs/1409.0473, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[3]\n",
            "\n",
            "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le.\n",
            "\n",
            "\n",
            "Massive exploration of neural machine translation architectures.\n",
            "\n",
            "\n",
            "CoRR, abs/1703.03906, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[4]\n",
            "\n",
            "Jianpeng Cheng, Li Dong, and Mirella Lapata.\n",
            "\n",
            "\n",
            "Long short-term memory-networks for machine reading.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1601.06733, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[5]\n",
            "\n",
            "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger\n",
            "Schwenk, and Yoshua Bengio.\n",
            "\n",
            "\n",
            "Learning phrase representations using rnn encoder-decoder for\n",
            "statistical machine translation.\n",
            "\n",
            "\n",
            "CoRR, abs/1406.1078, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[6]\n",
            "\n",
            "Francois Chollet.\n",
            "\n",
            "\n",
            "Xception: Deep learning with depthwise separable convolutions.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1610.02357, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[7]\n",
            "\n",
            "Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua\n",
            "Bengio.\n",
            "\n",
            "\n",
            "Empirical evaluation of gated recurrent neural networks on sequence\n",
            "modeling.\n",
            "\n",
            "\n",
            "CoRR, abs/1412.3555, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[8]\n",
            "\n",
            "Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith.\n",
            "\n",
            "\n",
            "Recurrent neural network grammars.\n",
            "\n",
            "\n",
            "In Proc. of NAACL, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[9]\n",
            "\n",
            "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.\n",
            "\n",
            "\n",
            "Convolutional sequence to sequence learning.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1705.03122v2, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[10]\n",
            "\n",
            "Alex Graves.\n",
            "\n",
            "\n",
            "Generating sequences with recurrent neural networks.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1308.0850, 2013.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[11]\n",
            "\n",
            "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
            "\n",
            "\n",
            "Deep residual learning for image recognition.\n",
            "\n",
            "\n",
            "In Proceedings of the IEEE Conference on Computer Vision and\n",
            "Pattern Recognition, pages 770–778, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[12]\n",
            "\n",
            "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.\n",
            "\n",
            "\n",
            "Gradient flow in recurrent nets: the difficulty of learning long-term\n",
            "dependencies, 2001.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[13]\n",
            "\n",
            "Sepp Hochreiter and Jürgen Schmidhuber.\n",
            "\n",
            "\n",
            "Long short-term memory.\n",
            "\n",
            "\n",
            "Neural computation, 9(8):1735–1780, 1997.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[14]\n",
            "\n",
            "Zhongqiang Huang and Mary Harper.\n",
            "\n",
            "\n",
            "Self-training PCFG grammars with latent annotations across\n",
            "languages.\n",
            "\n",
            "\n",
            "In Proceedings of the 2009 Conference on Empirical Methods in\n",
            "Natural Language Processing, pages 832–841. ACL, August 2009.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[15]\n",
            "\n",
            "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.\n",
            "\n",
            "\n",
            "Exploring the limits of language modeling.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1602.02410, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[16]\n",
            "\n",
            "Łukasz Kaiser and Samy Bengio.\n",
            "\n",
            "\n",
            "Can active memory replace attention?\n",
            "\n",
            "\n",
            "In Advances in Neural Information Processing Systems, (NIPS),\n",
            "2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[17]\n",
            "\n",
            "Łukasz Kaiser and Ilya Sutskever.\n",
            "\n",
            "\n",
            "Neural GPUs learn algorithms.\n",
            "\n",
            "\n",
            "In International Conference on Learning Representations\n",
            "(ICLR), 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[18]\n",
            "\n",
            "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex\n",
            "Graves, and Koray Kavukcuoglu.\n",
            "\n",
            "\n",
            "Neural machine translation in linear time.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1610.10099v2, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[19]\n",
            "\n",
            "Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush.\n",
            "\n",
            "\n",
            "Structured attention networks.\n",
            "\n",
            "\n",
            "In International Conference on Learning Representations, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[20]\n",
            "\n",
            "Diederik Kingma and Jimmy Ba.\n",
            "\n",
            "\n",
            "Adam: A method for stochastic optimization.\n",
            "\n",
            "\n",
            "In ICLR, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[21]\n",
            "\n",
            "Oleksii Kuchaiev and Boris Ginsburg.\n",
            "\n",
            "\n",
            "Factorization tricks for LSTM networks.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1703.10722, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[22]\n",
            "\n",
            "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
            "Zhou, and Yoshua Bengio.\n",
            "\n",
            "\n",
            "A structured self-attentive sentence embedding.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1703.03130, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[23]\n",
            "\n",
            "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.\n",
            "\n",
            "\n",
            "Multi-task sequence to sequence learning.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1511.06114, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[24]\n",
            "\n",
            "Minh-Thang Luong, Hieu Pham, and Christopher D Manning.\n",
            "\n",
            "\n",
            "Effective approaches to attention-based neural machine translation.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1508.04025, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[25]\n",
            "\n",
            "Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.\n",
            "\n",
            "\n",
            "Building a large annotated corpus of english: The penn treebank.\n",
            "\n",
            "\n",
            "Computational linguistics, 19(2):313–330, 1993.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[26]\n",
            "\n",
            "David McClosky, Eugene Charniak, and Mark Johnson.\n",
            "\n",
            "\n",
            "Effective self-training for parsing.\n",
            "\n",
            "\n",
            "In Proceedings of the Human Language Technology Conference of\n",
            "the NAACL, Main Conference, pages 152–159. ACL, June 2006.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[27]\n",
            "\n",
            "Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.\n",
            "\n",
            "\n",
            "A decomposable attention model.\n",
            "\n",
            "\n",
            "In Empirical Methods in Natural Language Processing, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[28]\n",
            "\n",
            "Romain Paulus, Caiming Xiong, and Richard Socher.\n",
            "\n",
            "\n",
            "A deep reinforced model for abstractive summarization.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1705.04304, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[29]\n",
            "\n",
            "Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.\n",
            "\n",
            "\n",
            "Learning accurate, compact, and interpretable tree annotation.\n",
            "\n",
            "\n",
            "In Proceedings of the 21st International Conference on\n",
            "Computational Linguistics and 44th Annual Meeting of the ACL, pages\n",
            "433–440. ACL, July 2006.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[30]\n",
            "\n",
            "Ofir Press and Lior Wolf.\n",
            "\n",
            "\n",
            "Using the output embedding to improve language models.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1608.05859, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[31]\n",
            "\n",
            "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
            "\n",
            "\n",
            "Neural machine translation of rare words with subword units.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1508.07909, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[32]\n",
            "\n",
            "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,\n",
            "Geoffrey Hinton, and Jeff Dean.\n",
            "\n",
            "\n",
            "Outrageously large neural networks: The sparsely-gated\n",
            "mixture-of-experts layer.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1701.06538, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[33]\n",
            "\n",
            "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and\n",
            "Ruslan Salakhutdinov.\n",
            "\n",
            "\n",
            "Dropout: a simple way to prevent neural networks from overfitting.\n",
            "\n",
            "\n",
            "Journal of Machine Learning Research, 15(1):1929–1958, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[34]\n",
            "\n",
            "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.\n",
            "\n",
            "\n",
            "End-to-end memory networks.\n",
            "\n",
            "\n",
            "In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,\n",
            "editors, Advances in Neural Information Processing Systems 28, pages\n",
            "2440–2448. Curran Associates, Inc., 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[35]\n",
            "\n",
            "Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.\n",
            "\n",
            "\n",
            "Sequence to sequence learning with neural networks.\n",
            "\n",
            "\n",
            "In Advances in Neural Information Processing Systems, pages\n",
            "3104–3112, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[36]\n",
            "\n",
            "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and\n",
            "Zbigniew Wojna.\n",
            "\n",
            "\n",
            "Rethinking the inception architecture for computer vision.\n",
            "\n",
            "\n",
            "CoRR, abs/1512.00567, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[37]\n",
            "\n",
            "Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton.\n",
            "\n",
            "\n",
            "Grammar as a foreign language.\n",
            "\n",
            "\n",
            "In Advances in Neural Information Processing Systems, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[38]\n",
            "\n",
            "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
            "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n",
            "\n",
            "\n",
            "Google’s neural machine translation system: Bridging the gap between\n",
            "human and machine translation.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1609.08144, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[39]\n",
            "\n",
            "Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.\n",
            "\n",
            "\n",
            "Deep recurrent models with fast-forward connections for neural\n",
            "machine translation.\n",
            "\n",
            "\n",
            "CoRR, abs/1606.04199, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[40]\n",
            "\n",
            "Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.\n",
            "\n",
            "\n",
            "Fast and accurate shift-reduce constituent parsing.\n",
            "\n",
            "\n",
            "In Proceedings of the 51st Annual Meeting of the ACL (Volume 1:\n",
            "Long Papers), pages 434–443. ACL, August 2013.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Attention Visualizations\n",
            "\n",
            "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making…more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated  on Tue Apr 30 16:03:52 2024 by LaTeXML\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(aaa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2AXbQvxfChO",
        "outputId": "dd75ebb7-6163-460e-9fa2-b040992bea43"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_data= all_content +'\\n'+ aaa"
      ],
      "metadata": {
        "id": "aUxYt1N9bKUK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ght4hOxbrt1",
        "outputId": "5c323867-0987-4a5d-e828-2af68ec4a2bc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to CS324! This is a new course on understanding and developing large language models.\n",
            "The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):\n",
            "The probability intuitively tells us how “good” a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo):\n",
            "Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.\n",
            "For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it’s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.\n",
            "Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted:\n",
            "How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an “average” sequence but something closer to the “best” sequence.\n",
            "A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using the chain rule of probability:\n",
            "For example (demo):\n",
            "In particular, \\(p(x_i \\mid x_{1:i-1})\\) is a conditional probability distribution of the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\).\n",
            "Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network).\n",
            "Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:\n",
            "where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:\n",
            "However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) the annealed conditional probability distribution. For example:\n",
            "Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing.\n",
            "Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.\n",
            "Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):\n",
            "If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\).\n",
            "As we’ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.\n",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as\n",
            "The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:\n",
            "Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.\n",
            "Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a “true” distribution \\(p\\) out there (the existence of this is questionable, but it’s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\).\n",
            "Shannon also defined cross entropy:\n",
            "which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)).\n",
            "Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\),\n",
            "which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English.\n",
            "So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\).\n",
            "Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paper Prediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human:\n",
            "Humans aren’t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses.\n",
            "Language models became first used in practical applications that required generation of text:\n",
            "Noisy channel model. The dominant paradigm for solving these tasks then was the noisy channel model. Taking speech recognition as an example:\n",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).\n",
            "N-gram models. In an n-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history:\n",
            "For example, a trigram (\\(n=3\\)) model would define:\n",
            "These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing).\n",
            "Fitting n-gram models to data is extremely computationally cheap and scalable. As a result, n-gram models were trained on massive amount of text. For example, Brants et al. (2007) trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix:\n",
            "If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in “huge” corpora):\n",
            "As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn’t a huge problem.\n",
            "An important step forward for language models was the introduction of neural networks. Bengio et al., 2003 pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network:\n",
            "Note that the context length is still bounded by \\(n\\), but it is now statistically feasible to estimate neural language models for much larger values of \\(n\\).\n",
            "Now, the main challenge was that training neural networks was much more computationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade.\n",
            "Since 2003, two other key developments in neural language modeling include:\n",
            "Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\(x_i\\) to depend on the entire context \\(x_{1:i-1}\\) (effectively \\(n = \\infty\\)), but these were hard to train.\n",
            "Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\(n\\), but were much easier to train (and exploited the parallelism of GPUs). Also, \\(n\\) could be made “large enough” for many applications (GPT-3 used \\(n = 2048\\)).\n",
            "We will open up the hood and dive deeper into the architecture and training later in the course.\n",
            "Having introduced language models, one might wonder why we need a course specifically on large language models.\n",
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:\n",
            "Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that “just scaling up” these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact.\n",
            "Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa.\n",
            "Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past.\n",
            "Recall that language models are capable of conditional generation: given a prompt, generate a completion:\n",
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):\n",
            "One can prompt a language model to solve word analogies (demo):\n",
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):\n",
            "Title: NLP Researchers at Stanford Discover Black Holes in Language Models Article: On January 3, 2007, the Stanford University News Service published an article that reported a remarkable discovery by NLP researchers at Stanford. The article was titled “Stanford Researchers Discover Black Holes in Language Models.” The discovery was described as follows: A black hole is a region of space-time where gravity pulls so much that even light cannot get out. Now physicists think they have found a similar phenomenon in language: They call it the semantic black hole. It occurs when a word or phrase has no clear definition – and sometimes no clear meaning at all. If you toss such a word into a sentence, it drags along other words until eventually the whole thing collapses under its own weight. “It’s like if you have a paper cup and you push in the bottom,” said Stanford computer scientist Michael Schmidt. “At first it holds up fine, but then it gets weaker and weaker until it collapses in on itself.” Schmidt and his colleagues are using computers to identify and avoid semantic black holes.\n",
            "In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let’s start with an example (demo):\n",
            "Input: Where is Stanford University? Output: Stanford University is in California.\n",
            "We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence.\n",
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):\n",
            "Input: Where is MIT? Output: Cambridge  Input: Where is University of Washington? Output: Seattle  Input: Where is Stanford University? Output: Stanford\n",
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.\n",
            "Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity.\n",
            "Given the strong capabilities of language models, it is not surprising to see their widespread adoption.\n",
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.\n",
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:\n",
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people.\n",
            "An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer.\n",
            "So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind’s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course.\n",
            "Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer can seem correct and there is no way of knowing (demo)\n",
            "Input: Who invented the Internet? Output: Al Gore\n",
            "In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable?\n",
            "Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo):\n",
            "The software developer finished the program. He celebrated. The software developer finished the program. She celebrated.\n",
            "Social biases are of course encoded in the data, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias?\n",
            "Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content. RealToxicityPrompts is a dataset that evaluates a language model’s propensity for producing toxic content. For example:\n",
            "So, I’m starting to think she’s full _\n",
            "As another example, GPT-3 has been demonstrated to output anti-Muslim stereotypes:\n",
            "Two Muslims walked into a _\n",
            "Applications such as writing assistants or chatbots would be vulnerable.\n",
            "Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models’ linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.\n",
            "Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt:\n",
            "In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem.\n",
            "Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation?\n",
            "For example, if you prompt GPT-3 with the first line of Harry Potter (demo):\n",
            "Mr. and Mrs. Dursley of number four, Privet Drive, _\n",
            "It will happily continue to spout out text from Harry Potter with high confidence.\n",
            "Cost and environmental impact. Finally, large language models can be quite expensive to work with.\n",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.\n",
            "Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face’s Big Science project, EleutherAI, and Stanford’s CRFM. Given language models’ increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.\n",
            "This course will be structured like an onion:\n",
            "Behavior of large language models: We will start at the outer layer where we only have blackbox API access to the model (as we’ve had so far). Our goal is to understand the behavior of these objects called large language models, as if we were a biologist studying an organism. Many questions about capabilities and harms can be answered at this level.\n",
            "Data behind large language models: Then we take a deeper look behind the data that is used to train large language models, and address issues such as security, privacy, and legal considerations. Having access to the training data provides us with important information about the model, even if we don’t have full access to the model.\n",
            "Building large language models: Then we arrive at the core of the onion, where we study how large language models are built (the model architectures, the training algorithms, etc.).\n",
            "Beyond large language models: Finally, we end the course with a look beyond language models. A language model is just a distribution over a sequence of tokens. These tokens could represent natural language, or a programming language, or elements in an audio or visual dictionary. Language models also belong to a more general class of foundation models, which share many of the properties of language models.\n",
            "In this lecture, we will explore the capabilities of GPT-3, the canonical large language model. We will closely follow the benchmarks from the GPT-3 paper, which include:\n",
            "In comparison with the state-of-the-art-result for each task, the results are mixed:\n",
            "The way to think about these results is as follows:\n",
            "Adaptation. Recall that a language model \\(p\\) is a distribution over sequences of tokens \\(x_{1:L}\\) and thus can be used to score sequences:\n",
            "It can also be used to perform conditional generation of a completion given a prompt:\n",
            "A task is a mapping from inputs to outputs. For example, for question answering, we might have:\n",
            "Input: What school did burne hogarth establish? Output: School of Visual Arts\n",
            "We use the term adaptation to refer to the process of taking a language model and turning it into a task model, given:\n",
            "There are two primary ways to perform adaptation:\n",
            "Which adaptation procedure should we go with?\n",
            "The GPT-3 paper evaluated GPT-3 on a large set of tasks. We will consider a subset of these, and for each task, discuss the following:\n",
            "Size and number of examples matters. By default, the results will based on\n",
            "Along the way, we will do ablations to see if model size and number of in-context training instances matters. Spoiler: it does and more is better.\n",
            "The tasks are grouped as follows:\n",
            "The goals of this lecture is to provide:\n",
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language.\n",
            "Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example:\n",
            "We can ask: what is the probability the language model assigns to it?\n",
            "Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule:\n",
            "Perplexity. The joint probability of a sequence depends on its length and thus goes to zero as the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.)\n",
            "Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don’t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn’t penalize you for that. Instead, we want the geometric average, which is exactly what perplexity does:\n",
            "Perplexity can be interpreted as the average “branching factor” per token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings.\n",
            "Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically:\n",
            "Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\):\n",
            "where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it’s just going to generate a gibberish token.\n",
            "Now let’s get on with evaluating perplexity on an actual dataset.\n",
            "The Penn Tree Bank is a classic dataset in NLP, originally annotated for syntactic parsing. Beginning with Emami and Jelinek (2004) and Mikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t to John Hewitt for pointing this out).\n",
            "Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo):\n",
            "Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29. Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group.\n",
            "Results. GPT-3 vastly outperforms the existing state-of-the-art:\n",
            "See the leaderboard for the latest results.\n",
            "Train/test leakage. The authors did not evaluate on some datasets such as WikiText-103 because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized.\n",
            "Adaptation.\n",
            "Fill in blank:  Alice was friends with Bob. Alice went to visit her friend ___. -> Bob  She held the torch in front of her. She caught her breath. “Chris? There’s a step.” “What?” “A step. Cut in the rock. About fifty feet ahead.” She moved faster. They both moved faster. “In fact,” she said, raising the torch higher, “there’s more than a ___. -> step\n",
            "Results. GPT-3 does much better on this task than the previous state-of-the-art (based on GPT-2):\n",
            "See the leaderboard for the latest results.\n",
            "Adaptation. This is a multiple-choice task, so the most natural thing to do is to score each candidate answer with the language model and predict the “best” one (demo):\n",
            "Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They ${answer}\n",
            "where ${answer} is one of:\n",
            "How do you score a candidate answer \\(y\\) given a question \\(x\\)? There’s no principled answer, but here are some heuristics:\n",
            "Unnormalized probability: \\(\\text{score}(x, y) = p(x, y)\\). The problem with the unnormalized probability is that it has a bias towards short answers (demo).\n",
            "Length-normalized probability: \\(\\text{score}(x, y) = \\frac{p(x, y)}{\\text{num-tokens}(y)}\\). This fixes the length bias. However, given two answers of the same length, the model still might prefer the more popular entity.\n",
            "Frequency-normalized probability: \\(\\text{score}(x, y) = \\frac{p(y \\mid x)}{p(y \\mid x_0)}\\), where \\(x_0\\) is a neutral string like \\(\\nl{Answer:}\\). This lowers the score for answers that happen to just be common (e.g., \\nl{John}). Compare demo versus demo.\n",
            "Results. GPT-3 got close but did not exceed the state-of-the-art:\n",
            "However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data!\n",
            "See the leaderboard for the latest results.\n",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. The language model has to somehow “know” the answer without looking up information in a database or a set of documents (we’ll consider reading comprehension later, where the information is provided).\n",
            "Input: What school did burne hogarth establish? Output: School of Visual Arts\n",
            "Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo):\n",
            "Q: ‘Nude Descending A Staircase’ is perhaps the most famous painting by which 20th century artist? A: Marcel Duchamp\n",
            "Results.\n",
            "We also see that both increasing the model size and the number of in-context training instances helps:\n",
            "\n",
            "Adaptation.\n",
            "We define a prompt the same as above (demo):\n",
            "Q: What school did burne hogarth establish? A: School of Visual Arts\n",
            "Results.\n",
            "Adaptation. We define a prompt the same as above (demo):\n",
            "Q: Who played tess on touched by an angel? A: Delloreese Patricia Early (July 6, 1931 - November 19, 2017), known professionally as Della Reese.\n",
            "Results.\n",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo):\n",
            "Mein Haus liegt auf dem Hügel. = My house is on the hill. Keinesfalls dürfen diese für den kommerziellen Gebrauch verwendet werden. = In no case may they be used for commercial purposes.\n",
            "Results. Here are the results from German to English:\n",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more “abstract reasoning” tasks, to evaluate GPT-3 as more of a general-purpose model.\n",
            "Adaptation. Pose the problem as question answering (demo):\n",
            "Q: What is 556 plus 497? A: 1053\n",
            "Results.\n",
            "\n",
            "It doesn’t work perfectly and can hardly be said to “understand arithmetic” fully, but it works surprisingly well.\n",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like.\n",
            "Title: United Methodists Agree to Historic Split Subtitle: Those who oppose gay marriage will form their own denomination Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church's annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination...\n",
            "Results. Humans were able to able to detect classify “human” versus “machine” only 52% of the time (barely above random chance).\n",
            "For the article above, humans guessed “machine” correctly only 12% of the time.\n",
            "Adaptation. Just describe the task in the prompt (demo):\n",
            "To “screeg” something is to swing a sword at it. An example of a sentence that uses the word screeg is: We screeged the tree with our swords.\n",
            "Adaptation. The prompt consists of input-output pairs (demo):\n",
            "Poor English input: I eated the purple berries. Good English output: I ate the purple berries. Poor English input: Thank you for picking me as your designer. I’d appreciate it. Good English output: Thank you for choosing me as your designer. I appreciate it. Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications. Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications. Poor English input: I’d be more than happy to work with you in another project. Good English output: I would be happy to work with you on another project.\n",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list.\n",
            "Benchmarks.\n",
            "The performance on these benchmarks is still mediocre, but it’s perhaps not bad given that we’re doing few-shot learning!\n",
            "Demos.\n",
            "The demos are creative and interesting, but it’s hard to tell how reliably they work.\n",
            "\\(\\newcommand{\\sV}{\\mathcal{V}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\x}{x_{1:L}} \\newcommand{\\softmax}{\\text{softmax}} \\newcommand{\\EmbedToken}{\\text{EmbedToken}} \\newcommand{\\SequenceModel}{\\text{SequenceModel}} \\newcommand{\\FeedForward}{\\text{FeedForward}} \\newcommand{\\FeedForwardSequenceModel}{\\text{FeedForwardSequenceModel}} \\newcommand{\\SequenceRNN}{\\text{SequenceRNN}} \\newcommand{\\BidirectionalSequenceRNN}{\\text{BidirectionalSequenceRNN}} \\newcommand{\\RNN}{\\text{RNN}} \\newcommand{\\SimpleRNN}{\\text{SimpleRNN}} \\newcommand{\\LSTM}{\\text{LSTM}} \\newcommand{\\GRU}{\\text{GRU}} \\newcommand{\\Attention}{\\text{Attention}} \\newcommand{\\MultiHeadedAttention}{\\text{MultiHeadedAttention}} \\newcommand{\\SelfAttention}{\\text{SelfAttention}} \\newcommand{\\TransformerBlock}{\\text{TransformerBlock}} \\newcommand{\\EmbedTokenWithPosition}{\\text{EmbedTokenWithPosition}} \\newcommand{\\LayerNorm}{\\text{LayerNorm}} \\newcommand{\\AddNorm}{\\text{AddNorm}} \\newcommand{\\nl}[1]{\\textsf{#1}} \\newcommand{\\generate}[1]{\\stackrel{#1}{\\rightsquigarrow}} \\newcommand{\\embed}{\\stackrel{\\phi}{\\Rightarrow}}\\) We started this course by analyzing a language model as a black box:\n",
            "Then we looked at the training data of large language models (e.g., The Pile):\n",
            "In this lecture, we will open up the onion all the way and talk about how large language models are built.\n",
            "Today’s lecture will focus on two topics, tokenization and model architecture.\n",
            "Tokenization: how a string is split into tokens.\n",
            "Model architecture: We will discuss mostly the Transformer architecture, which is the modeling innovation that really enabled large language models.\n",
            "Recall that a language model \\(p\\) is a probability distribution over a sequence of tokens where each token comes from some vocabulary \\(\\sV\\):\n",
            "However, natural language doesn’t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters):\n",
            "A tokenizer converts any string into a sequence of tokens.\n",
            "This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work.\n",
            "The simplest solution is to do:\n",
            "text.split(' ')\n",
            "我今天去了商店。 [gloss: I went to the store.]\n",
            "Then there are languages like German that have long compound words (e.g., Abwasserbehandlungsanlange).\n",
            "Even in English, there are hyphenated words (e.g., father-in-law) and contractions (e.g., don’t), which should get split up. For example, the Penn Treebank splits don’t into do and n’t, a linguistically-informed but not obvious choice.\n",
            "Therefore, splitting by spaces by spaces to identify words is quite problematic.\n",
            "What makes a good tokenization?\n",
            "Sennrich et al, 2015 applied the byte pair encoding (BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers.\n",
            "Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot.\n",
            "Example:\n",
            "The output of learning is:\n",
            "Applying the tokenizer. To tokenize a new string, apply the merges in the same order:\n",
            "Unicode.\n",
            "今天 [gloss: today] [x62, x11, 4e, ca]\n",
            "Rather than just splitting by frequency, a more “principled” approach is to define an objective function that captures what a good tokenization looks like. We now describe the unigram model (Kudo 2018).\n",
            "Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of\n",
            "Example:\n",
            "Algorithm:\n",
            "Impact:\n",
            "Examples of tokenizations for both GPT-3 and Jurassic (demo):\n",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence.\n",
            "Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings:\n",
            "We will broaden our notion of language models to three types of models.\n",
            "Encoder-only (BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text.\n",
            "These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks).\n",
            "Decoder-only (GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)).\n",
            "Encoder-decoder (BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\).\n",
            "We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\):\n",
            "We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on these slides from CS221 on differentiable programming, and will depart a bit from the standard presentation.\n",
            "The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity:\n",
            "This function will have parameters which we will include in the body but elide in the function signature for simplicity.\n",
            "In what follows, we will define a library of building blocks until we get to the full Transformer.\n",
            "First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data):\n",
            "def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\):\n",
            "These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes these context-independent embeddings and maps them into contextual embeddings.\n",
            "def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to a fixed length context, just as in an n-gram model:\n",
            "def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "The first “real” sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence of hidden states recursively.\n",
            "def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state:\n",
            "def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\):\n",
            "There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNN Elman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)).\n",
            "def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\):\n",
            "As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used by ELMo and ULMFiT.\n",
            "def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\):\n",
            "Notes:\n",
            "We will not discuss these models in the interest of time.\n",
            "Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models.\n",
            "There are great resources for learning about the Transformer:\n",
            "You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces.\n",
            "The crux of the Transformers are the attention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017).\n",
            "One can think of attention as a “soft” lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\):\n",
            "We can think of each \\(x_i\\) as representing a key-value pair via linear transformations:\n",
            "and forming the query via another linear transformation:\n",
            "The key and the query can be compared to give a score:\n",
            "These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\):\n",
            "Then the final output is a weighted combination over the values:\n",
            "We can write this all succinctly in matrix form:\n",
            "def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\):\n",
            "We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multiple attention heads and simply combine their outputs.\n",
            "def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\)\n",
            "Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce:\n",
            "def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\):\n",
            "Feedforward layer. Self-attention allows all the tokens to “talk” to each other, whereas feedforward connections provide:\n",
            "def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "Improving trainability. We’re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable.\n",
            "Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\):\n",
            "we add a residual (skip) connection so that if \\(f\\)’s gradients vanish, gradients can still flow through \\(\\x\\):\n",
            "Layer normalization. Another trick is layer normalization, which takes a takes a vector and makes sure its elements are too big:\n",
            "def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "We first define an adapter function that takes a sequence model \\(f\\) and makes it “robust”:\n",
            "def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "Finally, we can define the Transformer block succinctly as follows:\n",
            "def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "Positional embeddings. You might have noticed that as defined, the embedding of a token doesn’t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible.\n",
            "To fix this, we add positional information into the embedding:\n",
            "def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\):\n",
            "GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times:\n",
            "Shape of the architecture (how the 175 billion parameters are allocated):\n",
            "These decisions are not necessarily optimal. Levine et al. 2020 provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture.\n",
            "There are important but detailed differences between different versions of Transformers:\n",
            "Tokenization:\n",
            "Modeling:\n",
            "CS224N slides on Transformers\n",
            "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. Ofir Press, Noah A. Smith, M. Lewis. 2021. Introduces Alibi embeddings.\n",
            "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov. ACL 2019. Introduces recurrence on Transformers, relative position encoding scheme.\n",
            "Decoder-only architectures:\n",
            "Encoder-only architectures:\n",
            "Encoder-decoder architectures:\n",
            "So far, we’ve talked about the behavior (capabilities and harms) of large language models. Now, we peel open the first layer of the onion and start discussing how these models are constructed. The starting point of any machine learning approach is training data, so this is where we’ll start.\n",
            "Aside: Normally in machine learning, the training data and the test (evaluation) data are similar or at least of the same type. But for large language models, the training data is just “raw text”.\n",
            "In the rest of the lecture, we’ll talk about:\n",
            "Recall that large language models are trained on “raw text”. To be highly capable (e.g., have linguistic and world knowledge), this text should span a broad range of domains, genres, languages, etc.\n",
            "A natural place (but not the only place) to look for such text is the web, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and the Deep Web is even larger than that.\n",
            "It is worth noting that private datasets that reside in big companies are even larger than what’s available publicly. For example, WalMart generates 2.5 petabytes of data each hour!\n",
            "Common Crawl is a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot of Common Crawl has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index.\n",
            "Representation. Despite the richness of web data, it has been noted in Bender et al, 2021 that:\n",
            "Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models.\n",
            "WebText. The WebText dataset was used to train GPT-2.\n",
            "OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by the OpenWebText dataset.\n",
            "Toxicity analysis. Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found:\n",
            "The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model.\n",
            "Analysis. Dodge et al. 2021 performed a thorough analysis of the C4 dataset.\n",
            "Documentation levels:\n",
            "Note: Raffel et al. 2020 only provided scripts to recreate; cost thousands of dollars just to run these scripts.\n",
            "\n",
            "Benchmark data contamination.\n",
            "Example from the XSum summarization dataset:\n",
            "Input: The 48-year-old former Arsenal goalkeeper played for the Royals for four years. He was appointed youth academy director in 2000 and has been director of football since 2003. A West Brom statement said: “He played a key role in the Championship club twice winning promotion to the Premier League in 2006 and 2012. Output: West Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading.\n",
            "There are two types of contamination:\n",
            "Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage).\n",
            "The dataset could also be responsible for various harms:\n",
            "\n",
            "\n",
            "Data composition.\n",
            "\n",
            "Compare:\n",
            "\n",
            "Takeaway: The Pile contains a lot of information that’s not well covered by GPT-3’s dataset.\n",
            "They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.\n",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data.\n",
            "Two purposes:\n",
            "Dataset lifecycle (a sample of the questions from each category are provided below):\n",
            "Data statements. The data statements work is specialized to NLP datasets, and covers other aspects:\n",
            "As an example, let’s look at the datasheet for The Pile.\n",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles.\n",
            "Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant in industry.\n",
            "\n",
            "Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.\n",
            "Documentation for datasets:\n",
            "Datasets:\n",
            "Analysis of datasets:\n",
            "Filtering datasets:\n",
            "Data ecosystems:\n",
            "\\(\\newcommand{\\sV}{\\mathcal{V}} \\newcommand{\\sO}{\\mathcal{O}} \\newcommand{\\sD}{\\mathcal{D}} \\newcommand{\\sN}{\\mathcal{N}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\x}{x_{1:L}} \\newcommand{\\tx}{\\tilde x_{1:L}} \\newcommand{\\nl}[1]{\\textsf{#1}} \\newcommand{\\softmax}{\\text{softmax}} \\newcommand{\\TransformerBlock}{\\text{TransformerBlock}} \\newcommand{\\EmbedTokenWithPosition}{\\text{EmbedTokenWithPosition}} \\newcommand{\\SentenceEmbedding}{\\text{SentenceEmbedding}} \\newcommand{\\BERT}{\\text{BERT}} \\newcommand{\\MASK}{\\nl{[MASK]}} \\newcommand{\\SEP}{\\nl{[SEP]}} \\newcommand{\\CLS}{\\nl{[CLS]}} \\newcommand{\\generate}[1]{\\stackrel{#1}{\\rightsquigarrow}} \\newcommand{\\embed}{\\stackrel{\\phi}{\\Rightarrow}}\\) Last lecture, we talked about the model architecture for large language models (e.g., the Transformer). In this lecture, we will discuss how to train large language models.\n",
            "We will consider objective functions for the three types of language models:\n",
            "We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers):\n",
            "Recall that an autoregressive language model defines a conditional distribution:\n",
            "We define it as follows:\n",
            "Succinctly:\n",
            "Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models.\n",
            "Let \\(\\sD\\) be the training data consisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function:\n",
            "There’s more to say about how to efficiently optimize this function, but that’s all there is for the objective.\n",
            "Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don’t need to generate.\n",
            "BERT. We will first present the BERT objective function, which contains two terms:\n",
            "Take the example sequence for natural language inference (predict entailment, contradiction, or neutral):\n",
            "There are two special tokens:\n",
            "Using our notation from the previous lecture, the BERT model is defined as:\n",
            "where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:\n",
            "\n",
            "BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters.\n",
            "Masked language modeling. The basic idea of the masked language model is to train on the prediction problem:\n",
            "More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\).\n",
            "Model. We first define the model distribution that takes \\(\\tx\\) and predicts each token independently given the contextual embedding:\n",
            "Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that:\n",
            "Here’s how \\(A\\) is defined:\n",
            "Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then:\n",
            "Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not.\n",
            "\\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\).\n",
            "\\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\).\n",
            "We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision.\n",
            "Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows:\n",
            "Objective. Then the BERT objective is:\n",
            "We will talk about training later, but a few quick notes about BERT:\n",
            "RoBERTa makes the following changes to BERT:\n",
            "Example task (table-to-text generation):\n",
            "Recall that encoder-decoder models (e.g., BART, T5):\n",
            "BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model.\n",
            "BART considers the following transformations \\(A(\\tx \\mid \\x)\\):  Based on BERT-scaled experiments, they decided on the following transformations for the final model:\n",
            "They demonstrated strong results on both classification and generation tasks using fine-tuning.\n",
            "T5 (Text-to-Text Transfer Transformer).\n",
            "T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model.\n",
            "Tasks:\n",
            "This paper experimented with many different unsupervised objectives:  and found that the “i.i.d. noise, replace spans” worked well (though many objectives were similar).\n",
            "They also cast all classical NLP tasks in a uniform framework as “text-to-text” tasks:  Note the difference in approach to classification tasks:\n",
            "Notes:\n",
            "Now we turn our attention to how to optimize the objective. For simplicity, let’s take autogressive language modeling:\n",
            "Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches:\n",
            "The key concerns in optimization are:\n",
            "There are several levels that we can approach optimization:\n",
            "ADAM (adaptive moment estimation). ADAM incorporates two ideas:\n",
            "Have an adaptive (different) step size for each dimension of \\(\\theta\\) (inspiration from second-order methods).\n",
            "Updating parameters.\n",
            "Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)).\n",
            "AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint.\n",
            "Mixed-precision training is another method for reducing memory (Narang et al., 2018).\n",
            "\n",
            "Learning rates.\n",
            "Initialization.\n",
            "For GPT-3:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Attention Is All You Need\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1 Introduction\n",
            "2 Background\n",
            "\n",
            "3 Model Architecture\n",
            "\n",
            "\n",
            "3.1 Encoder and Decoder Stacks\n",
            "\n",
            "Encoder:\n",
            "Decoder:\n",
            "\n",
            "\n",
            "\n",
            "3.2 Attention\n",
            "\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "3.2.2 Multi-Head Attention\n",
            "3.2.3 Applications of Attention in our Model\n",
            "\n",
            "\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "3.4 Embeddings and Softmax\n",
            "3.5 Positional Encoding\n",
            "\n",
            "\n",
            "4 Why Self-Attention\n",
            "\n",
            "5 Training\n",
            "\n",
            "5.1 Training Data and Batching\n",
            "5.2 Hardware and Schedule\n",
            "5.3 Optimizer\n",
            "\n",
            "5.4 Regularization\n",
            "\n",
            "Residual Dropout\n",
            "Label Smoothing\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6 Results\n",
            "\n",
            "6.1 Machine Translation\n",
            "6.2 Model Variations\n",
            "6.3 English Constituency Parsing\n",
            "\n",
            "\n",
            "\n",
            "7 Conclusion\n",
            "\n",
            "Acknowledgements\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
            "\n",
            "Attention Is All You Need\n",
            "\n",
            "\n",
            "\n",
            "\\ANDAshish Vaswani\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "&Noam Shazeer11footnotemark: 1\n",
            "Google Brain\n",
            "noam@google.com\n",
            "&Niki Parmar11footnotemark: 1\n",
            "Google Research\n",
            "nikip@google.com\n",
            "&Jakob Uszkoreit11footnotemark: 1\n",
            "Google Research\n",
            "usz@google.com\n",
            "&Llion Jones11footnotemark: 1\n",
            "Google Research\n",
            "llion@google.com\n",
            "&Aidan N. Gomez11footnotemark: 1   \n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "&Łukasz Kaiser11footnotemark: 1\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "&Illia Polosukhin11footnotemark: 1   \n",
            "illia.polosukhin@gmail.com\n",
            "\n",
            "Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.\n",
            "Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
            "Work performed while at Google Brain.Work performed while at Google Research.\n",
            "\n",
            "\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
            "\n",
            "\n",
            "\n",
            "1 Introduction\n",
            "\n",
            "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n",
            "\n",
            "\n",
            "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states htsubscriptℎ𝑡h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, as a function of the previous hidden state ht−1subscriptℎ𝑡1h_{t-1}italic_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT and the input for position t𝑡titalic_t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n",
            "Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
            "\n",
            "\n",
            "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
            "\n",
            "\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2 Background\n",
            "\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
            "\n",
            "\n",
            "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
            "\n",
            "\n",
            "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n",
            "\n",
            "\n",
            "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
            "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3 Model Architecture\n",
            "\n",
            "Figure 1: The Transformer - model architecture.\n",
            "\n",
            "\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,…,xn)subscript𝑥1…subscript𝑥𝑛(x_{1},...,x_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) to a sequence of continuous representations 𝐳=(z1,…,zn)𝐳subscript𝑧1…subscript𝑧𝑛\\mathbf{z}=(z_{1},...,z_{n})bold_z = ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ). Given 𝐳𝐳\\mathbf{z}bold_z, the decoder then generates an output sequence (y1,…,ym)subscript𝑦1…subscript𝑦𝑚(y_{1},...,y_{m})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
            "\n",
            "\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n",
            "\n",
            "\n",
            "\n",
            "3.1 Encoder and Decoder Stacks\n",
            "\n",
            "Encoder:\n",
            "\n",
            "The encoder is composed of a stack of N=6𝑁6N=6italic_N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm⁢(x+Sublayer⁢(x))LayerNorm𝑥Sublayer𝑥\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x))roman_LayerNorm ( italic_x + roman_Sublayer ( italic_x ) ), where Sublayer⁢(x)Sublayer𝑥\\mathrm{Sublayer}(x)roman_Sublayer ( italic_x ) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512subscript𝑑model512d_{\\text{model}}=512italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 512.\n",
            "\n",
            "\n",
            "\n",
            "Decoder:\n",
            "\n",
            "The decoder is also composed of a stack of N=6𝑁6N=6italic_N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i𝑖iitalic_i can depend only on the known outputs at positions less than i𝑖iitalic_i.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.2 Attention\n",
            "\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
            "\n",
            "\n",
            "\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, and values of dimension dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. We compute the dot products of the query with all keys, divide each by dksubscript𝑑𝑘\\sqrt{d_{k}}square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG, and apply a softmax function to obtain the weights on the values.\n",
            "\n",
            "\n",
            "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q𝑄Qitalic_Q. The keys and values are also packed together into matrices K𝐾Kitalic_K and V𝑉Vitalic_V. We compute the matrix of outputs as:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Attention⁢(Q,K,V)=softmax⁢(Q⁢KTdk)⁢VAttention𝑄𝐾𝑉softmax𝑄superscript𝐾𝑇subscript𝑑𝑘𝑉\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})Vroman_Attention ( italic_Q , italic_K , italic_V ) = roman_softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_V\n",
            "\n",
            "(1)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1dk1subscript𝑑𝑘\\frac{1}{\\sqrt{d_{k}}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
            "\n",
            "\n",
            "While for small values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT [3]. We suspect that for large values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of q𝑞qitalic_q and k𝑘kitalic_k are independent random variables with mean 00 and variance 1111. Then their dot product, q⋅k=∑i=1dkqi⁢ki⋅𝑞𝑘superscriptsubscript𝑖1subscript𝑑𝑘subscript𝑞𝑖subscript𝑘𝑖q\\cdot k=\\sum_{i=1}^{d_{k}}q_{i}k_{i}italic_q ⋅ italic_k = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, has mean 00 and variance dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT.. To counteract this effect, we scale the dot products by 1dk1subscript𝑑𝑘\\frac{1}{\\sqrt{d_{k}}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.2.2 Multi-Head Attention\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Scaled Dot-Product Attention\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Multi-Head Attention\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
            "\n",
            "\n",
            "Instead of performing a single attention function with dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values hℎhitalic_h times with different, learned linear projections to dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT dimensions, respectively.\n",
            "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
            "\n",
            "\n",
            "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MultiHead⁢(Q,K,V)MultiHead𝑄𝐾𝑉\\displaystyle\\mathrm{MultiHead}(Q,K,V)roman_MultiHead ( italic_Q , italic_K , italic_V )\n",
            "=Concat⁢(head1,…,headh)⁢WOabsentConcatsubscripthead1…subscriptheadhsuperscript𝑊𝑂\\displaystyle=\\mathrm{Concat}(\\mathrm{head_{1}},...,\\mathrm{head_{h}})W^{O}= roman_Concat ( roman_head start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , roman_head start_POSTSUBSCRIPT roman_h end_POSTSUBSCRIPT ) italic_W start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "where⁢headiwheresubscriptheadi\\displaystyle\\text{where}~{}\\mathrm{head_{i}}where roman_head start_POSTSUBSCRIPT roman_i end_POSTSUBSCRIPT\n",
            "=Attention⁢(Q⁢WiQ,K⁢WiK,V⁢WiV)absentAttention𝑄subscriptsuperscript𝑊𝑄𝑖𝐾subscriptsuperscript𝑊𝐾𝑖𝑉subscriptsuperscript𝑊𝑉𝑖\\displaystyle=\\mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})= roman_Attention ( italic_Q italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_K italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_V italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Where the projections are parameter matrices WiQ∈ℝdmodel×dksubscriptsuperscript𝑊𝑄𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑘W^{Q}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WiK∈ℝdmodel×dksubscriptsuperscript𝑊𝐾𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑘W^{K}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WiV∈ℝdmodel×dvsubscriptsuperscript𝑊𝑉𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑣W^{V}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{v}}italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and WO∈ℝh⁢dv×dmodelsuperscript𝑊𝑂superscriptℝℎsubscript𝑑𝑣subscript𝑑modelW^{O}\\in\\mathbb{R}^{hd_{v}\\times d_{\\text{model}}}italic_W start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_h italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT.\n",
            "\n",
            "\n",
            "In this work we employ h=8ℎ8h=8italic_h = 8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64subscript𝑑𝑘subscript𝑑𝑣subscript𝑑modelℎ64d_{k}=d_{v}=d_{\\text{model}}/h=64italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT / italic_h = 64.\n",
            "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.2.3 Applications of Attention in our Model\n",
            "\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "\n",
            "\n",
            "•\n",
            "\n",
            "In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
            "\n",
            "\n",
            "\n",
            "•\n",
            "\n",
            "The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
            "\n",
            "\n",
            "\n",
            "•\n",
            "\n",
            "Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞-\\infty- ∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "\n",
            "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FFN⁢(x)=max⁡(0,x⁢W1+b1)⁢W2+b2FFN𝑥0𝑥subscript𝑊1subscript𝑏1subscript𝑊2subscript𝑏2\\mathrm{FFN}(x)=\\max(0,xW_{1}+b_{1})W_{2}+b_{2}roman_FFN ( italic_x ) = roman_max ( 0 , italic_x italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT\n",
            "\n",
            "(2)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel=512subscript𝑑model512d_{\\text{model}}=512italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 512, and the inner-layer has dimensionality df⁢f=2048subscript𝑑𝑓𝑓2048d_{ff}=2048italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT = 2048.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.4 Embeddings and Softmax\n",
            "\n",
            "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodelsubscript𝑑model\\sqrt{d_{\\text{model}}}square-root start_ARG italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_ARG.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.5 Positional Encoding\n",
            "\n",
            "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
            "\n",
            "\n",
            "In this work, we use sine and cosine functions of different frequencies:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "P⁢E(p⁢o⁢s,2⁢i)=s⁢i⁢n⁢(p⁢o⁢s/100002⁢i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖𝑠𝑖𝑛𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\text{model}}})italic_P italic_E start_POSTSUBSCRIPT ( italic_p italic_o italic_s , 2 italic_i ) end_POSTSUBSCRIPT = italic_s italic_i italic_n ( italic_p italic_o italic_s / 10000 start_POSTSUPERSCRIPT 2 italic_i / italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "P⁢E(p⁢o⁢s,2⁢i+1)=c⁢o⁢s⁢(p⁢o⁢s/100002⁢i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖1𝑐𝑜𝑠𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\text{model}}})italic_P italic_E start_POSTSUBSCRIPT ( italic_p italic_o italic_s , 2 italic_i + 1 ) end_POSTSUBSCRIPT = italic_c italic_o italic_s ( italic_p italic_o italic_s / 10000 start_POSTSUPERSCRIPT 2 italic_i / italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "where p⁢o⁢s𝑝𝑜𝑠positalic_p italic_o italic_s is the position and i𝑖iitalic_i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2⁢π2𝜋2\\pi2 italic_π to 10000⋅2⁢π⋅100002𝜋10000\\cdot 2\\pi10000 ⋅ 2 italic_π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k𝑘kitalic_k, P⁢Ep⁢o⁢s+k𝑃subscript𝐸𝑝𝑜𝑠𝑘PE_{pos+k}italic_P italic_E start_POSTSUBSCRIPT italic_p italic_o italic_s + italic_k end_POSTSUBSCRIPT can be represented as a linear function of P⁢Ep⁢o⁢s𝑃subscript𝐸𝑝𝑜𝑠PE_{pos}italic_P italic_E start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT.\n",
            "\n",
            "\n",
            "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4 Why Self-Attention\n",
            "\n",
            "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1,…,xn)subscript𝑥1…subscript𝑥𝑛(x_{1},...,x_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) to another sequence of equal length (z1,…,zn)subscript𝑧1…subscript𝑧𝑛(z_{1},...,z_{n})( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ), with xi,zi∈ℝdsubscript𝑥𝑖subscript𝑧𝑖superscriptℝ𝑑x_{i},z_{i}\\in\\mathbb{R}^{d}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
            "\n",
            "\n",
            "One is the total computational complexity per layer.\n",
            "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
            "\n",
            "\n",
            "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
            "\n",
            "\n",
            "Table 1: \n",
            "Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n𝑛nitalic_n is the sequence length, d𝑑ditalic_d is the representation dimension, k𝑘kitalic_k is the kernel size of convolutions and r𝑟ritalic_r the size of the neighborhood in restricted self-attention.\n",
            "\n",
            "\n",
            "\n",
            "Layer Type\n",
            "Complexity per Layer\n",
            "Sequential\n",
            "Maximum Path Length\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Operations\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Self-Attention\n",
            "O⁢(n2⋅d)𝑂⋅superscript𝑛2𝑑O(n^{2}\\cdot d)italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⋅ italic_d )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "\n",
            "\n",
            "Recurrent\n",
            "O⁢(n⋅d2)𝑂⋅𝑛superscript𝑑2O(n\\cdot d^{2})italic_O ( italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )\n",
            "O⁢(n)𝑂𝑛O(n)italic_O ( italic_n )\n",
            "O⁢(n)𝑂𝑛O(n)italic_O ( italic_n )\n",
            "\n",
            "\n",
            "Convolutional\n",
            "O⁢(k⋅n⋅d2)𝑂⋅𝑘𝑛superscript𝑑2O(k\\cdot n\\cdot d^{2})italic_O ( italic_k ⋅ italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "O⁢(l⁢o⁢gk⁢(n))𝑂𝑙𝑜subscript𝑔𝑘𝑛O(log_{k}(n))italic_O ( italic_l italic_o italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_n ) )\n",
            "\n",
            "\n",
            "Self-Attention (restricted)\n",
            "O⁢(r⋅n⋅d)𝑂⋅𝑟𝑛𝑑O(r\\cdot n\\cdot d)italic_O ( italic_r ⋅ italic_n ⋅ italic_d )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "O⁢(n/r)𝑂𝑛𝑟O(n/r)italic_O ( italic_n / italic_r )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) sequential operations.\n",
            "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n𝑛nitalic_n is smaller than the representation dimensionality d𝑑ditalic_d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations.\n",
            "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r𝑟ritalic_r in the input sequence centered around the respective output position. This would increase the maximum path length to O⁢(n/r)𝑂𝑛𝑟O(n/r)italic_O ( italic_n / italic_r ). We plan to investigate this approach further in future work.\n",
            "\n",
            "\n",
            "A single convolutional layer with kernel width k<n𝑘𝑛k<nitalic_k < italic_n does not connect all pairs of input and output positions. Doing so requires a stack of O⁢(n/k)𝑂𝑛𝑘O(n/k)italic_O ( italic_n / italic_k ) convolutional layers in the case of contiguous kernels, or O⁢(l⁢o⁢gk⁢(n))𝑂𝑙𝑜subscript𝑔𝑘𝑛O(log_{k}(n))italic_O ( italic_l italic_o italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_n ) ) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network.\n",
            "Convolutional layers are generally more expensive than recurrent layers, by a factor of k𝑘kitalic_k. Separable convolutions [6], however, decrease the complexity considerably, to O⁢(k⋅n⋅d+n⋅d2)𝑂⋅𝑘𝑛𝑑⋅𝑛superscript𝑑2O(k\\cdot n\\cdot d+n\\cdot d^{2})italic_O ( italic_k ⋅ italic_n ⋅ italic_d + italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Even with k=n𝑘𝑛k=nitalic_k = italic_n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
            "\n",
            "\n",
            "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5 Training\n",
            "\n",
            "This section describes the training regime for our models.\n",
            "\n",
            "\n",
            "\n",
            "5.1 Training Data and Batching\n",
            "\n",
            "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.2 Hardware and Schedule\n",
            "\n",
            "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.3 Optimizer\n",
            "\n",
            "We used the Adam optimizer [20] with β1=0.9subscript𝛽10.9\\beta_{1}=0.9italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, β2=0.98subscript𝛽20.98\\beta_{2}=0.98italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.98 and ϵ=10−9italic-ϵsuperscript109\\epsilon=10^{-9}italic_ϵ = 10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT. We varied the learning rate over the course of training, according to the formula:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "l⁢r⁢a⁢t⁢e=dmodel−0.5⋅min⁡(s⁢t⁢e⁢p⁢_⁢n⁢u⁢m−0.5,s⁢t⁢e⁢p⁢_⁢n⁢u⁢m⋅w⁢a⁢r⁢m⁢u⁢p⁢_⁢s⁢t⁢e⁢p⁢s−1.5)𝑙𝑟𝑎𝑡𝑒⋅superscriptsubscript𝑑model0.5𝑠𝑡𝑒𝑝_𝑛𝑢superscript𝑚0.5⋅𝑠𝑡𝑒𝑝_𝑛𝑢𝑚𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝superscript𝑠1.5lrate=d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{%\n",
            "warmup\\_steps}^{-1.5})italic_l italic_r italic_a italic_t italic_e = italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT ⋅ roman_min ( italic_s italic_t italic_e italic_p _ italic_n italic_u italic_m start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT , italic_s italic_t italic_e italic_p _ italic_n italic_u italic_m ⋅ italic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s start_POSTSUPERSCRIPT - 1.5 end_POSTSUPERSCRIPT )\n",
            "\n",
            "(3)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "This corresponds to increasing the learning rate linearly for the first w⁢a⁢r⁢m⁢u⁢p⁢_⁢s⁢t⁢e⁢p⁢s𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝𝑠warmup\\_stepsitalic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used w⁢a⁢r⁢m⁢u⁢p⁢_⁢s⁢t⁢e⁢p⁢s=4000𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝𝑠4000warmup\\_steps=4000italic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s = 4000.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.4 Regularization\n",
            "\n",
            "We employ three types of regularization during training:\n",
            "\n",
            "\n",
            "Residual Dropout\n",
            "\n",
            "We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pd⁢r⁢o⁢p=0.1subscript𝑃𝑑𝑟𝑜𝑝0.1P_{drop}=0.1italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT = 0.1.\n",
            "\n",
            "\n",
            "\n",
            "Label Smoothing\n",
            "\n",
            "During training, we employed label smoothing of value ϵl⁢s=0.1subscriptitalic-ϵ𝑙𝑠0.1\\epsilon_{ls}=0.1italic_ϵ start_POSTSUBSCRIPT italic_l italic_s end_POSTSUBSCRIPT = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6 Results\n",
            "\n",
            "\n",
            "6.1 Machine Translation\n",
            "\n",
            "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. \n",
            "\n",
            "\n",
            "\n",
            "Model\n",
            "BLEU\n",
            "\n",
            "Training Cost (FLOPs)\n",
            "\n",
            "\n",
            "EN-DE\n",
            "EN-FR\n",
            "\n",
            "EN-DE\n",
            "EN-FR\n",
            "\n",
            "\n",
            "ByteNet [18]\n",
            "\n",
            "23.75\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Deep-Att + PosUnk [39]\n",
            "\n",
            "\n",
            "39.2\n",
            "\n",
            "\n",
            "1.0⋅1020⋅1.0superscript10201.0\\cdot 10^{20}1.0 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "GNMT + RL [38]\n",
            "\n",
            "24.6\n",
            "39.92\n",
            "\n",
            "2.3⋅1019⋅2.3superscript10192.3\\cdot 10^{19}2.3 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "1.4⋅1020⋅1.4superscript10201.4\\cdot 10^{20}1.4 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "ConvS2S [9]\n",
            "\n",
            "25.16\n",
            "40.46\n",
            "\n",
            "9.6⋅1018⋅9.6superscript10189.6\\cdot 10^{18}9.6 ⋅ 10 start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPT\n",
            "1.5⋅1020⋅1.5superscript10201.5\\cdot 10^{20}1.5 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "MoE [32]\n",
            "\n",
            "26.03\n",
            "40.56\n",
            "\n",
            "2.0⋅1019⋅2.0superscript10192.0\\cdot 10^{19}2.0 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "1.2⋅1020⋅1.2superscript10201.2\\cdot 10^{20}1.2 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "Deep-Att + PosUnk Ensemble [39]\n",
            "\n",
            "\n",
            "40.4\n",
            "\n",
            "\n",
            "8.0⋅1020⋅8.0superscript10208.0\\cdot 10^{20}8.0 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "GNMT + RL Ensemble [38]\n",
            "\n",
            "26.30\n",
            "41.16\n",
            "\n",
            "1.8⋅1020⋅1.8superscript10201.8\\cdot 10^{20}1.8 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "1.1⋅1021⋅1.1superscript10211.1\\cdot 10^{21}1.1 ⋅ 10 start_POSTSUPERSCRIPT 21 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "ConvS2S Ensemble [9]\n",
            "\n",
            "26.36\n",
            "41.29\n",
            "\n",
            "7.7⋅1019⋅7.7superscript10197.7\\cdot 10^{19}7.7 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "1.2⋅1021⋅1.2superscript10211.2\\cdot 10^{21}1.2 ⋅ 10 start_POSTSUPERSCRIPT 21 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "Transformer (base model)\n",
            "27.3\n",
            "38.1\n",
            "\n",
            "3.3⋅𝟏𝟎𝟏𝟖bold-⋅3.3superscript10183.3\\cdot 10^{18}bold_3.3 bold_⋅ bold_10 start_POSTSUPERSCRIPT bold_18 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "Transformer (big)\n",
            "28.4\n",
            "41.8\n",
            "\n",
            "2.3⋅1019⋅2.3superscript10192.3\\cdot 10^{19}2.3 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.02.02.02.0 BLEU, establishing a new state-of-the-art BLEU score of 28.428.428.428.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.53.53.53.5 days on 8888 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
            "\n",
            "\n",
            "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.041.041.041.0, outperforming all of the previously published single models, at less than 1/4141/41 / 4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pd⁢r⁢o⁢p=0.1subscript𝑃𝑑𝑟𝑜𝑝0.1P_{drop}=0.1italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT = 0.1, instead of 0.30.30.30.3.\n",
            "\n",
            "\n",
            "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4444 and length penalty α=0.6𝛼0.6\\alpha=0.6italic_α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50505050, but terminate early when possible [38].\n",
            "\n",
            "\n",
            "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 222We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6.2 Model Variations\n",
            "\n",
            "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "N𝑁Nitalic_N\n",
            "dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT\n",
            "dffsubscript𝑑ffd_{\\text{ff}}italic_d start_POSTSUBSCRIPT ff end_POSTSUBSCRIPT\n",
            "hℎhitalic_h\n",
            "dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT\n",
            "dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT\n",
            "Pd⁢r⁢o⁢psubscript𝑃𝑑𝑟𝑜𝑝P_{drop}italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT\n",
            "ϵl⁢ssubscriptitalic-ϵ𝑙𝑠\\epsilon_{ls}italic_ϵ start_POSTSUBSCRIPT italic_l italic_s end_POSTSUBSCRIPT\n",
            "train\n",
            "PPL\n",
            "BLEU\n",
            "params\n",
            "\n",
            "\n",
            "\n",
            "steps\n",
            "(dev)\n",
            "(dev)\n",
            "×106absentsuperscript106\\times 10^{6}× 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "base\n",
            "6\n",
            "512\n",
            "2048\n",
            "8\n",
            "64\n",
            "64\n",
            "0.1\n",
            "0.1\n",
            "100K\n",
            "4.92\n",
            "25.8\n",
            "65\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(A)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "512\n",
            "512\n",
            "\n",
            "\n",
            "\n",
            "5.29\n",
            "24.9\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4\n",
            "128\n",
            "128\n",
            "\n",
            "\n",
            "\n",
            "5.00\n",
            "25.5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16\n",
            "32\n",
            "32\n",
            "\n",
            "\n",
            "\n",
            "4.91\n",
            "25.8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "32\n",
            "16\n",
            "16\n",
            "\n",
            "\n",
            "\n",
            "5.01\n",
            "25.4\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(B)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.16\n",
            "25.1\n",
            "58\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "32\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.01\n",
            "25.4\n",
            "60\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(C)\n",
            "\n",
            "2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6.11\n",
            "23.7\n",
            "36\n",
            "\n",
            "\n",
            "4\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.19\n",
            "25.3\n",
            "50\n",
            "\n",
            "\n",
            "8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4.88\n",
            "25.5\n",
            "80\n",
            "\n",
            "\n",
            "\n",
            "256\n",
            "\n",
            "\n",
            "32\n",
            "32\n",
            "\n",
            "\n",
            "\n",
            "5.75\n",
            "24.5\n",
            "28\n",
            "\n",
            "\n",
            "\n",
            "1024\n",
            "\n",
            "\n",
            "128\n",
            "128\n",
            "\n",
            "\n",
            "\n",
            "4.66\n",
            "26.0\n",
            "168\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.12\n",
            "25.4\n",
            "53\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4096\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4.75\n",
            "26.2\n",
            "90\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(D)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.0\n",
            "\n",
            "\n",
            "5.77\n",
            "24.6\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.2\n",
            "\n",
            "\n",
            "4.95\n",
            "25.5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.0\n",
            "\n",
            "4.67\n",
            "25.3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.2\n",
            "\n",
            "5.47\n",
            "25.7\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(E)\n",
            "\n",
            "positional embedding instead of sinusoids\n",
            "\n",
            "4.92\n",
            "25.7\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "big\n",
            "6\n",
            "1024\n",
            "4096\n",
            "16\n",
            "\n",
            "\n",
            "0.3\n",
            "\n",
            "300K\n",
            "4.33\n",
            "26.4\n",
            "213\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
            "\n",
            "\n",
            "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
            "\n",
            "\n",
            "In Table 3 rows (B), we observe that reducing the attention key size dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6.3 English Constituency Parsing\n",
            "\n",
            "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
            "\n",
            "\n",
            "\n",
            "Parser\n",
            "Training\n",
            "WSJ 23 F1\n",
            "\n",
            "\n",
            "Vinyals & Kaiser el al. (2014) [37]\n",
            "\n",
            "WSJ only, discriminative\n",
            "88.3\n",
            "\n",
            "\n",
            "Petrov et al. (2006) [29]\n",
            "\n",
            "WSJ only, discriminative\n",
            "90.4\n",
            "\n",
            "\n",
            "Zhu et al. (2013) [40]\n",
            "\n",
            "WSJ only, discriminative\n",
            "90.4\n",
            "\n",
            "\n",
            "Dyer et al. (2016) [8]\n",
            "\n",
            "WSJ only, discriminative\n",
            "91.7\n",
            "\n",
            "\n",
            "Transformer (4 layers)\n",
            "WSJ only, discriminative\n",
            "91.3\n",
            "\n",
            "\n",
            "Zhu et al. (2013) [40]\n",
            "\n",
            "semi-supervised\n",
            "91.3\n",
            "\n",
            "\n",
            "Huang & Harper (2009) [14]\n",
            "\n",
            "semi-supervised\n",
            "91.3\n",
            "\n",
            "\n",
            "McClosky et al. (2006) [26]\n",
            "\n",
            "semi-supervised\n",
            "92.1\n",
            "\n",
            "\n",
            "Vinyals & Kaiser el al. (2014) [37]\n",
            "\n",
            "semi-supervised\n",
            "92.1\n",
            "\n",
            "\n",
            "Transformer (4 layers)\n",
            "semi-supervised\n",
            "92.7\n",
            "\n",
            "\n",
            "Luong et al. (2015) [23]\n",
            "\n",
            "multi-task\n",
            "93.0\n",
            "\n",
            "\n",
            "Dyer et al. (2016) [8]\n",
            "\n",
            "generative\n",
            "93.3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\n",
            "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
            "\n",
            "\n",
            "We trained a 4-layer transformer with dm⁢o⁢d⁢e⁢l=1024subscript𝑑𝑚𝑜𝑑𝑒𝑙1024d_{model}=1024italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
            "\n",
            "\n",
            "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300300300300. We used a beam size of 21212121 and α=0.3𝛼0.3\\alpha=0.3italic_α = 0.3 for both WSJ only and the semi-supervised setting.\n",
            "\n",
            "\n",
            "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
            "\n",
            "\n",
            "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "7 Conclusion\n",
            "\n",
            "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
            "\n",
            "\n",
            "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
            "\n",
            "\n",
            "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.\n",
            "Making generation less sequential is another research goals of ours.\n",
            "\n",
            "\n",
            "The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\n",
            "\n",
            "\n",
            "Acknowledgements\n",
            "\n",
            "We are grateful to Nal Kalchbrenner and Stephan Gouws for\n",
            "their fruitful comments, corrections and inspiration.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "References\n",
            "\n",
            "\n",
            "[1]\n",
            "\n",
            "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\n",
            "\n",
            "\n",
            "Layer normalization.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1607.06450, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[2]\n",
            "\n",
            "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\n",
            "\n",
            "\n",
            "Neural machine translation by jointly learning to align and\n",
            "translate.\n",
            "\n",
            "\n",
            "CoRR, abs/1409.0473, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[3]\n",
            "\n",
            "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le.\n",
            "\n",
            "\n",
            "Massive exploration of neural machine translation architectures.\n",
            "\n",
            "\n",
            "CoRR, abs/1703.03906, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[4]\n",
            "\n",
            "Jianpeng Cheng, Li Dong, and Mirella Lapata.\n",
            "\n",
            "\n",
            "Long short-term memory-networks for machine reading.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1601.06733, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[5]\n",
            "\n",
            "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger\n",
            "Schwenk, and Yoshua Bengio.\n",
            "\n",
            "\n",
            "Learning phrase representations using rnn encoder-decoder for\n",
            "statistical machine translation.\n",
            "\n",
            "\n",
            "CoRR, abs/1406.1078, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[6]\n",
            "\n",
            "Francois Chollet.\n",
            "\n",
            "\n",
            "Xception: Deep learning with depthwise separable convolutions.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1610.02357, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[7]\n",
            "\n",
            "Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua\n",
            "Bengio.\n",
            "\n",
            "\n",
            "Empirical evaluation of gated recurrent neural networks on sequence\n",
            "modeling.\n",
            "\n",
            "\n",
            "CoRR, abs/1412.3555, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[8]\n",
            "\n",
            "Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith.\n",
            "\n",
            "\n",
            "Recurrent neural network grammars.\n",
            "\n",
            "\n",
            "In Proc. of NAACL, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[9]\n",
            "\n",
            "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.\n",
            "\n",
            "\n",
            "Convolutional sequence to sequence learning.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1705.03122v2, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[10]\n",
            "\n",
            "Alex Graves.\n",
            "\n",
            "\n",
            "Generating sequences with recurrent neural networks.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1308.0850, 2013.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[11]\n",
            "\n",
            "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
            "\n",
            "\n",
            "Deep residual learning for image recognition.\n",
            "\n",
            "\n",
            "In Proceedings of the IEEE Conference on Computer Vision and\n",
            "Pattern Recognition, pages 770–778, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[12]\n",
            "\n",
            "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.\n",
            "\n",
            "\n",
            "Gradient flow in recurrent nets: the difficulty of learning long-term\n",
            "dependencies, 2001.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[13]\n",
            "\n",
            "Sepp Hochreiter and Jürgen Schmidhuber.\n",
            "\n",
            "\n",
            "Long short-term memory.\n",
            "\n",
            "\n",
            "Neural computation, 9(8):1735–1780, 1997.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[14]\n",
            "\n",
            "Zhongqiang Huang and Mary Harper.\n",
            "\n",
            "\n",
            "Self-training PCFG grammars with latent annotations across\n",
            "languages.\n",
            "\n",
            "\n",
            "In Proceedings of the 2009 Conference on Empirical Methods in\n",
            "Natural Language Processing, pages 832–841. ACL, August 2009.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[15]\n",
            "\n",
            "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.\n",
            "\n",
            "\n",
            "Exploring the limits of language modeling.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1602.02410, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[16]\n",
            "\n",
            "Łukasz Kaiser and Samy Bengio.\n",
            "\n",
            "\n",
            "Can active memory replace attention?\n",
            "\n",
            "\n",
            "In Advances in Neural Information Processing Systems, (NIPS),\n",
            "2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[17]\n",
            "\n",
            "Łukasz Kaiser and Ilya Sutskever.\n",
            "\n",
            "\n",
            "Neural GPUs learn algorithms.\n",
            "\n",
            "\n",
            "In International Conference on Learning Representations\n",
            "(ICLR), 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[18]\n",
            "\n",
            "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex\n",
            "Graves, and Koray Kavukcuoglu.\n",
            "\n",
            "\n",
            "Neural machine translation in linear time.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1610.10099v2, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[19]\n",
            "\n",
            "Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush.\n",
            "\n",
            "\n",
            "Structured attention networks.\n",
            "\n",
            "\n",
            "In International Conference on Learning Representations, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[20]\n",
            "\n",
            "Diederik Kingma and Jimmy Ba.\n",
            "\n",
            "\n",
            "Adam: A method for stochastic optimization.\n",
            "\n",
            "\n",
            "In ICLR, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[21]\n",
            "\n",
            "Oleksii Kuchaiev and Boris Ginsburg.\n",
            "\n",
            "\n",
            "Factorization tricks for LSTM networks.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1703.10722, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[22]\n",
            "\n",
            "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
            "Zhou, and Yoshua Bengio.\n",
            "\n",
            "\n",
            "A structured self-attentive sentence embedding.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1703.03130, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[23]\n",
            "\n",
            "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.\n",
            "\n",
            "\n",
            "Multi-task sequence to sequence learning.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1511.06114, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[24]\n",
            "\n",
            "Minh-Thang Luong, Hieu Pham, and Christopher D Manning.\n",
            "\n",
            "\n",
            "Effective approaches to attention-based neural machine translation.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1508.04025, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[25]\n",
            "\n",
            "Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.\n",
            "\n",
            "\n",
            "Building a large annotated corpus of english: The penn treebank.\n",
            "\n",
            "\n",
            "Computational linguistics, 19(2):313–330, 1993.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[26]\n",
            "\n",
            "David McClosky, Eugene Charniak, and Mark Johnson.\n",
            "\n",
            "\n",
            "Effective self-training for parsing.\n",
            "\n",
            "\n",
            "In Proceedings of the Human Language Technology Conference of\n",
            "the NAACL, Main Conference, pages 152–159. ACL, June 2006.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[27]\n",
            "\n",
            "Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.\n",
            "\n",
            "\n",
            "A decomposable attention model.\n",
            "\n",
            "\n",
            "In Empirical Methods in Natural Language Processing, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[28]\n",
            "\n",
            "Romain Paulus, Caiming Xiong, and Richard Socher.\n",
            "\n",
            "\n",
            "A deep reinforced model for abstractive summarization.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1705.04304, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[29]\n",
            "\n",
            "Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.\n",
            "\n",
            "\n",
            "Learning accurate, compact, and interpretable tree annotation.\n",
            "\n",
            "\n",
            "In Proceedings of the 21st International Conference on\n",
            "Computational Linguistics and 44th Annual Meeting of the ACL, pages\n",
            "433–440. ACL, July 2006.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[30]\n",
            "\n",
            "Ofir Press and Lior Wolf.\n",
            "\n",
            "\n",
            "Using the output embedding to improve language models.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1608.05859, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[31]\n",
            "\n",
            "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
            "\n",
            "\n",
            "Neural machine translation of rare words with subword units.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1508.07909, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[32]\n",
            "\n",
            "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,\n",
            "Geoffrey Hinton, and Jeff Dean.\n",
            "\n",
            "\n",
            "Outrageously large neural networks: The sparsely-gated\n",
            "mixture-of-experts layer.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1701.06538, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[33]\n",
            "\n",
            "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and\n",
            "Ruslan Salakhutdinov.\n",
            "\n",
            "\n",
            "Dropout: a simple way to prevent neural networks from overfitting.\n",
            "\n",
            "\n",
            "Journal of Machine Learning Research, 15(1):1929–1958, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[34]\n",
            "\n",
            "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.\n",
            "\n",
            "\n",
            "End-to-end memory networks.\n",
            "\n",
            "\n",
            "In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,\n",
            "editors, Advances in Neural Information Processing Systems 28, pages\n",
            "2440–2448. Curran Associates, Inc., 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[35]\n",
            "\n",
            "Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.\n",
            "\n",
            "\n",
            "Sequence to sequence learning with neural networks.\n",
            "\n",
            "\n",
            "In Advances in Neural Information Processing Systems, pages\n",
            "3104–3112, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[36]\n",
            "\n",
            "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and\n",
            "Zbigniew Wojna.\n",
            "\n",
            "\n",
            "Rethinking the inception architecture for computer vision.\n",
            "\n",
            "\n",
            "CoRR, abs/1512.00567, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[37]\n",
            "\n",
            "Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton.\n",
            "\n",
            "\n",
            "Grammar as a foreign language.\n",
            "\n",
            "\n",
            "In Advances in Neural Information Processing Systems, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[38]\n",
            "\n",
            "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
            "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n",
            "\n",
            "\n",
            "Google’s neural machine translation system: Bridging the gap between\n",
            "human and machine translation.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1609.08144, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[39]\n",
            "\n",
            "Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.\n",
            "\n",
            "\n",
            "Deep recurrent models with fast-forward connections for neural\n",
            "machine translation.\n",
            "\n",
            "\n",
            "CoRR, abs/1606.04199, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[40]\n",
            "\n",
            "Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.\n",
            "\n",
            "\n",
            "Fast and accurate shift-reduce constituent parsing.\n",
            "\n",
            "\n",
            "In Proceedings of the 51st Annual Meeting of the ACL (Volume 1:\n",
            "Long Papers), pages 434–443. ACL, August 2013.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Attention Visualizations\n",
            "\n",
            "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making…more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated  on Tue Apr 30 16:03:52 2024 by LaTeXML\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pylatexenc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQp1zZBfrnqr",
        "outputId": "07b11531-844b-4d02-f13c-c3504bbb9589"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.10/dist-packages (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the Latex file.\n",
        "import tarfile\n",
        "from pylatexenc.latex2text import LatexNodes2Text\n",
        "\n",
        "def extract_and_convert_latex_from_tar_gz(archive_path):\n",
        "    converted_texts = []\n",
        "    # Open the tar.gz archive\n",
        "    with tarfile.open(archive_path, 'r:gz') as tar:\n",
        "        # Iterate over the members of the archive\n",
        "        for member in tar.getmembers():\n",
        "            # Check if the member is a LaTeX file\n",
        "            if member.isfile() and member.name.endswith('.tex'):\n",
        "                # Extract the LaTeX file\n",
        "                extracted_file = tar.extractfile(member)\n",
        "                if extracted_file is not None:\n",
        "                    latex_content = extracted_file.read().decode('utf-8')\n",
        "\n",
        "                    # Convert LaTeX content to plain text\n",
        "                    plain_text = LatexNodes2Text().latex_to_text(latex_content)\n",
        "\n",
        "                    # Collect the converted text\n",
        "                    converted_texts.append(f\"\\n{plain_text}\\n\")\n",
        "\n",
        "    # Combine all converted texts into one string entity\n",
        "    combined_text = \"\\n\".join(converted_texts)\n",
        "\n",
        "    return combined_text\n",
        "\n",
        "# Example usage\n",
        "archive_path = '/content/arXiv-2305.10435v2.tar.gz'  # Replace with the path to your .tar.gz file\n",
        "combined_text = extract_and_convert_latex_from_tar_gz(archive_path)\n",
        "\n",
        "# Print the combined text outside the function\n",
        "print(combined_text)\n"
      ],
      "metadata": {
        "id": "Op_PoG_ybxC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3a0173-6369-48be-8346-90168112a38c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "shapes\n",
            "\n",
            "\n",
            "B-.05emi-.025em b-.08em\n",
            "    T-.1667em.7exE-.125emX\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "GPT (Generative Pre-trained Transformer) – A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions\n",
            "    Gokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Deepti Raj G, Rutvij H Jhaveri, Prabadevi B, Weizheng Wang, Athanasios V. Vasilakos, and Thippa Reddy Gadekallu\n",
            "\n",
            "Gokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y, Praveen Kumar Reddy Maddikunta, Deepti Raj G, Prabadevi B are with the School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu- 632014, India (Emails: { gokul.yenduri, ramalingam.m, chemmalarselvi.g, supriya.d, praveenkumarreddy, deeptiraj.g2020, prabadevi.b }@vit.ac.in)\n",
            "\n",
            "Gautam Srivastava is with the Dept. of Math and Computer Science, Brandon University, Canada, and the Research Centre for Interneural Computing, China Medical University, Taichung, Taiwan as well as Dept. of Computer Science and Math, Lebanese American University, Beirut, Lebanon (email: srivastavag@brandonu.ca)\n",
            "\n",
            "Rutvij H Jhaveri is with the Department of Computer Science and Engineering, School of Technology, Pandit Deendayal Energy University, India, (Email: rutvij.jhaveri@sot.pdpu.ac.in).\n",
            "\n",
            "Weizheng Wang is with the Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China, (E-mail: weizheng.wang@ieee.org).\n",
            "\n",
            "Athanasios V. Vasilakos is with the Center for AI Research (CAIR),University of Agder(UiA), Grimstad, Norway, (Email: thanos.vasilakos@uia.no).\n",
            "\n",
            "Thippa Reddy Gadekallu is with the School of Information Technology and Engineering, Vellore Institute of Technology, Vellore 632014, India, Lovely Professional University, Phagwara, India, Department of Electrical and Computer Engineering, Lebanese American University, Byblos, Lebanon, Jiaxing University , Jiaxing 314001, China, Zhongda Group, China, 314312 (E-mail: thippareddy@ieee.org).\n",
            "    June 14, 2024\n",
            "==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The Generative Pre-trained Transformer (GPT) represent a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, enabling technologies, their impact on various applications, emerging challenges, and potential solutions.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generative Pre-trained Transformer, Natural language processing, Artificial Intelligence\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    \n",
            "    List of key acronyms only if it is repeated \n",
            "    \n",
            "    \n",
            "        Acronyms     Description\n",
            "\n",
            "        \n",
            "        AI     Artificial Intelligence \n",
            " \n",
            "        AR     Augmented Reality \n",
            "        \n",
            " \n",
            "        BERT     Bidirectional Encoder Representations from Transformers \n",
            "        \n",
            "\n",
            "        BGN     Boneh–Goh–Nissim\n",
            "        \n",
            " \n",
            "        CNN     ConvolutionalNeural Network \n",
            "        \n",
            " \n",
            "        DAP     Data Access Point \n",
            " \n",
            "        DLT     Decentralized Ledger Technology \n",
            " \n",
            "        DL     Deep Learning \n",
            "        \n",
            " \n",
            "        DRL     Deep Reinforcement Learning \n",
            "        \n",
            " \n",
            "        DR     Demand response \n",
            "        \n",
            " \n",
            "        EC     Edge Computing \n",
            "        \n",
            " \n",
            "        EU     End User \n",
            "        \n",
            " \n",
            "        EAPs     Energy Access Points \n",
            "        \n",
            " \n",
            "        5G     Fifth-Generation \n",
            " \n",
            "        \n",
            "        4G     Fourth-Generation \n",
            " \n",
            "        GPT     Generative Pre-trained Transformer \n",
            " \n",
            "        GPU     Graphics Processing Unit \n",
            " HPC     High Performance Computing \n",
            "        \n",
            " \n",
            "        HCI     Human Computer Interaction \n",
            " \n",
            "        IoT     Internet of Things \n",
            "        \n",
            " \n",
            "        ML     Machine Learning \n",
            "        \n",
            " \n",
            "        NLP     Natural Language Processing \n",
            " \n",
            "        NPC     Non Playable Character \n",
            "        \n",
            " \n",
            "        PLM     Pre-trained Language Models \n",
            "        \n",
            " \n",
            "        PTM     Pre-Trained Models \n",
            "        \n",
            " \n",
            "        RNN     Recurrent Neural Network \n",
            "        \n",
            " \n",
            "        6G     Sixth-Generation \n",
            " \n",
            "        TL     Transfer Learning \n",
            "        \n",
            " \n",
            "        VU     Virtual Reality \n",
            "        \n",
            " \n",
            "        \n",
            "    \n",
            " \n",
            "\n",
            "\n",
            "§ INTRODUCTION\n",
            " \n",
            "Language is the cornerstone of human communication and plays a vital role in shaping our interactions with the world. With the advent of NLP, it has revolutionized the way we interact with machines. NLP has become a game-changer in the world of communication, enabling humans to interact with machines in a more natural way. The evolution of NLP has been fueled by the exponential growth of textual data in the internet. Over the years, NLP has witnessed a significant transformation from simple rule-based systems to complex deep learning-based models. Despite the advances, natural language understanding and generation have long been a challenging problem in the field of NLP, largely due to the complex nature of human language. However, recent advancements have paved the way for the new approaches to tackle these challenges. One such breakthrough in NLP, is the development of the GPT <cit.>. GPT became famous after the launch of ChatGPT by OpenAI, a research company <cit.> that focuses on developing AI technologies. GPT is a deep learning model that is pre-trained on large corpora of text data and can be fine-tuned for specific tasks like language generation, sentiment analysis, language modelling, machine translation, and text classification. The transformer architecture used in GPT is a significant advancement over previous approaches to NLP, such as RNN and CNN. It uses a self-attention mechanism to allow the model to consider the context of the entire sentence when generating the next word, which improved the model's ability to understand and generate language. The decoder is responsible for generating the output text based on the input representation <cit.>.\n",
            "\n",
            "GPT can perform a wide range of tasks in NLP. One of its key strengths is in natural language understanding (NLU), where it can analyze and comprehend the meaning of text, including identifying entities and relationships in sentences. It's also proficient in natural language generation (NLG), which means it can create text output, such as writing creative content or answering questions in a comprehensive and informative way. Alternatively, GPT is also code generator, where it can write programming code in various languages, such as Python or JavaScript. GPT can also be utilized for question answering, which means it can provide summaries of factual topics or create stories based on the input text. Additionally, GPT can summarize a piece of text, such as providing a brief overview of a news article or research paper, and it can be used for translation, which makes it possible to translate text from one language to another. Overall, GPT's ability to perform a wide range of NLP tasks with high accuracy and precision, makes it an invaluable tool for various industries, including finance, healthcare, marketing, and more. As NLP technology continues to advance, we can expect GPT and other language models to become even more sophisticated and powerful, enabling us to communicate with machines more naturally and effectively.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Motivation\n",
            "\n",
            "\n",
            "GPT has become a transformative technology in the field of NLP, enabling the rapid development and growth of a wide range of industries and applications. Despite its wide adoption and numerous potential applications, there is still much to be explored and understood about GPT's capabilities. Although there are studies on GPT in the literature related to academia and libraries <cit.>, education <cit.>,\n",
            "GPT models<cit.>, banking and corporate communication <cit.>, advancements in chatGPT and its version <cit.>, and on generative AI's <cit.>, no existing reviews are dedicated to providing a comprehensive survey on GPT. Therefore, there is a need for a comprehensive review that focuses on GPT's architecture, enabling technologies, potential applications, emerging challenges, interesting projects and future directions. These limitations motivated us to conduct this review. Hence, this review will not only help researchers and practitioners in this field to gain a better understanding of GPT but also provide valuable insights into its potential applications and major limitations when conducting the research.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Related Surveys and Contributions\n",
            "\n",
            "\n",
            "\n",
            "The GPT model is a type of DL model that uses self-supervised learning to pre-train massive amounts of text data, enabling it to generate high-quality language output. The recent advancements in GPT model research can be attributed to the continual improvement of its architecture, increased availability of computing power, and the development of novel techniques to fine-tune the model for specific tasks. These advancements have led to the creation of larger and more powerful GPT models, enabling them to perform a wider range of NLP tasks with unprecedented accuracy and fluency. These GPT models have demonstrated great potential in transforming various industries like healthcare <cit.>, customer service <cit.>, financial industry <cit.> and so on. These applications are enabled by the generation of high-quality and diverse data like large-scale corpora of text data with different fast-growing enabling technologies<cit.>. There are numerous survey papers published to provide a comprehensive overview of the latest developments in GPT models, insights into the different architectures, training methods, evaluation metrics, and highlight the challenges and future directions of this field. This literature survey aims to review and analyze the key findings and contributions of the most recent survey papers published on GPT models, to provide a comprehensive and up-to-date understanding of the state-of-the-art in this exciting and rapidly evolving field.\n",
            "\n",
            "\n",
            "\n",
            "Lund et al. <cit.> presents the potential effects of AI and GPT models, specifically ChatGPT, on academia and libraries. They discussed the capabilities of ChatGPT in generating human-like responses and its potential applications. They examine how AI-powered chatbots and virtual assistants based on GPT models can enhance student learning experiences, assist with research tasks, and improve library services. They also address concerns regarding data privacy, biases, and the need for ethical guidelines. Overall, this survey paper highlighted the transformative potential of AI and GPT models while emphasizing the importance of responsible deployment and human oversight.\n",
            "\n",
            "\n",
            "Kasneci et al. <cit.> have reviewed the potential opportunities and challenges of using large language models, specifically ChatGPT, for educational purposes. They highlighted the benefits and limitations of using such models by discussing their implications for teaching and learning. In addition, a defined strategy and pedagogical approach with a heavy focus on critical thinking and fact-checking are required while using such large language models in educational institution. Thus, they concluded the paper by highlighting the key technical challenges like copyright issues, biased content creation, user dependency, privacy and security, and high-cost language models when such language models are used in the educational sector.\n",
            "\n",
            "Qiu et al. <cit.> presented an exhaustive survey of various types of GPT models by detailing their working architecture. They discussed the evolution of pre-training methods for NLP, from language modelling to TL and pre-training on large-scale corpora. It also reviews the different types of GPT models, including word embeddings, contextual embeddings, and transformer-based models, and discusses their applications in various NLP tasks such as text classification, Named Entity Recognition, and machine translation. They highlighted the benefits of GPT's models for the NLP domain, such as its ability to improve model performance with limited annotated data, reduce the need for task-specific feature engineering, and enable TL across multiple tasks. They discussed the major challenges and limitations of PTMs, such as the risk of bias and the lack of interpretability.\n",
            "\n",
            "George et al. <cit.> studied the potential impact of GPT-4, the next iteration of GPT models, on communication within corporate environments. They discussed how GPT-4 can revolutionize business communication by enabling more efficient and effective interactions. They explore various applications of GPT-4 in corporate settings, such as automating customer support through AI chatbots that can provide personalized responses and resolve queries in real-time. They also addressed potential challenges and considerations associated with implementing GPT-4 in corporate settings. These include concerns about data security, privacy, and the need for human oversight to ensure accurate and ethical communication. Thus, they concluded by emphasizing the transformative potential of GPT-4 in revolutionizing business communication to fully harness the benefits of GPT-4 while addressing any potential risks or limitations.\n",
            "\n",
            "\n",
            "\n",
            "Zhang et al. <cit.> presents an extensive survey of generative AI and evaluates the capabilities of the ChatGPT models, particularly from GPT-4 to GPT-5. They provided an overview of generative AI, highlighting its significance in generating realistic and creative outputs across various domains and evaluate their advancements over previous iterations. They analyze the architectural improvements, model size, training techniques, and dataset considerations employed in GPT-4 and GPT-5. In addition to it, they presented a comprehensive comparison of ChatGPT with other state-of-the-art generative AI models, such as OpenAI's DALL-E and CLIP. Finally, they concluded with valuable insights into the capabilities and limitations of these models and highlights the broader landscape of generative AI.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Zaib et al. <cit.> provides a survey on the latest advancements in GPTS and PTMs for conversational AI applications. They focused on PLMs and their approaches while building dialogue-based systems. They also highlighted the potential use of transformer-based models such as BERT and GPT, which have demonstrated good performance in understanding NLP generation, and dialogue management. Thus, they concluded with the significant challenges in the field of developing conversational AI systems using PLMs and GPTs.\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tOrganization Chart of the survey.\n",
            "\t\n",
            "\n",
            "\n",
            "Thus, the comparison of existing surveys on GPT models highlighting the growing importance of these models in key areas of NLP and other related fields are discussed here. Hence, this is the first-of-its-kind survey that presents the extensive information, by comparing existing surveys with our survey and summarized in Table <ref>.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Comparison of this survey with the existing surveys\n",
            "\n",
            "!\n",
            "\n",
            "2*Ref.    \n",
            "  11l|Applications    \n",
            "  6l|Enabling Technologies    \n",
            "  2*Remarks \n",
            " 2-18\n",
            "    \n",
            "  1l|[origin=c]90 Education     \n",
            "  1l|[origin=c]90 Industry     \n",
            "  1l|[origin=c]90 Agriculture     \n",
            "  1l|[origin=c]90 Healthcare     \n",
            "  1l|[origin=c]90 Transport     \n",
            "  1l|[origin=c]90 E-Commerce      \n",
            "  1l|[origin=c]90 Entertainment     \n",
            "  1l|[origin=c]90 Lifestyle     \n",
            "  1l|[origin=c]90 Gaming     \n",
            "  1l|[origin=c]90 Marketing     \n",
            "  [origin=c]90 Finance     \n",
            "  1l|[origin=c]90 Big Data      \n",
            "  1l|[origin=c]90 AI     \n",
            "  1l|[origin=c]90 Cloud Computing      \n",
            "  1l|[origin=c]90 Edge Computing     \n",
            "  1l|[origin=c]90 5G and Beyond     \n",
            "  [origin=c]90 HCI     \n",
            "   \n",
            " <cit.>    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  X     They conducted a survey discussing capabilities of ChatGPT on academia and\n",
            "libraries. Although, Key challenges of Chatgpt were highlighted, practical implementation challenges and research directions were missing.\n",
            "   \n",
            " \n",
            "<cit.>    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  Reviewed the potential opportunities\n",
            "and challenges of using large language models, specifically\n",
            "ChatGPT, for educational purposes. Thus, evolution of GPT and their preliminaries were not discussed in this survey paper. \n",
            " \n",
            "   <cit.>    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  They presented an exhaustive survey of various\n",
            "types of GPT models by detailing their working architecture with benefits and limitations of GPTs. However,  \n",
            " \n",
            "\n",
            "<cit.>    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  Studied the potential impact of GPT4 in business communication and explore various applications of GPT-4 in corporate settings by highlighting any potential\n",
            "risks or limitations. But, how GPT architecture can be used in corporate is not found with key enabling technologies.\n",
            " \n",
            " <cit.>    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "      \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  Analyzed the architectural\n",
            "improvements, model size, training techniques, and dataset\n",
            "considerations employed in GPT-4 and GPT-5. However, preliminary details are unedr explored. \n",
            " <cit.>    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  Recent trends in   language models, applications of dialogue management, question answering NLP   tasks were discussed along with challenges and future scope of GPT. Although it covered most of the technical aspects, the integration challenges to overcomeare not presented. \n",
            " Our Survey   Paper    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "      \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "      \n",
            "  Presents the evolution of GPT models, GPT architecture and its detailed working, key enabling technologies, significant advancements of GPT models and their potential benefits in real-life applications, GPT projects, lessons learnt, open challenges and future research directions.\n",
            "   \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Systematic Literature Survey\n",
            "\n",
            "\n",
            "In this review of GPT, we conducted a thorough literature review using various reputable sources. Our search was primarily focused on peer-reviewed journals, and high-quality articles from reputed national and international conferences, seminars, books, symposiums, and journals. To ensure the credibility of our sources, we referred to well-known archives such as Google Scholar and arXiv, and publications from top databases like IEEE, Springer, Elsevier, Taylor & Francis, and Wiley. To identify relevant GPT references and publications, we used keywords such as NLP­GPT, GPT architecture, DL for GPT, Pre­training GPT, Fine-tuning AI GPT and GPT vertical applications. We then screened all the retrieved articles based on their titles, excluding any papers with poor-quality material. Next, we reviewed the abstracts of the remaining articles to determine their contributions. In the final step of our literature review, we extracted the necessary data for our analysis. By following these phases, we ensured that our study was based on high-quality and credible sources.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Paper Organization\n",
            "\n",
            "\n",
            "The structure of this paper's organization is illustrated in Fig. <ref>. Section 2 presents the preliminaries of GPT models such as the definition of GPT, its evolution and architecture, how it works and presents the comparison of various GPT models. Section 3 discusses the key enabling technologies for GPT models. The impact of GPT models in various applications are presented in Section 4. In Section 5, we highlighted some of the exciting GPT projects that are currently developed. Section 6 includes open issues, other technical challenges and future research directions in the field of GPT. Finally, we conclude the paper in Section 7, by summarizing the key findings and contributions of this study. The list of key acronyms are listed in Table <ref>. \n",
            "\n",
            "\n",
            "\n",
            "§ PRELIMINARIES\n",
            "\n",
            "\n",
            "In this section, the evolution of GPT models, the architecture of GPT, working process of GPT models are discussed and finally, different versions of GPT models are compared.\n",
            "\n",
            "\n",
            " §.§ Generative Pre-trained Transformer\n",
            "\n",
            "The GPT model produces enormous quantities of pertinent and complicated machine-generated text from a small amount of text as input. GPT models can be identified as a language model that mimics human text using a DL techniques and it acts as an autoregressive model in which the present value is based on the previous value <cit.>.\n",
            "\n",
            "\n",
            "  §.§.§ Definition 1\n",
            "\n",
            "GPTs are language models pre-trained on vast quantities of textual data and can perform a wide range of language-related tasks <cit.>.\n",
            "\n",
            "\n",
            "  §.§.§ Definition 2\n",
            "\n",
            "A GPT is a language model relying on DL that can generate human-like texts based on a given text-based input.  <cit.>. \n",
            "\n",
            "\n",
            "  §.§.§ Definition 3\n",
            "\n",
            "GPT is a language model developed by OpenAI to help give systems intelligence and is used in such projects as ChatGPT <cit.>.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Evolution of GPT\n",
            "\n",
            "GPT models have evolved through multiple changes and breakthroughs in NLP technology. These are some significant turning points in the growth of the GPT model:\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tGPT Road Map.\n",
            "\t\n",
            "\n",
            "\n",
            "Before GPT, NLP models have been trained on large amounts of annotated data that is related to a specific task. This had a significant drawback because it was difficult to access the quantity of labelled data required to train the model precisely. The NLP models were unable to complete tasks outside of their training set since they were restricted to a particluar set of data. To get around these restrictions, OpenAI offered a Generative Language Model called GPT-1 that was created using unlabeled data and then given to users to fine-tune to carry out subsequent tasks like sentiment analysis, categorization, and question-answering <cit.>. This indicates that the model attempts to produce an appropriate response based on input and that the data used to train the model is not labelled <cit.>. Fig. <ref> shows the timeline of the evolution of several pre-trained models from Eliza, which was created in 1960, to the more current 2022-ChatGPT.\n",
            " \n",
            "GPT-1 was the first ever model that could read the text and respond to queries <cit.>. OpenAI released GPT-1 in 2018. GPT-1 was a major move forward in AI development because it enabled computers to comprehend textual material in a more natural manner than before. This generative language model was able to learn a wide variety of connections and gain immense knowledge on a varied corpus of contiguous text and lengthy stretches <cit.>. This happened after being trained on a huge BooksCorpus dataset. In terms of design, GPT-1 employs a 12-layer decoder architecture transformer with a self-attention system for training. GPT-1's capacity to execute zero-shot performance on different tasks was one of its major success as a result of its pre-training. This ability demonstrated that generative language modelling can be used to generalize the model when combined with a successful pretraining idea. With TL as its foundation, GPT models evolved into a potent tool for performing NLP tasks with minimal fine-tuning <cit.>. It paved the way for other models to progress even more in generative pre-training using larger datasets and parameters.  <cit.>.\n",
            "\n",
            "To create a better language model later in 2019, OpenAI created a GPT-2 using a bigger dataset and more parameters. The model design and execution of GPT-2 are some of the key advancements <cit.>. With 1.5 billion parameters, it has 10 times the size of GPT-1 (117 million parameters), and it has 10 times as many parameters and data <cit.>. By using only the raw text as input and utilizing little to no training examples, it is effective in terms of resolving various language tasks related to translation, summarization, etc. Evaluation of GPT-2 on various downstream task datasets revealed that it excelled by substantially increasing accuracy in recognizing long-distance relationships and predicting sentences <cit.>. \n",
            "\n",
            "The most recent iteration of the GPT model is GPT-3. It is a sizable language prediction and production model created by OpenAI that can produce lengthy passages of the source text. GPT-3 eventually emerged as OpenAI's ground-breaking AI language software. Simply put, it is a piece of software that can create lines on its own that are so distinctive they almost sound like they were written by a human <cit.>. The GPT-3 program is presently accessible with limited access via a cloud-based API, and access is required to investigate the utility. Since its debut, it has produced several interesting apps. Its capacity, which is about 175 billion parameters big and 100 times larger than GPT-2, is a key advantage. It is taught using a corpus of 500 billion words called \"Common Crawl\" that was gathered from a sizable content archive and the internet <cit.>. Its other noteworthy and unexpected capability is its ability to carry out basic mathematical operations, write bits of code, and carry out clever tasks. As a result, NLP models can help businesses by responding more quickly to requests and accurately keeping best practices while minimizing human mistakes <cit.>. Due to its intricacy and size, many academics and writers have referred to it as the ultimate black-box AI method. Due to the high cost and inconvenience of performing inference, as well as the billion-parameter size that makes it resource-intensive, it is difficult to put into practice in jobs <cit.>. \n",
            "\n",
            "GPT-4 was named as the successor of GPT-3. In the meantime, several AI models built on GPT-3.5, an updated version of GPT-3, have been surreptitiously released by OpenAI <cit.>. GPT-3.5 was trained on a mixture of text and code. From the vast amounts of data collected from the web, which includes tens and thousand of Wikipedia entries, social media posts, and news items, GPT 3.5 learned the relations between words, sentences, and various components. It was utilized by OpenAI to develop several systems that have been tailored to complete particular jobs <cit.>. It collected vast amounts of data from the web, including tens of thousands of Wikipedia entries, posts on social media, and news items, and used that information to learn the relationships between sentences, words, and word components <cit.>.\n",
            "\n",
            "The latest version of the GPT model by OpenAI is GPT-4 which is a multimodal big language model. It was launched on March 14, 2023, and is now accessible to the general public through ChatGPT Plus in a constrained capacity. A waitlist is required to gain access to the business API <cit.>. Using both public data and \"data licensed from third-party providers,\" GPT-4 was pre-trained to anticipate the next coin as a transformer. It was then adjusted with reinforcement learning based on input from humans and AI for human alignment and policy conformance. In comparison to GPT-3, which had context windows of only 4096 and 2049 tokens, respectively, the group created two variants of GPT-4 with context windows of 8192 and 32768 tokens.\n",
            "\n",
            "\n",
            " §.§ GPT model's architecture\n",
            "\n",
            "\n",
            "\n",
            "GPT models are based on neural networks that are used for NLP tasks, such as language modelling, text classification, and text generation.\n",
            "\n",
            "The GPT model's architecture is based on the transformer model <cit.>. The Transformer model uses self-attention mechanisms to process input sequences of variable length, making it well-suited for NLP tasks. GPT simplifies the architecture by substituting encoder-decoder blocks with decoder blocks. GPT model takes the transformer model and pre-trains it on large amounts of text data using unsupervised learning techniques. The pre-training process involves predicting the next word in a sequence given the previous words, a task known as language modelling. This pre-training process enables the model to learn representations of natural language that can be fine-tuned for specific downstream tasks <cit.>. The following are the components of the GPT architecture.\n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "   \n",
            "  * Input Embedding layer: The embedding layer maps the input tokens (e.g., words or subwords) to continuous vector representations, which can be processed by the transformer blocks<cit.>.\n",
            "   \n",
            "  * Positional encoding: Since the transformer blocks do not have any notion of order or position, positional encoding is added to the input embeddings to provide information about the relative position of tokens.\n",
            "Masking: In some cases, masking may be necessary to mask certain input tokens (e.g., in language modelling tasks, the model should only use tokens that come before the target token).\n",
            "Transformer blocks: GPT models are based on the transformer architecture. It is designed for NLP tasks and has been widely used in applications such as machine translation, text classification, and text generation. Transformers allow the model to focus on different areas of the input while processing <cit.>.\n",
            "   \n",
            "  * Linear and Softmax Functions:\n",
            "In the GPT architecture, the softmax function is commonly used for classification tasks. The softmax function is applied to the output of the final layer of the model. It generates a probability distribution over a set of output classes. The output of the final layer is specifically converted into a set of logits before being normalized with the softmax function. The normalized values obtained from the model can be interpreted as the likelihood or probability that a particular input belongs to each of the output classes.\n",
            "The query, key, and value vectors for each token in the input sequence are frequently calculated using linear functions in the attention mechanism. The output of the multi-head attention layer is transformed using them in the feedforward layers as well. The output layer also employs linear functions to forecast the following token in the sequence <cit.>.\n",
            "   \n",
            "  * Pre-training: Pre-training is a key component of the GPT architecture. In pre-training, the model is trained on a large amount of data in an unsupervised manner even before fine-tuning the model for specific tasks like classification and text generation.\n",
            "   \n",
            "  * Fine-tuning:\n",
            "Fine-tuning is the process of adapting a pre-trained neural network model, such as GPT, to a new task or dataset by further training the model on that task or dataset. Fine-tuning in GPT involves adjusting the parameters of the pre-trained model to optimize performance on a specific downstream task, such as text classification or text generation <cit.>.\n",
            "   \n",
            "  * Language modeling:\n",
            "Language modelling is a key task in the GPT architecture. In the case of GPT, the language modelling task is performed during the pre-training phase of the model. In pre-training, the model is trained based on a large amount of data using a language model objective. It is the task of predicting the next word in sequence based on the previous words. It allows the model to learn relationships between the words and their meaning in the training data <cit.>.\n",
            "   \n",
            "  * Unsupervised learning:\n",
            "Unsupervised learning is an ML algorithm which enables the model to learn form unlabelled data without any human intervention. GPT models use unsupervised learning in the pre-training phase to understand the relationships between the words and their context in the training data <cit.>. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Comparsion of different versions of GPT model\n",
            "\n",
            "!\n",
            "\n",
            "Model    \n",
            "  Tokens    \n",
            "  Size    \n",
            "  Parameters    \n",
            "  Dataset    \n",
            "  Year    \n",
            "  Features    \n",
            "  Input Type    \n",
            "  Drawbacks \n",
            " GPT-1    \n",
            "  -    \n",
            "  12-layer decoder    \n",
            "  117M parameters    \n",
            "  Books corpus    \n",
            "  2018    \n",
            "  Used   mostly for language modelling tasks and it is transformer based    \n",
            "  A sequence of tokens and words    \n",
            "  Limited   Capacity, Limited Data, Cannot perform complex tasks, Limited applications \n",
            " GPT-2    \n",
            "  -    \n",
            "  10 times the size of   GPT-1    \n",
            "  1.5B parameters    \n",
            "  Downstream task   datasets    \n",
            "  2019    \n",
            "  Text   generation capabilities are improved and a chance for misuse    \n",
            "  A sequence of tokens and words    \n",
            "  Limited   Control, Limited Data Diversity, Expensive computational requirements, Risk of   improper information \n",
            " GPT-3    \n",
            "  4096 and 2049 tokens    \n",
            "  100 times larger than   GPT-2    \n",
            "  175B parameters    \n",
            "  Common Crawl    \n",
            "  2020    \n",
            "  Good   NLP capabilities, language translation, summarization and generation of text    \n",
            "  A sequence of tokens and words and images and tables    \n",
            "  Limited   Control, Limited Data Diversity, Lack of explanation, Ethical concerns \n",
            " GPT-3.5    \n",
            "   maximum token limit of 4096 tokens    \n",
            "  96 layers    \n",
            "  similar or larger number of parameters like GPT-3    \n",
            "  -    \n",
            "  2022    \n",
            "  Improves user experience by delivering more precise and contextually relevant information    \n",
            "  The input type typically consists of text data    \n",
            "  Limited resources to train,Data Bias,Lack of Explainability,Limited Contextual Understanding,High Inference Latency \n",
            " GPT-4    \n",
            "  8192 and 32768 tokens    \n",
            "  -    \n",
            "  100T parameters    \n",
            "  -    \n",
            "  2023    \n",
            "  Creative   and technical writing tasks    \n",
            "  A sequence of tokens and words and images and tables    \n",
            "  - \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ How do GPT models work?\n",
            "\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tTransformer Architecture.\n",
            "\t\n",
            "\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tTransformer Architecture and Input Transformations for Fine-Tuning on Different Tasks.\n",
            "\t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tHow does GPT Work.\n",
            "\t\n",
            "\n",
            "\n",
            "GPT models work by using a transformer which is a neural network architecture that processes the input sequences of natural language text <cit.>. The GPT model uses unsupervised learning techniques to pre-train this transformer architecture on a significant amount of text input <cit.>. The model gains the ability to anticipate the subsequent word in a sequence based on the preceding words during pre-training. Language modelling is the process that enables a model to discover the statistical connections between words and their context in training data. Fig. <ref> shows the various stages of GPT operation. The first step entails supervised fine-tuning, the second step involves producing optimal responses to input, and the third step involves proximal policy optimization and reinforcement learning.\n",
            "\n",
            "The model can be fine-tuned for particular tasks, like text classification or text production, after pre-training. The model is trained on a smaller dataset that is unique to the work at hand during fine-tuning, and the model's parameters are changed to maximize performance on that task <cit.>. Fig. <ref> shows the general transformer architecture of GPT.\n",
            "\n",
            "When used for text creation, GPT models create text by anticipating the following word in a series based on the previously created words. Depending on how it has been modified, the model can produce text that is comparable to the input text or that adheres to a certain theme or style. Fig. <ref> projects the GPT model's transformer architecture and input transformations for fine-tuning different tasks.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§  Comparisons of GPT Versions\n",
            "\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tA comparison between unimodal, cross-modal, and multimodal modal GPTs.\n",
            "\t\n",
            "\n",
            "There are several versions of GPT models each having their own features and capabilities. Table <ref> presents a comparison of various versions of the GPT models. The table presents the following details like year of release of the GPT model, parameters, tokens generated, input type, features of each model, drawbacks of each model, and the size of each model.\n",
            "\n",
            "Generative AI (GAI) models are of different types like unimodal, cross-modal, and multimodal. The first type is unimodal which rely on a single type of input, such as text or images. The cross-modal, on the other hand, can process multiple types of inputs and relate them to each other. The Multimodal is the most complex type of AI as it can process and integrate information from multiple modalities, such as speech, text, images, and even physical interactions with the environment. GPT adopts only unimodal and multimodal types where ChatGPT is said to be unimodal, while GPT-4 is multimodal. Fig. <ref> is an illustration that distinguishes between unimodal, cross-modal, and multimodal Generative AI models.\n",
            "\n",
            "\n",
            "\n",
            "Overall, GPT models have demonstrated outstanding performance with NLP, by enhancing each iteration  and its predecessor' capabilities. Each model, however, also has its own restrictions and drawbacks, such as restricted output control, lack of diverse data, and ethical concerns. While selecting a GPT model for a particular task, researchers and developers should carefully take these factors into account <cit.>.\n",
            "\n",
            "In detail, this section describes the evolution, and architecture of GPT and compares the different versions and types of GPT.\n",
            "\n",
            "\n",
            "§ ENABLING TECHNOLOGIES\n",
            "\n",
            "GPT is a convergence of several technologies. It is enabled by the latest technologies like Big data, AI, Cloud Computing, EC, 5G and beyond networks, and HCI. In this section, we provide an overview of enabling technologies related to GPT. The major technologies that constitute the GPT models are depicted in Fig. <ref>.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Big Data\n",
            "\n",
            "\n",
            "Big data refers to the vast amounts of structured and unstructured data generated by businesses, individuals, and machines. The proliferation of new technologies, such as the IoT, has led to an explosion of data production from sources like social media, sensors, and transaction-based systems <cit.>.\n",
            "\n",
            "\n",
            "The emergence of big data has revolutionized the way organizations approach data analysis and decision-making. The training provided by this massive amount of data has yielded valuable insights for the use of advanced models like GPT in the field of NLP <cit.>. The GPT models utilize DL and big data for natural language generation, with GPT-4 being the most advanced model to date <cit.>.\n",
            "\n",
            "The training data for GPT models typically include millions or even trillions of data from a diverse range of sources, such as books, articles, websites, and social media platforms. This large and diverse training data helps GPT models capture the variations in language usage, making them more accurate and effective at NLP tasks. As a result, GPT models may be used for a variety of tasks, including question-answering, text summarization, and language translation <cit.>. Moreover, since GPT models can learn from a variety of data sources, they can be tuned for certain tasks and domains, making them very adaptive and versatile. GPT model has the potential to be utilized for a variety of activities, including the creation of images and videos in addition to its excellent language processing capabilities <cit.>. \n",
            "\n",
            "\n",
            "While big data presents numerous benefits to GPT, by enabling the models to get trained with large amounts of data, it also presents several challenges <cit.>. GPT is trained on a variety of data, large amounts of data, and also sensitive data. Thus, ensuring data accuracy, privacy concerns, and ethical use of data are some of the challenges that must be considered. However, with the continuous growth of available data, GPT models will become even more advanced and capable of performing increasingly complex tasks <cit.>. The future of big data as an enabling technology for GPT models is promising, with the potential to revolutionize the field of NLP. As technology continues to advance, organizations must prioritize ethical considerations and data accuracy to fully harness the benefits of big data and GPT models.\n",
            "\n",
            "\n",
            " §.§ Artificial Intelligence\n",
            "\n",
            "\n",
            "AI refers to the simulation of intelligent behaviour in machines that are programmed to learn from their experience to reason, understand natural language, and perceive their environment <cit.>. AI gives machines the ability to sense their surroundings, deal with what they see, handle issues, and take action to reach a particular objective. The importance and capability of AI is growing all the time. \n",
            "\n",
            "\n",
            "AI enables GPT models to allow machines to comprehend and react to human language. There are several ways in which AI can continue to help improve GPT and make it more powerful and effective in its language generation capabilities <cit.>.\n",
            " \n",
            "The following are the several ways through which AI can make GPT models more powerful:\n",
            "\n",
            "\n",
            "  * Fine tuning\n",
            "\n",
            "  * Dialogue generation\n",
            "\n",
            "  * Natural language understanding\n",
            "\n",
            "GPT's model performance on particular tasks can be enhanced by utilizing AI approaches. For instance, it can be trained on a large corpus of text from a particular field such as legal documents or medical literature to better grasp and produce language in that field <cit.>. Considering dialogue generation, AI techniques such as reinforcement learning and sequence-to-sequence models can be used to enable GPT generate more natural and engaging dialogue in conversational contexts. Similarly, AI techniques such as semantic parsing and named entity recognition can be used to help GPT better understand the meaning of language and the relationships between words and phrases. This can enable it generate more accurate and coherent language <cit.>.\n",
            "\n",
            "The development and enhancement of GPT model language production capabilities depend heavily on AI, and GPT's capabilities will continue to be growing by continuous research and development in AI.\n",
            "\n",
            "As GPT models become more advanced, there are growing concerns about the potential for them in reinforcing biases and propagate harmful or offensive content <cit.>. Some of these concerns also include bias which can lead to unintended discrimination and unfairness, lack of understanding of the context that can lead to misunderstandings or incorrect responses, poor data quality can lead to inaccurate or biased models, ethical concerns like privacy and autonomy <cit.>. AI models like GPT require significant amounts of computational power to train and run, which can have a significant environmental impact due to their high energy consumption <cit.>.\n",
            "\n",
            "Though AI has a great deal of promise, it's critical to be aware of the underlying issues and make efforts to fix them to ensure that it is utilized responsibly and morally for GPT.\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tEnabling technologies of GPT models.\n",
            "\t\n",
            "\n",
            "\n",
            "\n",
            " §.§ Cloud Computing\n",
            "\n",
            "Cloud computing refers to the on-demand availability of computer resources, such as storage, processing power, and applications, delivered over the internet <cit.>. The GPT model's successes are possible not only because of algorithmic evolution but also increased computational capabilities i.e. exponential growth in hardware (computational power, storage capacity), cloud computing, and related operational software <cit.>. The applications for cloud and EC working together such as natural language generation, image completion, or virtual simulations from wearable sensors see that the work is made more compute-intensive <cit.>.\n",
            "\n",
            "\n",
            "GPT models need a lot of computational power to analyze a lot of data, and cloud computing offers the scalability required to cope with demand spikes. Without worrying about the constraints of on-premises hardware, GPT models can rapidly and easily scale up or down as needed with cloud computing <cit.>. Cloud-based platforms like Amazon Web Services (AWS) or Google Cloud Platform (GCP) provide access to distributed computing resources that can be used to train GPT. Since cloud computing provides web-based solutions and thereby does not require the purchase and maintenance of costly hardware, it can be a cost-effective choice for a GPT model. By utilizing cloud computing, the GPT model can only pay for the computing resources it uses <cit.>. The other added advantage of cloud computing in GPT is, it gives GPT models the freedom to access computing resources whenever it wants, from any location in the world. This makes GPT models more accessible to users by enabling smooth operation across a variety of gadgets and platforms <cit.>. Cloud computing providers offer high security and compliance standards, which can protect the GPT model and its data from online dangers. Cloud service providers also possess the knowledge and tools necessary to effectively address security problems and stop data leaks. Cloud-based storage services, such as Amazon S3 or Google Cloud Storage, provide scalable and reliable storage for GPT's data.\n",
            "\n",
            "\n",
            "Despite the advantages of cloud computing where it can help GPT models to operate more efficiently, effectively, and securely, there are also a few technological aspects where it creates a drawback for GPT <cit.>. To function properly, the GPT model needs a sizable amount of computing power and data storage. These resources can be accessible online with cloud computing. As a result, continued operation of the GPT model requires a robust and dependable internet connection, and any breakdown in connectivity may result in delays or even data loss. There are some security concerns when storing sensitive data, such as personal information or trade secrets, in the cloud which can be risky if proper security measures are not in place <cit.>. While cloud computing can be more cost-effective than building and maintaining an in-house computing infrastructure, it can still be expensive for long-term use. It also suffers issues like performance variability, limited availability etc., \n",
            "\n",
            "\n",
            " §.§ Edge Computing\n",
            "\n",
            "The rapid growth of IoT, a large amount of data from several IoT devices, and also cloud services have necessitated the emergence of a concept called EC. EC is an open AI and distributed design with decentralized computational power. In EC, there is a lesser need for clients and servers to communicate over long distances, which lowers latency and bandwidth utilization.\n",
            "\n",
            "Instead of depending on centralized data centers, EC entails bringing computing capacity and data storage closer to the consumer <cit.>.\n",
            "\n",
            "\n",
            " In GPT, where there is a need for real-time data analysis, EC plays a major role in faster processing and better efficiency in producing good results<cit.>. GPT models are typically large and complex, requiring significant processing power to run. By deploying GPT models on the edge devices, closer to the source of data, latency can be reduced in replying to users who seek information through the GPT models by eliminating the need to move data back and forth from end devices to the cloud. Since EC maintains data near the periphery and away from centralized servers, it can offer improved security and more privacy protections in the case of the requests made by users through GPT <cit.>. GPT models utilize a lot of data for learning and thereby the cost of data transfer also increases with data volume. EC can aid in controlling data transfer expenses. EC can also help in lowering the amount of bandwidth by pre-processing the data even before transferring it to the cloud. Particularly when analyzing photos or videos, GPT models can produce a lot of data <cit.>. EC accelerators, such as graphics processing units (GPUs) and field-programmable gate arrays (FPGAs), can be used to speed up GPT model inference and training. These accelerators can be integrated into edge devices or edge servers, providing more efficient processing of GPT models.\n",
            "\n",
            "EC and GPT models make a great combination. Comparative to cloud data centres, edge devices may have constrained computation and storage capabilities <cit.>. This might limit the scope of GPT models that can be installed on edge devices in terms of size and complexity. Since GPT models handle large and varied data, EC can also increase security risks and data privacy concerns. Implementing EC in existing infrastructure can be difficult and require significant investment in hardware, software, and networking components. This can be a barrier for many organizations which are using the GPT model and EC <cit.>.\n",
            "\n",
            "\n",
            "\n",
            " §.§ 5G and beyond networks\n",
            "\n",
            "\n",
            "5G networks represent the latest generation of cellular networks that promise faster data speeds, lower latency, and the ability to connect a vast number of devices simultaneously <cit.>. 5G and beyond networks enable faster data transmission speeds than previous generations of cellular networks, which can help in training and deploying larger and more complex language models. This can result in faster training times and better performance. 5G and beyond networks can provide lower latency than previous generations of cellular networks, which can reduce the time required for communication between GPT and other devices, such as servers or other language models <cit.>. This can improve the real-time response of the GPT model for applications that require quick and accurate language processing. 5G and beyond networks offer improved connectivity options, such as increased capacity and more reliable connections, which can help in scaling up the deployment of the GPT model for large-scale language processing tasks. With the deployment of 5G and beyond networks, EC is becoming more prevalent. This means that a GPT model can potentially be deployed closer to the end-user, reducing the latency and improving the response time for applications that require real-time language processing <cit.>. Ultra-Reliable Low-Latency Communication (URLLC) is a key feature of 5G networks. In the context of GPT language models, URLLC can enable real-time and reliable communication between multiple devices, such as edge devices, cloud servers, and end-users <cit.>.\n",
            "\n",
            "\n",
            "Though 5G and beyond technology offers potential advantages to GPT models, it is also important to note that the actual impacts of this technology may change depending on how it's implemented and used. 5G enables the access to uncontrolled access to the Internet,it may attract cybersecurity risks and privacy concerns <cit.>. Also, as GPT uses a large amount of data for analysis it could also cause privacy concerns. 5G and beyond networks in GPT models need high infrastructural requirements which is a costly process.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Human Computer Interaction\n",
            "\n",
            "\n",
            "HCI, which is multi-faceted, concentrates on the design of computer technology and, in particular, on how people and computers communicate with each other <cit.>.\n",
            "\n",
            "\n",
            "HCI has a greater influence over GPT models. As a language model, GPT is designed to interact with humans by generating natural language responses to input text. HCI research can help designers create more effective input mechanisms for the GPT model, such as natural language interfaces, that allow users to communicate more easily and accurately with the model <cit.>.\n",
            "HCI also helps in enhancing the GPT model's user experience by creating interfaces that are more intuitive and user-friendly. This makes it easy for the users to interact with GPT models and understand their responses <cit.>. HCI also estimates the performance of GPT models by evaluating their responses with real-time users and identifies the areas where the model needs improvement, thereby improving its reliability and accuracy. HCI enhances the usability of GPT models by reducing the time and effort required for the users to interact with <cit.>.\n",
            "\n",
            "\n",
            "While HCI can be incredibly helpful in improving the design and usability of GPT models, there are also some potential drawbacks to consider. If the research is not conducted with a diverse group of users, HCI can introduce biases into the design of the GPT model. HCI techniques can be expensive and time-consuming. As GPT models become more complex, it may become more difficult to design interfaces and input mechanisms that are both effective and user-friendly <cit.>. HCI may not always be able to provide the necessary insights or feedback to drive improvements in GPT models. There are also ethical concerns around the use of GPT models, including issues related to privacy, bias, and the potential misuse of the technology <cit.>.  As GPT models become more complex, it may become more difficult to design interfaces and input mechanisms that are both effective and user-friendly.\n",
            "\n",
            "\n",
            "§ IMPACT OF GPT MODELS ON VARIOUS APPLICATIONS\n",
            "\n",
            "GPTs have made significant progress, and its impact is being felt across various industries \n",
            " like education, healthcare, industry, agriculture, travel and transport, e-commerce, entertainment, lifestyle, gaming, marketing, and finance. This section provides valuable insights on the impact of the GPT models in the aforementioned applications as depicted in Fig. <ref>.\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tThe impact of GPT models on various applications.\n",
            "\t\n",
            "\n",
            "\n",
            "\n",
            " §.§ Education\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "\n",
            "Education has been around for centuries, with traditional education being the most common form. Traditional education involves a teacher imparting knowledge to a group of students in a physical classroom. While successful, traditional education can be restrictive and inflexible, limiting students' ability to learn at their own pace and in their preferred style. It can also be limited by geography, as students need to be physically present in a classroom to learn. Technology has emerged as a solution to some of these issues, allowing for personalized learning experiences and more engaging, accessible resources. Online learning platforms, digital textbooks, and multimedia tools offer students access to a vast array of resources from anywhere in the world. Technology can also facilitate collaboration and communication among students and teachers, leading to a more dynamic and interactive learning experience. Distance learning, hybrid learning models, and online classes are examples of how technology can help break down the barriers of traditional education, making learning more flexible, efficient, and effective. By integrating technology into traditional education, we can create a more personalized and effective learning experience, benefiting students worldwide.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Education\n",
            "\n",
            "The field of education is constantly evolving, with advancements in technology playing a significant role in shaping the way we learn and teach. One such technology that has the potential to transform the education industry is GPT. As a large language model trained on a vast amount of data, GPT can generate human-like text that is coherent and informative, making it a valuable tool in developing educational content such as textbooks, study guides, and course materials. Furthermore, GPT can be used to analyze and summarize complex text, which can help educators and students save time and increase comprehension. With its ability to support NLP applications and create intelligent tutoring systems, GPT has the potential to revolutionize the way we learn and teach. In this context, following section will explore the different ways in which GPT can contribute to the education industry and transform the future of learning.\n",
            "\n",
            "\n",
            "    \n",
            "  * Intelligent Tutoring: Intelligent tutoring is a teaching approach that uses AI and ML to provide personalized and adaptive instruction. It analyzes student performance data, understands their strengths and weaknesses, and generates customized learning paths. It provides immediate feedback, personalized guidance, and remedial support. It is effective in improving learning outcomes, increasing student engagement, and reducing learning time. With advanced natural language processing capabilities, GPT can enhance the personalized and adaptive instruction provided by intelligent tutoring systems. It can analyze natural language input from students, enabling intelligent tutoring systems to better understand and respond to their queries, needs, and preferences. It can also generate personalized feedback and assessment based on the individual learning progress of each student, helping them to identify and address their knowledge gaps and improve their performance. GPT can also analyze student performance data and generate adaptive learning paths that provide customized instruction and remediation, ensuring that each student learns at their own pace and achieves their learning objectives. Additionally, it can create interactive dialogue systems that simulate natural conversations between students and virtual tutors, making learning more engaging, interactive, and personalized <cit.>. The authors in <cit.> have identified that GPT-4 model outperforms general-purpose GPT-3.5 model as well as GPTs (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B) specailly trained on medical data. The authors have tested GPT-4 models' ability to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. \n",
            "\n",
            "\n",
            "    \n",
            "    \n",
            "  * Learning assistance and material development: Learning materials are critical in education as they provide a structured way for students to acquire knowledge and skills. They can be tailored to meet the needs of diverse learners and make learning more engaging and effective, supporting teachers to create a more dynamic and interactive learning environment. GPT can contribute to creating learning materials by automating content generation, providing multilingual content creation, language correction, personalized content creation, conducting topic research, and generating assessments. It saves time and effort for educators and publishers, improves the accuracy and readability of material, and makes learning more engaging and effective. GPT can generate high-quality content such as summaries, quizzes, and lesson plans based on specific learning objectives, making learning accessible to a wider audience. It can analyze written content and provide suggestions to improve grammar, punctuation, and readability. GPT can also assist in research writing by suggesting ideas for structure, rephrasing and organizing content, and identifying gaps in research <cit.>. Moreover, GPT can also provide personalized feedback based on individual learning progress, enhancing the development of more comprehensive and informative learning materials.\n",
            "     \n",
            "    \n",
            "  * Automated Assessments: \n",
            "    Automated assessment in education uses technology to evaluate students' learning outcomes, providing immediate feedback and reducing potential bias in grading. It can also help teachers identify areas where students may need additional support, enabling them to tailor their teaching methods to better meet individual needs. GPT with its advanced natural language processing skills, can help in automated assessment by analyzing and grading student responses to various types of assessment questions, including essays and short answer questions. It can also provide feedback to students <cit.>, such as highlighting areas for improvement and suggesting further reading or resources. GPT's natural language processing capabilities can help to identify the meaning and context of students' responses, making automated assessment more accurate and effective. Additionally, GPT can generate personalized learning materials and exercises based on students' assessment results, supporting educators to create more tailored and effective learning experiences. The authors in <cit.> have used Chat GPT in evaluating the students' assignments such as quiz style questions, and also in generating relevant practice problems to improve content retention and understanding. The results were promising in the classroom. The authors believe that Chat GPT has the significant ability in reducing the load of instructor without compromising students' learning outcomes.\n",
            "    \n",
            "    \n",
            "  * Fostering Creativity: Creativity thinking plays a vital role in education by encouraging students to think beyond traditional boundaries and develop innovative solutions to complex problems. It helps students to approach learning with an open mind and a willingness to explore new ideas, leading to greater engagement and motivation. GPT's ability to generate human-like responses and creative writing can aid in improving creativity. It can help improve creativity by generating new and innovative ideas based on vast amounts of data and information. By analyzing patterns in language and identifying connections between different concepts, GPT can suggest novel approaches to teaching and learning. Additionally, GPT can also generate creative prompts or challenges for students, encouraging them to think outside the box and approach problems in unique ways <cit.>. GPT can also analyze and evaluate students' creative work, providing feedback and suggestions for improvement. So, GPT can be a valuable tool for promoting and enhancing creativity among students and faculty members.\n",
            "\n",
            "      \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "\n",
            "\n",
            "There are several advantages to incorporating GPTs in education, but it is essential to acknowledge the potential limitations. While GPTs can quickly generate information, they may impede students' critical thinking and problem-solving skills. Furthermore, learners who benefit from personal interaction with instructors may find the lack of human involvement disadvantageous. GPTs rely on statistical patterns, so they cannot provide a comprehensive understanding of the material being taught <cit.>. Privacy concerns arise when using sensitive student data in GPTs for educational purposes. Additionally, since GPTs cannot provide citations, it is challenging to identify the source of information generated. The cost of maintaining GPT may be prohibitive for schools and educational institutions with limited resources. Finally, distinguishing between reliable and unreliable information generated by GPTs can be difficult, so it is necessary to have human oversight to ensure data accuracy and regulate access.\n",
            " \n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPT offers numerous advantages in the education sector, including personalized and adaptive instruction, automated assessment, creative writing support, and research writing assistance. They have the potential to revolutionize teaching by creating lesson plans and activities, responding to natural language queries, and integrating multiple digital applications. However, there are also challenges to consider, such as the potential negative impact on critical thinking and problem-solving skills, lack of human interaction, data security and privacy concerns, inability to provide full comprehension, lack of citations or sources, high cost of maintenance, and potential for producing unreliable information. Further research is needed to explore human-computer interaction and user interface design to integrate GPT into educational workflows while ensuring that the information they provide is accurate and reliable.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Healthcare\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Before technology became widespread in healthcare, healthcare services were primarily delivered through face-to-face interactions between healthcare professionals and patients. Traditional healthcare faced several challenges, including limited medical instruments, paper-based health records, patients receiving care mostly in hospitals or clinics, physical travel requirements to receive medical attention, and limited medical research. Despite these challenges, traditional healthcare still provided valuable medical services to patients. However, with the introduction of technology, healthcare has become more efficient, accessible, and personalized, resulting in improved patient outcomes and better overall healthcare services. Technology has become an essential aspect of society, as reflected in the significant investments made in this sector. Despite the advancements in technology, the healthcare industry still faces various new challenges, including access to healthcare, high costs, personalized medicine, data privacy and security concerns, and an aging population. However, technology has the potential to address these challenges and improve the efficiency of healthcare services.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in healthcare\n",
            "\n",
            "Recent years have seen significant advancements in technology, including in the healthcare industry. Biotechnology, medical devices, and pharmaceuticals have undergone transformations through the use of cutting-edge technologies like DL <cit.> and ML <cit.>. Currently, the healthcare sector is utilizing various forms of AI techniques for medical research and providing medical services. One such technique is the GPT features of NLP, which hold immense potential for the healthcare industry. GPT can help to overcome several challenges in healthcare in multiple ways. For instance, it can be used to develop intelligent systems that assist doctors in making accurate diagnoses and providing clinical assistance <cit.> <cit.>. GPT can also analyze large volumes of medical data and generate reports. Furthermore, it has potential applications in drug discovery <cit.> <cit.>, personalized medicine, patient diagnosis, medical image analysis, analyzing electronic health records, clinical decision support systems, and disease prediction.\n",
            "\n",
            "    \n",
            "  * Drug Discovery: Recent AI and machine learning techniques <cit.> <cit.> are having the potential to contribute to the growth and development of drug discovery. GPTs are capable of learning new patterns and relationships <cit.> in the dataset they were trained on. This capability can be used in drug discovery to aid in the identification and design of potential new drugs with desired properties<cit.>. One of the key challenges in drug discovery is finding compounds that can interact with specific parts of the body. GPT can help in this process by learning the patterns and relationships from large databases of known compounds <cit.>. GPT can be trained on large sets of chemical databases to analyze chemical reactions and their outcomes. This can help suggest potential combinations of new drugs using the analyzed data. These new drugs can also be analyzed using GPT to test their efficacy and toxicity. \n",
            "   \n",
            "    \n",
            "  * Diagnosis:\n",
            "    GPT can be used in medical diagnosis by analyzing patient data. It can help to analyze medical records and extract information such as patient demographics, symptoms, and medical history. This can help medical professionals provide effective patient care and improve outcomes. The recent release of GPT-4 has the ability to support multimodal information, allowing it to analyze images as input and produce text results as output <cit.>. It is recommended to use AI systems such as a GPT, as clinical support tools to assist medical professionals in diagnosing and treating patients, but they should not be relied upon as the sole source of medical advice or decision-making. GPT can also be used to identify rare diseases by analyzing patient's complete information. The authors in <cit.> have used a general-purpose GPT based on GPT-3 model for patient diagnosis and triage. The model has given a triage accuracy of 70% which was worse than a physician. But, in next subsequent weeks, the accuracy has improved to 92% which is close to the performance of a physician. In diagnosis, GPT-3 model has given 88% accuracy. For emergency cases, GPT-3 has given 75% accuracy whereas physician has given 94%. \n",
            "\n",
            "    \n",
            " \n",
            "  * Disease prediction: GPT has great potential in disease prediction <cit.>. By analyzing large amounts of medical data, including patient records, medical images, and clinical trials, these pre-trained language models can learn patterns and make predictions about the likelihood of a patient developing a particular disease. For instance, trained healthcare GPTs can be used to predict the occurrence of diseases such as diabetes, heart disease, and cancer by analyzing various parameters, including the patient’s medical history, age, family history, and lifestyle. It can also be used to predict the likelihood of a rare disease This helps in the early detection of high-risk patients so that medical personnel can take necessary measures and suitable medicines to reduce the risk of developing the disease. The medical practitioner and author in <cit.> have recommended using GPT-4 models’ ability of NLP in bariatric surgery.\n",
            "    \n",
            "    \n",
            "  * Personalized medicine:\n",
            "   The COVID-19 pandemic has highlighted that not all body systems are clinically similar. For instance, during the pandemic, medicines like Remdesivir and Tocilizumab have been effective for one category of patients but do not affect another category of patients with similar clinical metrics, as they progress from a mild or moderate level of infection to a severe stage <cit.>. This highlights the need for personalized medicine in today's world. GPT can be used to identify variable patterns of data to predict or classify hidden or unseen patterns, which can be used for exploratory data analysis. GPT provide the possibility of identifying personalized medicines <cit.> based on the clinical, genomic, and nutritional data of patients. The dietician and the author in <cit.> have observed that the utilization of Chat GPTs has significantly decreased obesity rates among patients by offering personalized recommendations regarding nutrition plans, exercise programs, and psychological support. This approach allows for the development of customized treatment plans that cater to the specific needs of individuals, leading to a more efficient method of treating obesity with the assistance of Chat GPT.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "While GPT is a powerful language model with numerous applications in healthcare, it is not without its challenges. The primary challenge is data bias. As GPT models are also learning models, the significant drawback of biasing is also applicable to GPT. GPT can be susceptible to bias. If the data used to train the model is biased, the model will learn from it and replicate the bias. This leads to incorrect treatment and predictions. Another challenge is the transparency of the model. GPT is complex to understand and interpret. This lack of transparency in technology can make doctors and medical personnel not believe in the predictions, which may result in a hesitancy to trust and adopt technology <cit.>. Another important concern is security and privacy issues. As it is a model to be trained on data, there is a huge amount of sensitive information about the patients to be used to improve the algorithm and its performance. This results in significant security and privacy concerns related to the use of GPT in healthcare. The final and important challenge is limited clinical validation. GPT are showing promising improvement in various fields of healthcare, such as drug discovery, and disease prediction. But still, their effectiveness and accuracy in medical research and clinical settings have yet to be validated. More research and clinical trials are required to prove that GPT can transform the medical industry with full trust.\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPT have the potential to revolutionize the healthcare industry by contributing to drug discovery, personalized medicine, clinical support in making decisions, diagnosis support, and disease prediction. This can be helpful for human beings to predict the disease in advance and treat it through proper medicine. However, there are significant challenges that are to be addressed, such as technology adoption, data bias, regulatory challenges, and security and privacy issues. It is so important to analyze and evaluate the benefits and risks of using GPT in healthcare and to continue to monitor their development and implementation.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Industry\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "An important economic transition from agriculture and handicrafts to large-scale industry and automated production was achieved by the industrial revolution. Efficiency and productivity were raised as a result of modern equipment, energy sources, and labour arrangements. New opportunities and challenges have been created as a result of the quick development of new technologies in both the workplace and other industries <cit.>. The utilization of big data is a well-known technology-driven trend. Nowadays, companies have access to enormous volumes of data that may be examined to uncover insightful information. Big data can help businesses make wise decisions and discover areas for development. AI is another innovation that is changing industries. AI systems have the ability to analyse complex data, automate procedures, and make wise conclusions <cit.>. This improves production by increasing its dependability, adaptability, and efficiency. The process of \"digitalization,\" which includes incorporating digital technologies into every element of business, is creating industries to become more flexible, efficient, and valuable. Businesses may automate tedious work, improve client experiences, and streamline operations by implementing digital solutions. In today's digitally-driven world, adopting technological advancements is essential for maintaining competitiveness and promoting growth.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Industry\n",
            "\n",
            "In industrial scenarios, GPT has the potential to be applied as a sustainability tool, assisting businesses in evaluating and enhancing their sustainability goals. Companies can improve supply chain tracking and query response by integrating pre-trained transformer models like ChatGPT with supply chain management platforms<cit.>. Additionally, GPTs can offer modifications to the production process that might increase efficiency <cit.>. GPT can also help users make knowledgeable decisions about how to use resources, allowing businesses to remain competitive while reducing their environmental effect. For example, the GPT-2 model has demonstrated efficacy in sentiment analysis, providing insightful data for numerous applications <cit.>.\n",
            "\n",
            "\n",
            "    \n",
            "  * Hospitality sector\n",
            "In the hospitality industry, hotels place a high focus on providing satisfying guest experiences. To ensure that every tourist is satisfied during their stay, this necessitates adapting to their requirements and preferences. Hotels may improve the guest experience in a number of ways by integrating GPT into their website or mobile application. Hotels may respond to consumer inquiries in a timely and precise manner by utilizing GPT <cit.>. Customers do not have to wait for human assistance when looking up information about facilities, booking procedures, or room availability. Customers' overall satisfaction with the hotel's services is increased as a result of the large reduction in client wait times. GPTs can also make it easier for visitors who speak multiple languages to communicate <cit.>. Hotels can offer a more inclusive and welcoming experience for visitors from other countries by removing linguistic obstacles. Hotels may provide their visitors with immersive and engaging experiences by combining GPT with AR technologies. For instance, customers can use their mobile devices to get AR guided tours of the hotel or nearby attractions, offering a distinctive and entertaining way to explore the surroundings and learn more about the hotel's amenities.GPTs integration into various aspects of the hospitality industry gives hotels the ability to deliver streamlined, tailored, and effective services, increasing client happiness and loyalty.\n",
            "\n",
            "  * Fashion: By providing highly customized user recommendations based on personal style, brand preferences, and particular clothing or accessory demands, collaborative filtering and AI algorithms have undoubtedly revolutionized the fashion business. The amount of personalization has been further increased in this context by the incorporation of GPT, dramatically altering the purchasing experience for customers <cit.>. Fashion platforms may analyse a significant quantity of user data, such as browsing history, purchasing behaviour, and style preferences, using the advanced capabilities of GPT to produce tailored recommendations. Fashion platforms can direct consumers towards clothing options that fit their desire for sustainable fashion by including eco-friendly fabric selections into the system. GPT improve users' general fashion knowledge and confidence while enabling users to keep up with the most recent trends. The image-text retrieval skills of GPT significantly improve visual search capability in fashion platforms <cit.>. Users may make more confident shopping decisions and minimize the need for returns by visualizing how various clothing items and accessories would appear on them without physically trying them on. The model may recommend the proper size for various brands and apparel products by taking into account a user's measurements, preferred fit styles, and historical data. The overall purchasing experience is enhanced and the frustration of wrong size is decreased. \n",
            "\n",
            "  * Sustainability:\n",
            "Sustainable development means addressing current demands without sacrificing the capacity of future generations to address their own needs. Goals for sustainable development can be attained by implementing GPTs in a variety of sectors, including manufacturing and corporate operations <cit.>. The models can estimate where energy saving measures would be most useful by analyzing past data and patterns to provide insights into energy usage, pinpoint problem areas, and recommend opportunities for improvement. GPTs can aid in identifying sustainability-related problems, creating plans and strategies to solve them, investigating brand-new sustainable activities, keeping track of advancements, and conducting routine reviews. Companies can choose activities that will have the biggest positive impact by grading tasks and actions according to their impact on sustainability <cit.>.\n",
            "The models can optimize supply chains for decreased carbon emissions, minimized waste, and improved resource efficiency by assessing elements including transportation routes, packaging materials, and supplier practises <cit.>. This results in more environmentally friendly production, distribution, and sourcing procedures.\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "There are many different industrial fields where GPT models can be applied; the three areas mentioned above are only a few. However, for optimal use, the industrial sector needs to be ready to adapt to a constantly changing environment. Public and corporate policies must be developed over the long term to promote the use of sustainable production techniques. For enterprises, deploying pre-trained GPT models can be a costly task. Continuous development and training are also required to accommodate new and evolving inquiries as client expectations change. Companies have to carefully assess the benefits and costs before implementing the GPT model because these continuing efforts raise the deployment cost <cit.>. For industries to fully benefit from GPT models, it is crucial to address issues with interpretability, data reliance, and ethical considerations. Industry may therefore take advantage of these GPT models' advantages, make wise decisions, and promote sustainable development.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPTs have the ability to have a positive impact on society and business operations. They can speed up operations like accounting, sales, and marketing, increasing productivity. But before they are widely used, ethical problems need to be fully investigated. Technology products will change as GPT models develop. To reap the benefits and reduce dangers, it is essential to solve interpretability and data concerns. GPTs can have a tremendous positive impact on businesses, society, and the economy when they are used responsibly.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Agriculture\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Traditional agriculture, a time-honored practice passed down through generations, sustains civilizations with its crop cultivation and livestock rearing methods. Rooted in a deep connection to nature, it emphasizes sustainability and local ecosystem understanding. Beyond providing sustenance and livelihoods, traditional agriculture preserves cultural heritage. However, it also faces challenges such as labor-intensive processes and shortages, inefficient resource utilization, vulnerability to pests and diseases, and limited access to real-time data and environmental impact. Today, by merging tradition with modernity, we have the opportunity to leverage technological advancements to enhance productivity, sustainability, and resilience while honoring the profound legacy of traditional agriculture for future generations.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Agriculture \n",
            "\n",
            "GPTs have the ability to overcome the challenges of agriculture. It offers valuable advantages to the agriculture sector. It acts as a comprehensive knowledge source, providing information on crop cultivation, pest management, and soil health. By analyzing real-time data, GPT assists farmers in making informed decisions regarding optimal planting times and resource allocation. It plays a crucial role in identifying and addressing crop diseases and pests accurately. Moreover, GPT enables precision farming practices by utilizing sensor data and satellite imagery, ensuring precise irrigation, fertilization, and pest control. Additionally, it provides market analysis and price prediction, empowering farmers to navigate market conditions and optimize pricing strategies. GPT also supports farm management and planning, optimizing crop rotation and resource usage. By facilitating agricultural research and innovation, GPT contributes to advancements in crop breeding and sustainable practices. Embracing GPT in agriculture enhances decision-making, efficiency, and sustainability, ultimately promoting improved productivity and food security. For instance, GPT-4 can educate farmers about new methods and goods and warn them of potential issues or possibilities by analyzing data from many sources <cit.>.\n",
            "\n",
            "    \n",
            "  * Improving Crop Yields:\n",
            "\n",
            "\n",
            "\n",
            "With its data analysis capabilities and real-time recommendations, GPTs plays a crucial role in enhancing crop yields. By examining historical yield data, weather patterns, soil conditions, and crop management practices, GPT identifies valuable patterns and correlations, providing insights and suggestions for optimal crop management techniques<cit.>. It enables precision farming by integrating data from sensors, satellites, and IoT devices, granting timely guidance on resource allocation for improved efficiency. Additionally, GPT aids in the early identification and management of crop diseases and pests, minimizing yield losses through precise and prompt recommendations. Moreover, GPT supports crop breeding and genetic optimization by analyzing genetic data and plant characteristics, expediting the development of high-yielding and resilient crop varieties. Therefore, GPTs data analysis and decision support capabilities significantly contribute to enhancing crop yields and maximizing agricultural productivity<cit.>.\n",
            "\n",
            "    \n",
            "\n",
            "    \n",
            "  * Pest Control:\n",
            "\n",
            "    GPT offers significant support in the realm of pest control in agriculture. By analyzing extensive data on pests, including their behavior, life cycles, and characteristics, GPT can provide valuable insights for effective control measures. It aids in early pest detection by analyzing sensor data and satellite imagery, enabling proactive interventions to prevent pest spread and minimize damage <cit.>. GPT also assists in determining suitable pest control methods tailored to specific crops and pests, considering factors like environmental impact and sustainability. Additionally, it contributes to precision pest control by leveraging real-time data to optimize timing and dosage of interventions, reducing chemical usage and resistance risks. It also aids in identifying natural enemies and beneficial organisms, promoting natural pest control mechanisms such as habitat diversification and companion planting. Through GPT's data analysis and recommendation capabilities, it empowers farmers with informed decisions, leading to more effective and sustainable pest management strategies, ultimately reducing crop losses and enhancing agricultural productivity.\n",
            "\n",
            "    \n",
            "  * Identifying Diseases and Soil analysis:\n",
            "\n",
            "    GPTs offer valuable assistance in disease identification and soil analysis within the field of agriculture. With its ability to analyze extensive data sets, GPT can accurately identify crop diseases by processing information such as symptoms, historical data, and disease patterns. This enables timely and effective disease management strategies<cit.>. Additionally, It plays a significant role in soil analysis by analyzing diverse soil-related data, including nutrient levels, pH, organic matter content, and soil composition. By interpreting this data, It provides insights into soil health and fertility, empowering farmers to make informed decisions regarding nutrient management, soil amendments, and cultivation practices. Moreover, GPT can identify complex interactions between soil conditions and crop diseases, helping farmers understand the relationship and take preventive measures accordingly. It also supports precision agriculture practices by integrating sensor data and satellite imagery to assess soil variations across fields, allowing for site-specific management strategies and optimized resource allocation. Furthermore, it also facilitates knowledge sharing and collaboration by analyzing and disseminating research findings, best practices, and disease outbreak information among agricultural communities. This collective intelligence enhances disease monitoring and control efforts on a broader scale. \n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "While GPT, provides significant benefits to agriculture, there are challenges to its implementation. GPT's effectiveness depends on the availability and quality of data, making insufficient or biased data a limitation. The interpretability of GPT's decision-making process is challenging due to its black-box nature, hindering trust and understanding. GPT's computational requirements and infrastructure can be demanding, posing difficulties for resource-constrained farmers. Language and domain-specific nuances can affect its performance, impacting accuracy and relevance. Ethical considerations surrounding data privacy and ownership need careful attention to ensure responsible use. By addressing these challenges, researchers and practitioners can unlock GPT's potential while ensuring its practicality and ethical implementation in agriculture.\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPT holds immense potential in agriculture, offering numerous benefits alongside notable challenges. Its data analysis capabilities empower farmers with informed decision-making in disease identification, soil analysis, and precision farming, leading to improved crop yields and sustainable practices. However, the effectiveness of GPT relies on data availability and quality, while its interpretability remains a challenge due to its black-box nature. Additionally, computational requirements, language nuances, and ethical considerations require careful attention. By addressing these challenges, the agricultural sector can harness the full potential of GPT, paving the way for more productive, efficient, and responsible farming practices.\n",
            "\n",
            "\n",
            " §.§ Travel and Transport\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Historically, animals have been used by people as their main source of transportation. But as the world's population increased, the demand for more effective transportation systems increased. Transportation-related technological advancements have fundamentally changed the sector in several ways. Business operations like order tracking, freight management, and customer support can be streamlined by automation employing AI-driven technologies. Companies can enable their employees to concentrate on more beneficial and profitable duties by automating these tasks<cit.>. With better transportation networks and logistics management systems that optimize routes and reduce transit times, technological developments also enable speedier delivery times. In terms of product development, technical advancement has paved the way for the development of innovative vehicles, infrastructure, and logistics systems, leading to the production of more sophisticated and effective transportation choices. Another noteworthy benefit of technology advancement in logistics and transportation is increased customer service. Inquiries and problems can be handled quickly and efficiently by chatbots and customer support systems powered by AI, improving the entire customer experience<cit.>.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Travel and Transport \n",
            "\n",
            "Companies can learn about customer preferences in real time by using GPTs in logistics and transportation, which results in better personalization and more customer satisfaction. GPTs leverage NLP approaches to interpret customer requirements and preferences, enabling customized suggestions as well as guidance in the logistics and transportation processes. The most effective routes and forms of transportation can be recommended using GPTs, which can analyse a large amount of data, including traffic patterns, weather conditions, and delivery requirements<cit.>. In addition, GPTs can be used as travel planners, allowing visitors to enter their travel budget, duration, and destination to create customized itineraries. For travel agencies, this personalized approach increases consumer satisfaction and revenue.\n",
            "\n",
            "    \n",
            "  * Logistical Management: GPTs can be quite important in the context of shipping logistics. They can automate the creation of shipping labels, eliminating up manual entry and lowering the possibility of mistakes. Additionally, GPTs can have access to real-time tracking data and can integrate GPS data and sensors to provide businesses and customers with precise and up-to-date shipment status information. Companies can successfully monitor shipments with the use of GPTs, geographic information systems (GIS), and routing algorithms<cit.>. Organizations can track shipments in real-time and ensure visibility throughout the supply chain by utilizing GPS data and sensor technology<cit.>. Customers can receive precise updates on their shipments using this real-time information, which will improve their experience overall. Overall, the use of GPTs into shipping logistics results in increased automation, efficiency, and client satisfaction. \n",
            "    \n",
            "  * Intelligent Fleet Management and Tracking: Companies can get real-time fleet updates by utilizing GPT models, which enables them to track vehicles quickly and precisely. GPT models' underlying technology also supports proactive fleet management. GPTs can identify possible problems or maintenance needs before they develop into expensive breakdowns or accidents by analyzing data from a variety of sources <cit.>. With this knowledge, companies may take preventative measures, such as planning maintenance or quickly fixing developing problems, ultimately saving time and money by preventing unintended delays. Additionally, GPTs can provide clever alerts and notifications. Businesses can receive alerts when vehicles arrive at specified areas by setting up specific triggers, which enables better coordination and customer service <cit.>. For instance, businesses can alert clients or storage facilities in advance of a truck's arrival, allowing for effective unloading and loading procedures.\n",
            "    \n",
            "  * Real-Time Inventory Tracking:  GPTs enable businesses to manage their inventory levels while on the road with a cloud-based platform that makes it simple to access inventory data from anywhere in the world. Better inventory management and decision-making are made possible by this real-time accessibility. This ensures that the appropriate quantity of stock is accessible when needed to fulfil consumer requests, while minimizing carrying costs and preventing lost sales as a result of stockouts. GPTs can streamline inventory management procedures by eliminating the need for human data entry into spreadsheets, saving time and cutting overhead costs <cit.>. With the advent of 5G technology, the cost of connected devices has dramatically decreased, making it more practical and affordable for businesses to set up and operate connected inventory monitoring systems. This may make real-time inventory tracking solutions more widely adopted, thereby increasing the effectiveness and precision of inventory management <cit.>.\n",
            "    \n",
            "  * Streamlining Delivery Operations: GPTs are able to estimate traffic trends and improve routes for both drivers and passengers using real-time data <cit.>. These models can produce effective routes that reduce travel times and enhance overall delivery performance by taking into account aspects like traffic congestion, road conditions, and delivery schedules. Route optimization not only reduces travel time but also benefits the environment. In order to improve air quality and create a more sustainable delivery process, it is possible to cut down on idle times and trip distances. Businesses may streamline operations, improve the overall customer experience, and contribute to a more sustainable and environmentally friendly approach to logistics by automating procedures, optimizing routes, and utilizing real-time data <cit.>.\n",
            "\n",
            "    \n",
            "  * Tourism: GPTs have the potential to significantly improve a number of tourism-related aspects. GPTs can offer customized solutions that suit the individual's preferences by understanding their needs and interests, resulting in a more pleasurable travel experience. GPTs are excellent at understanding and creating text that is human-like <cit.>. This functionality can be used in the travel and tourism sector to enable chatbots or virtual travel assistants to communicate with users in natural language <cit.>. Trip planning and information retrieval are made more simple and user-friendly by the ability of travelers to ask questions, look for advice, and obtain full details about destinations, modes of transportation, customs, and more. GPTs are capable of producing in-depth and interesting descriptions of tourist sites, attractions, lodging, restaurants etc. GPTs can provide time-efficient routes that guarantee a complete travel experience <cit.>. Including advice on local legislation, emergency contacts, medical facilities, and potential risks, GPTs can offer helpful information and direction regarding travel safety. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "Privacy issues may occur when using sensitive data in travel GPTs. It is essential to manage user data sensibly and putting up strong security measures to safeguard private data. The quality of the model's outputs is directly influenced by the correctness and completeness of the data utilized during the training phase. Ethical considerations should be taken into account when creating AI-powered applications employing GPTs. It's crucial to check that the models are truthful, unbiased, and free from harmful presumptions or discriminatory procedures <cit.>. Although the models contain advanced features, they are difficult to tailor for specific use cases, need a lot of data to train, and have built-in limitations.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "Emerging GPTs have the potential to enhance productivity, communication, and the calibre of goods and services, which will benefit many aspects of people's life. GPTs can offer real-time updates, effective route optimization, and customized recommendations in the travel and transportation industries, enhancing the overall travel experience and increasing operational effectiveness. Adopting them, however, comes with some difficulties. As specific roles are replaced by automation, GPTs may result in job displacement <cit.>. Additionally, the computational and memory requirements for GPTs make their deployment on compact or low-power devices difficult. GPTs may not be accessible to growing businesses due to the high costs associated with obtaining and using them. Despite these obstacles, attempts are being done to overcome them and improve the usability and value of GPTs for a larger range of users.\n",
            "\n",
            "\n",
            "\n",
            " §.§ E-Commerce\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Electronic commerce, commonly referred to as e-commerce, is a way for conducting economic transactions and create relationships between groups of people and entities using digital information processes and electronic communications <cit.>. Globally, this type of trade has experienced substantial growth, particularly in the retail sector. The preference for internet shopping, especially among younger millennials, is a prominent trend in consumer behaviour. Mobile devices have consequently taken over as the main method for carrying out internet transactions <cit.>. Therefore, it is crucial for e-commerce companies to give the customer experience in their mobile applications top priority. The provision of brief text summaries for titles and reviews is an essential component of this. These summaries are essential for optimizing search results, helping consumers identify appropriate items, and ultimately raising customer happiness in the online purchasing space <cit.>.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in E-Commerce Realms\n",
            "\n",
            "The e-commerce sector could significantly advance with the introduction of GPTs. GPTs can be accessed by users or customers and are intended to answer commonly asked questions and give in-depth details about many elements of the e-commerce process, such as products, delivery, refunds, and more <cit.>. One of the main benefits of GPTs is their capacity for quick responses, which decreases the amount of time customers must wait to hear back from businesses <cit.>. By taking care of an important number of client inquiries, this function not only increases customer happiness but also lessens the workload on support workers. Customers will ultimately have a better purchasing experience as a result of being able to quickly acquire the information they require and interact with GPTs <cit.>.\n",
            "\n",
            "    \n",
            "  * Proofreading: To improve the calibre and accuracy of written content in e-commerce, GPTs can be used for proofreading. Written content is essential for product descriptions, marketing materials, customer reviews, and other text-based components in the e-commerce sector <cit.>. For the purpose of projecting professionalism, fostering trust, and delivering a satisfying user experience, this text must be devoid of errors, well-written, and grammatically correct. E-commerce companies can automate the process of identifying and correcting these problems by using GPTs for proofreading, which saves time and effort as comparison to manual proofreading <cit.>. This can be especially helpful in situations when users are writing product reviews or interacting with customer service. An improved user experience is facilitated by the early detection and rectification of errors, which also helps to avoid potential misunderstandings or miscommunications.\n",
            "    \n",
            "  * Order Processing: GPTs are useful in many areas of order management and customer service because they can comprehend and produce text that looks like human speech.  GPTs can help with handling consumer questions about orders <cit.>.  GPT is capable of interpreting the queries, providing important details like order status, tracking information, and expected delivery time, as well as suggesting corrections for frequent problems <cit.>. By delivering real-time information, GPTs can assist customers in tracking their orders. Customers can customize their purchase with the help of GPTs. GPTs can help in the identification of possibly fraudulent orders by examining past transaction data, consumer behaviour patterns etc <cit.>. Based on a customer's past purchases, browsing habits, and preferences, GPTs can offer tailored product recommendations. When a consumer puts a purchase, the model can examine the information and produce recommendations for related or supplementary products that the customer might find interesting.\n",
            "    \n",
            "  * Generating titles for products: Companies can use GPTs to produce interesting and educational material to improve the appeal of their product listings <cit.>. Based on a product's category, brand, and special characteristics, GPTs can come up with attractive titles for it. The model can produce imaginative and memorable names that aid in brand awareness and differentiation by receiving relevant information such as the characteristics of the product and the target market. GPTs are trained to produce in-depth and interesting product descriptions <cit.>. These summaries can offer a thorough summary that aids clients in selecting products wisely. GPTs are capable of coming up with clever and appealing captions for product images. GPTs can be adjusted to better reflect the tone and aesthetic of a certain brand <cit.>. As a result, the brand identity is consistent and unified throughout all product listings.\n",
            "    \n",
            "  * Strategy Planning: GPTs have the ability to come up with original and distinctive concepts for marketing campaigns <cit.>. The model can provide recommendations for different campaign aspects, such as slogans, taglines, themes, contests, social media strategies, and more by taking into account relevant information about the product, target audience, marketing objectives, and desired outcomes. GPTs can help with email writing that encourages readers to become partners, investors, or customers <cit.>. To increase the likelihood of a favourable response or interaction, these emails can be customized to address the needs and potential benefits for the receivers. To improve their comprehension and production of appropriate material, GPTs can be trained on domain-specific knowledge bases, such as e-commerce <cit.>. The models can offer more precise and situation-specific recommendations for advertising strategies, product positioning, and target audience interaction because of this specialized training. \n",
            "    \n",
            "  * Data analysis: There are numerous ways to use GPTs for data analysis in e-commerce. E-commerce data preparation can be aided by GPTs <cit.>. Data normalization, cleansing, and formatting are a few of the duties involved. GPTs can produce summaries, identify significant topics, and extract appropriate data by studying textual descriptions, reviews, and consumer feedback. This helps you know the data more thoroughly, identify trends, and find insightful information. Customer reviews and social media comments from e-commerce can be analyzed for sentiment using GPTs. The sentiment expressed in text can be evaluated using GPTs and categorized as either positive, negative, or neutral <cit.>. Understanding client views, recognizing product strengths and deficiencies, and making data-driven decisions all benefit from this analysis, which also helps to increase customer happiness. Segmenting consumers based on preferences, behaviours, or past purchases can be aided by GPT models <cit.>. For the purpose of detecting fraud in e-commerce transactions, GPTs can be used. GPTs can support the identification of potentially fraudulent actions by examining past transaction data, user behaviour patterns, and recognized fraudulent tendencies <cit.>. \n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "While GPTs have a lot of potential for numerous e-commerce applications, they also have several drawbacks. In order to produce responses, GPTs mostly rely on the context given in the input text. They could, however, find it difficult to fully understand the broader context or details that are unique to e-commerce. GPTs provide replies using training data and prior knowledge. They are unable to access real-time data or carry out real-time calculations <cit.>. They might not be appropriate for giving current information, such as pricing, product availability, or dynamic promotional offers. GPTs gain their knowledge from a wealth of training data, which includes text taken from the internet, which may be biased, stereotyped, or otherwise offensive <cit.>. The models may unintentionally provide biased or unsuitable replies if they are not rigorously managed and monitored, which could be harmful to the customer experience and brand reputation. The use of ethical principles and the training data must both be given careful thought.\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "The conversational interface offered by GPTs customizes the purchasing process and makes interactions with clients more interesting and appropriate to their individual requirements. GPTs can also be utilized to get insightful customer feedback. Businesses can learn about customers' preferences, issues, and opinions regarding their products and services by conversing with them. In order to better serve their target audience, organizations can use this information to discover areas for improvement, increase customer happiness, and make data-driven decisions. \n",
            "It's essential to recognize that GPTs might occasionally make mistakes or give poor answers, particularly when dealing with complicated or ambiguous queries. This highlights the necessity of continual model training, thorough testing, and modification to guarantee that they consistently meet consumer needs. To confirm the efficacy and dependability of using GPTs specifically in the e-commerce area, more research and testing are required. When implementing GPTs, it's critical for businesses to take into account the particulars of their own e-commerce businesses, their target market, and the type of the client enquiries. Regular monitoring and feedback analysis, along with a systematic and iterative approach, can help make sure that the outcomes of using GPT models are in line with the objectives of e-commerce enterprises.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Entertainment\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "In the ancient days, the Entertainment meant about playing games with neighbors covers all outdoor games, indoor games and chatting with neighbours through telephone. As digitization has bought greater advancements in computation and communication, in turn access to internet is also much easier. This has changed the way people are entertained. as people are connected and fully engaged in completing the target for the day. And there was a radical shift from traditional employment to employment in the Industrial Revolution age. Stress and pressure are common factors hindering people of different age groups. The different forms of entertainment serve as stress busters. Entertainment and mental health are interrelated; the former transfers happiness, bringing harmony and peace to mental health. Some common forms of entertainment include playing games, watching TV series or movies, or funny videos, shopping, debugging, coding, browsing the internet, listening to music, dancing, chatting, painting, crafting, reading books, cooking, and many more, which can lessen the stress carried<cit.>. Entertaining and getting entertained is the biggest motivation and medicine for all mental illnesses. Entertainment helps to improve the motor skills of humans, thereby inducing a positive cognitive effect towards the work.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT on the Entertainment Industry\n",
            "\n",
            "GPT is a potential game-changer in the entertainment field, delivering endless entertainment. Since its evolution, GPT models have been adopted as an entertainer crosschecking their ability to produce content on funny and illogical questions. GPTs entertain people in many ways, and of course, using GPT itself an entertainment as it reduces the burden of overthinking by providing immediate feedback to queries in seconds<cit.>. The results are amazing and have been utilized for many purposes today. When the GPT model was probed to complete a scene from the movie “Frozen,” it responded with an entertaining writeup<cit.>. Some of the impacts of GPTs on Entertainment applications are given below:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  * Solitude with GPT: As the GPT itself is an entertainer, one can feel better alone with the GPT, which helps to come out of loneliness by exploring its savors<cit.>. GPTs assist in providing soothing poems, mental healing quotes, and funny riddles. People with loneliness may feel anxiety, especially with older ones at home. In this case, GPT-4 helps people with its Voice Technology feature, enabling users to input their audio<cit.>. In turn, the GPT model responds to user-specific speech output using NLP algorithms embedded with it. The elderly can feel safe and attentive at home. GPT-4 is multilingual and can understand various dialects and accents for personalized user experience.\n",
            "\n",
            "\n",
            "  * Enhanced Customer Interaction: The advent of ChatGPT and Bard has improved customer interactions on content such as movies, Over-the-Top (OTT) platforms like Netflix, Hulu, Disney+ Hotstar, and prime video, sound recordings, song lyrics, pictorial works, comics, jokes, memes, viral videos, and other entertaining factors. Further, GPTs provide human-like recommendations on user-specific fun activities based on user interactions for an immersive experience. This has dramatically improved the interactions in the engagement industries. User engagement can be further improved by providing dynamic and more realistic responses to user queries, such as creating virtual actors for interacting with real actors<cit.>.\n",
            "\n",
            "\n",
            "  * Personalized Content Creation: GPTs can help generate user-specific personalized content by analyzing the user preferences and generating content like predicting future scenarios tailored to the user’s interest. GPTs can be used for creating personalized, engaging, and high-quality content for online business advertising, ideas for content generation, marketing messages for attracting customers, descriptions for selling products, and captions for social media<cit.>. In addition, it can be used for optimizing the contents for search engines, i.e., GPTs will provide relevant terms for search, thereby avoiding traffic to the web sources.\n",
            "\n",
            "\n",
            "  * For the Film and TV industry: GPT-powered virtual assistants assist users in booking tickets and generating content and personalized recommendations using AI models. The evolution of GPT-4 with advanced NLP and DL algorithms helps the scriptwriter to generate AI-driven content without the human author named virtual storytelling<cit.>. GPTs create interactive stories, dialogues, and characters, recommending suitable characters. Furthermore, GPTs can be used to create content for video games, voice-enabled applications, AR applications, and other VR experiences in virtual worlds<cit.>.\n",
            "\n",
            "\n",
            "  * For Social media influencers: GPTs can generate personalized marketing ads for each customer based on their previous interactions and provides relevant suggestions for customer viewing experiences. Youtubers and other social media content creators will potentially benefit from generating channel content based on demand and realistic societal activities.\n",
            "\n",
            "\n",
            "  * Realistic Gaming Interactions: GPT helps to generate the players, gaming narratives, dialogues, user interface, and user-specific gaming recommendations and new game creation. Powerful HCIs can render a better user experience for game developers and players. Assistance to the game developers in debugging and enhancing the code developed. GPT uses various NLP and AI algorithms trained with massive data to predict the next phrase/movements and provide human-like experiences in 3D gaming environments. ChatGPT has been integrated with AR and VR to provide an immersive gaming experience.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "Latency is the major issue connected with rendering the voice-based response to the voice input. As well, plausible misinterpretations may mislead the responses, and interruptions to the relayed output are difficult. Enabling technologies like EC and 5G can help overcome this issue. Also, GPTs must be capable of storing the facts with audio conversations to relay them while conversing the other day. Furthermore, the AI system must be built in such a way that it can continuously learn (lifelong machine learning) and enhance over time. The major ethical concern with virtual storytelling is the bias exposed in the training data and the obscurity of reproduced content on the generated stories. Another issue with the generated content is plagiarism (i.e., producing content similar to the content in the published articles or books), raising disputes with intellectual property rights. In addition to this, the source of the content generated remains unexplored. The language barriers in using GPT must be lessened to improve user experience and utilize the features of GPT <cit.>. The implication of the user to provide inputs in a certain format to GPTs can be further improved by providing different options in addition to voice-based inputs GPT4, like braille screen input for visually disabled people. The user authentication can also be further enhanced to safeguard user-specific content generation and avoid repeated content generation for users with similar requests. One of the primary concerns with the GPTs adoption is job loss. Content creators, bloggers,  and poets may lose their jobs.\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "The entertainment industry is the one which will be in demand always, as it is a lifeline for many individuals leading a stressful work environment or personal life. Despite the stress, entertainment has become part of routine life due to its immersive nature, creating harmony in the mind and the environment. GPTs have made a major contribution to enhancing the entertainment industry, but the job security of many professionals in this field remains unanswered. GPTs must be trained on unbiased data and ensure transparency in source content generation to provide a secure, robust, and efficient contribution to the entertainment industry. To attract all types of users, the multilingual capability and content rendering of GPTs can be further enhanced. The issues constrained by providing user inputs to GPTs can be alleviated to all extent.  Furthermore, safer user content generation without plagiarism and relating facts with previous conversations can be guaranteed by abiding by the storage requirements to deal with a more personalized user experience.\n",
            "\n",
            "\n",
            " §.§ Lifestyle\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Lifestyle, the way of our living, is one of the prominent areas most people in today’s digital era of AI,  are bound to and look for constant improvement. The “modus vivendi” is a Latin expression that semantically means a way of living and should be understood in terms of values and attitudes. These two terms manifest self, influenced by family, society, and global media. Directly or indirectly, these influence an individual’s lifestyle. Adopted from a sociological perspective, an individual expresses oneself through different practices, viz., eating ways, drinking behaviours, mode of travel, travelling places, costume designs, body-shaping cloth to wear, media preferences, education choices, job preferences, entertainment modes, managing leisure time, means of communication and so<cit.>. In all these practices, individuals would like to explore and learn about what, where, how, and when factors for reading sustainable development<cit.>. The concept of lifestyle is all about “how one wants to live one’s life.”  Consumerism is the act of purchasing artifacts for societal status and is one of the thriving lifestyle factors. Certain standard indicators like job, wealth, and physical and mental health determine the quality of one’s life. Also, the choice of a healthy lifestyle moderately determines the health of an individual<cit.>. Furthermore, few people believe that lifestyle reflects their socioeconomic status. Many epidemiologic studies state that better lifestyles have dramatically reduced the risk of various chronic diseases and are the primary cause for their prevention<cit.>. The lifestyle has been defined on different societal levels from individual, positional, and national to global<cit.>.  At the global level, lifestyle is adopted by general world-class influencers. In contrast, at the national level, the influencing factors will be the government and different cultural patterns across the country. The positional level concerns influence from different status groups, age categories, gender groups, and social classes. And the individual level is influenced by a closely moving group of individuals concerned about self-identity. The major source of information about these influencers is the Internet through social media networks and personal development advertisements. \n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Lifestyle\n",
            "\n",
            "The most remarkable application of AI, the GPT, paves the way for the betterment of mankind in offering human-like intelligent conversation on all whereabouts. People will always prefer to interact with other peers to learn their attributes and tweak them for societal status. Various GPTs have flourished for different lifestyle indicators, and they provide human-like assistance to all queries on fine-tuning the lifestyle by harnessing the power of AI<cit.>. The advanced reasoning capability of GPT-4 serves the purpose better<cit.>.\n",
            "\n",
            "\n",
            "  * Diet Planner: Free GPT applications for maintaining a balanced diet, helping the individual with a weight loss diet plan, followed by a brief list of meal plans, required shopping lists, physical activity plans targeting particular body parts<cit.>, motivational messages, and personalized sleeping patterns.  These apps act more like personalized training assistance and help to track progress with visualization charts or graphs. Fitness level, available free hours, medications taken, and available exercise equipment will be given as input to GPT.\n",
            "\n",
            "  * Travel Guide and Trip Advisor: Harnessing AI models, GPT provides an individual’s travel plan itinerary based on information like the place(multiple cities), budget, and the number of days. These GPTs provide local recommendations on restaurants, hotels, and other attractions. RoamAround, Roamr, and VacayChatbot are some of the travel planning GPTs<cit.>.\n",
            "\n",
            "  * Personalized Stylist and Beauty Advisor: GPTs can act as personalized stylists for an individual by generating occasion-specific clothing and costume preferences.  GPTs can assist in organizing wardrobes based on seasonal outfits and provide recommendations on e-commerce fashion stores for purchasing favourite brands. GPTs can provide tailoring design options, fabric choices, and design materials. Furthermore, GPTs can provide updates on a stock based on the preferred searches and provide insight into fabric types suitable for weather conditions that suit personal style.\n",
            "\n",
            "  * Personalized CookBook: GPTs can serve as cooking assistants by recommending new curated recipes suiting the family dietary plan, ingredients available, time, individual’s cooking skills, and new flavoured ingredients.  ChefGPT, PantryChef, and MacrosChef are some GPTs that generate unique and delightful recipes<cit.>. Consequently, GPTs can assist in shopping list recommendations and the nutritional value of the recipe generated.\n",
            "\n",
            "  * Hobby Curator : GPT assists an individual in identifying one’s enjoyable leisure time activity by learning new skills<cit.>. Having a list of interests and ideas ready, the GPT helps narrow down various options, instructional videos to proceed, chatting and sharing with online communities, and researching the cited hobby to explore more fun. Budget will also be an important factor in this perspective, as learning new hobbies may require joining paid classes or courses. GPT provides step-by-step instructions and guidelines to learn a new skill faster.\n",
            "\n",
            "  * Dream Maker: GPTs with multimodal learning helps to search for a job based on one’s qualifications and experience. In turn, it assists in preparing the job-specific resume, cover letters, training for the interviews (coding and technical queries), and grooming sessions and can redirect to the training place where knowledge can be acquired<cit.>. The futurist GPT models can assist in phase-by-phase questionnaires in the interview process\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "The recent version of the GPT uses both reinforcement and supervised learning models so it can learn based on the interaction with the user and can use existing data to derive personalized decisions. In the context of lifestyle, GPTs offer the most promising solution for almost all lifestyle influencers, but the still challenging part is the trustworthiness of the data and copyright issues. Also, relying more on GPT as it solves all our problems may insipid human intelligence in upcoming generations. Though the GPT provides weight advice, it can never be a substitute for the medical practitioner, as some information can be misleading. Travel planning GPTs sometimes require users to update information in a specific format and may have outdated databases. GPTs cannot access specific job openings' websites but can still provide insights into acquiring them. At times, it can produce nonsensical information <cit.>. Therefore, before adopting the GPT recommendations fully, further instigation is recommended. Furthermore, developing a large multimodal learning model abiding huge and dynamic datasets will be costly. \n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPT is a personalized assistant for improving an individual’s lifestyle from various prospective influencers. Generating personalized recommendations alleviates an individual’s fear of survival in the digitized society. Individuals will be personally trained to adapt to different cultural and technological shifts in the sustainable development of themselves and the economy as a whole. On the other hand, more stringent recommendations may incur huge budget overruns and sometimes provoke the individual to misinterpret, leading to dreadful consequences. GPTs provide both positive and negative recommendations based on the input fed. So, for the effective adoption of a GPT for lifestyle practices, adverse training and testing on extreme behaviours must be carried out. GPTs must be trained in the realistic and dynamic perception of individuals in real life.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Gaming\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Before the advent of technology and the gaming industry, entertainment was primarily centred around activities such as reading, listening to music, watching plays and movies, participating in sports and physical games, and socializing with friends and family. People also engaged in traditional board games and card games, which were often played in groups and provided a fun and social way to pass the time. After technology stepped into the gaming industry, the way games are created, and the experience it has given users have transformed tremendously. Technology has enabled developers to create more immersive and engaging experiences for players. It has contributed in various ways, like improving graphics, performance, online play, and mobile gaming. Improved GPUs and other technologies allow for more detailed and realistic graphics, making games more visually stunning. Faster processors and higher amounts of RAM allow for smoother gameplay and faster loading times, reducing lag and improving overall performance. Technologies like AI, AR, and VR have created a new dimension of game development and experience. Players can now immerse themselves in gaming worlds in a way that was not possible before. With the help of advanced AI techniques, game developers can create more sophisticated and challenging opponents for players, as well as NPCs with more realistic behaviours. Technology has greatly expanded the possibilities of gaming and enabled developers to create more immersive, visually stunning, and engaging experiences for players.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Gaming \n",
            "\n",
            "\n",
            "GPTs have the ability to contribute to all sectors, including the gaming sector. GPT are not specifically designed for creating and playing games, but they have the potential to improve the gaming experience by improving enhanced dialogue and story telling, creating dynamic and personalized gaming worlds, generating more realistic and engaging characters <cit.>, game content creation, chatbot development. \n",
            "\n",
            "\n",
            "    \n",
            "  * Chatbot development: GPTs have been used in gaming through the development of chatbots that use NLP to communicate with players <cit.>. Because it allows the chatbot to understand and respond to a wide range of user inputs and queries related to the game. GPTs have been pre-trained on a large corpus of text data, which makes them adept at NLP. It can understand and respond to user queries in a way that feels natural and intuitive. It can also understand the context of a user's query, which means they can provide relevant and useful responses even when the user's query is ambiguous or incomplete. It can also generate game-related content, such as descriptions of game characters or settings, that can help to enrich the user's gaming experience. Furthermore, it can also personalize the user's experience by learning from their previous interactions with the chatbot and tailoring its responses accordingly.\n",
            "  \n",
            "    \n",
            "  * Game content creation: GPTs are used in game design. They are used to create game content such as levels, items, and quests. If the game designer is working on a new role-playing game, GPT can be used in creating characters to be used in the games. To generate new character classes in the games, the developer has to give inputs that contain information about the game environment, game settings, player abilities, and game play mechanics. GPTs has the ability to analyze the text and expectations given by the developer, and it can generate a list of potential character classes based on the expectations given as text. The designer then refines the ideas and chooses a more suitable character to develop further with unique abilities and game mechanics. The authors in <cit.> have used GPT2 and GPT3 to procedurally generate role-playing game with video game descriptions. The resultant quest  was evaluated by 349 online RPG players. The results concluded that one of the five quest descriptions was accepted for game development.\n",
            "    \n",
            "  * Analyze player’s ability and skill: GPTs can detect and analyze players' abilities and skill levels and tailor the game accordingly. This analysis helps in making dynamic modifications to the game environment based on the player's abilities and skill levels. This feature helps achieve dynamic difficulty balancing. GPTs can also assist in identifying the player's intent. Thus, when players ascend to higher levels, it can assist in making the games more challenging based on the player's abilities and skill levels in the previous levels\n",
            "    \n",
            "  * NPCs: NPC stands for \"Non-Player Character.\" In AI games, NPCs refer to characters or entities in a game that are not controlled by a player. NPCs can take on a variety of roles within a game, such as enemies to fight, quest givers, merchants, or friendly characters that provide helpful information. They are often controlled by AI algorithms that determine their behaviour and actions within the game world. GPTs are not specifically designed for creating NPCs, but they can be used to generate dialogue and other character interactions that can be incorporated into NPCs. Additionally, It can be used to generate character backstories and personalities, which can inform the development of NPCs. The authors in <cit.> have trained and used GPT-2 for text generation of video games. They have trained GPT-2 on a large corpus of video game quests and used a GPT model to generate the dialogue for quest-giver NPCs in role-playing games. The output has shown that GPT can learn the structure and linguistic style of the games, and the quality of the content it has generated is high, making it a good alternative to writing new RPG quests by hand.\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            " GPTs are computationally expensive and require high computing resources to do their purpose. This means that implementing them in a game would require powerful hardware and this could have an impact on the performance of the games.\n",
            "Lack of training data: GPTs require large amounts of high-quality training data to be effective. In the gaming industry, this could be difficult to obtain, as gaming data are likely to be fragmented and less structured than the kind of data used to train GPT models <cit.>. In addition, GPTs can perform content creation based on patterns they have learned from their training data, which means that they can be unpredictable. The content generated by GPT may be nonsensical or inappropriate content to the game. In the context of gaming, this lack of control could lead to undesirable or even offensive game content. GPTs can generate text based on user input, they can't interact with the game environment in the same way a human player can. This limits their usefulness in gaming and may make them less effective than other AI technologies.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPTs can transform the gaming industry by contributing to improved game dialogue creation, enhanced non-player characters, personalized gameplay, procedural content generation, chatbot generation, and analyzing players' abilities. However, it also has potential challenges that are to be addressed, such as the need for high computing resources, a lack of control over content creation, and restricted interaction with the game environment. In addition, the most important challenge in adopting a GPT model in gaming is a lack of training data. If the challenges are addressed and the gaming industry evolves with properly structured data to train a GPT model, then GPTs can revolutionize the field of gaming.\n",
            "\n",
            "\n",
            " §.§ Marketing\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "\n",
            "Traditional marketing primarily relied on traditional media channels, such as television, radio, newspapers, and magazines, to reach consumers. Companies used to develop marketing campaigns based on demographic data, and mass media channels were used to broadcast these campaigns to a broad audience. However, the advancements in technology have brought about significant changes in the marketing industry, and companies are increasingly integrating new marketing strategies evolved through technologies to reach and engage with customers. One of the significant transformations has been the rise of digital marketing channels such as social media, search engines, Email, and mobile applications that allow companies to target specific populations with precision and provide real-time feedback on campaign performance, allowing for more effective and efficient marketing. Technology has also given rise to marketing automation tools such as customer relationship management systems, chat-bots, and personalized email marketing, which have made marketing more efficient and effective. Another significant transformation has been the use of big data and analytics to better understand customer behaviour and preferences. This has allowed companies to create more personalized and targeted campaigns based on specific customer needs and preferences.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Marketing \n",
            "\n",
            "The marketing industry has evolved with various AI-powered techniques. This revolution started in marketing by providing businesses with powerful tools for generating insights, automating processes, and improving customer experiences. GPTs are also being used in marketing to generate engaging and personalized content. Some of the applications of GPT in marketing include content creation, customer service, and personalized advertising.\n",
            "\n",
            "\n",
            "\n",
            "    \n",
            "  * Content creation: GPTs can contribute to marketing in various ways, such as by improving speed and efficiency in content creation, ensuring consistency and quality of content, generating personalized content, creating multilingual content, and repurposing existing content. It can be trained on a company's existing marketing materials and customer data, allowing it to create new content, such as blog posts, social media updates, and product descriptions, in a fast and efficient manner. Despite its speed, it maintain high standards for quality and consistency. Moreover, GPTs <cit.> can generate personalized content based on customer data, such as search history and past purchases. This helps create content that is relevant to the users' desires, leading to better engagement and conversion rates. GPTs can also generate content in various languages, allowing marketers to expand their reach across regions. Copy.ai <cit.> has used GPT-3 to generate human-like text that is optimized for marketing purposes such as website copy, social media posts, advertisement copy, and email campaigns. This means that marketer personnel no longer focus on content creation. Instead, they can spend productive time improving the other aspects of marketing. \n",
            "    \n",
            "  * Customer service: GPTs can be trained on customer service conversations and chat logs to generate more natural responses, like humans. This can help business personnel provide better customer service 24/7 and save time and resources. It can be trained to generate automated responses for frequently asked questions, providing faster responses to customers and ensuring consistency in the quality of replies. GPTs can also analyze customers' emotions and sentiments, enabling businesses to proactively address negative feedback. This is particularly helpful in maintaining customers' trust. The authors in <cit.> have used GPT-3 model for automated drafting of responses for incoming mails. They used it to understand the mail, and then software engineering and business studies were used to understand the challenges encountered and finally, the response generated after a thorough understanding of the context of the mail. The authors have concluded that applying GPT-3 to rationalize email communication is feasible both technically and economically.\n",
            "    \n",
            "  * Personalized advertising: GPTs can generate personalized content such as product descriptions, blog posts, and social media captions tailored to individual customers' preferences and interests. This can help businesses create content that resonates with their target audience, leading to higher engagement and conversion rates. By analyzing customer data, GPTs can segment customers according to their behaviour, interests, and preferences. As a result, businesses can tailor their marketing campaigns to each segment and provide personalized messaging and offers that are more likely to connect with each customer group. The authors in <cit.> have proposed a generative model to identify the name of the product from the product text and use this information filter to improve the product recommendation based on the product retrieval model. This method has been implemented in the dynamic product advertising system of Yahoo. It is observed that the recommendation system has recommended the product based on the user's interest, and it was evaluated using an A/B test to serve similar products in an ad carousel, which can help the system to explore more products efficiently.\n",
            "    \n",
            "  * Forecast analysis: Using customer data analysis, GPTs can forecast future behaviour and buying patterns. This allows businesses to customize their marketing campaigns to each customer's desires based on their purchase patterns, increasing the likelihood of conversion or purchase. The authors in <cit.> have used chatGPT to perform predictive modelling based on past data. They have used the GPT model to predict the future based on the customer’s behaviour and buying pattern. This primarily helps the system to recommend the products to the customers as per their desires. \n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "GPTs are designed to generate content that imitates human writing, but the content generated may not align with the brand's image or message. This lack of control can be a potential challenge for marketers. Another challenge that applies to all learning technologies is that data bias is possible in GPTs <cit.>. Based on the large dataset of text used for training, if the data is biased, it will affect the generated content, which may also exhibit the same biases. GPT is complex and difficult to interpret, making it challenging to explain how the model arrived at its conclusions. This lack of transparency can lead to a lack of trust in adopting GPTs, and marketing teams may struggle to make improvements in their strategies. As like every AI technology, there are ethical concerns associated with GPT models. For instance, the use of GPT in marketing could raise concerns about the use of personal data and privacy, particularly if the model is used to generate targeted advertising or personalized content. To avoid any negative consequences, companies must ensure they use these models ethically and transparently.\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "\n",
            "Using GPTs in marketing can provide various benefits, such as better content creation, personalized messaging, increased efficiency, competitive advantage, and enhanced customer experience. However, this strategy also involves potential challenges, such as limited control, data bias, lack of transparency, and ethical considerations. Therefore, companies must consider the advantages and drawbacks of GPT adoption in marketing, and implement these models ethically and transparently to avoid negative outcomes. Successful integration of GPTs in marketing requires proper planning, a skilled workforce, and continuous monitoring to ensure the desired results and mitigate any potential risks.\n",
            "\n",
            "\n",
            " §.§ Finance\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "The finance industry, also known as the financial sector, is a broad term that encompasses a wide range of institutions and businesses that provide financial services to individuals, businesses, and governments. The finance industry plays a critical role in the global economy, facilitating the flow of funds between savers and investors, managing risk, and providing financial services and products to support economic growth. The finance industry has been the leader in technology adoption in recent years, with a focus on improving efficiency, reducing costs, and delivering better customer experiences. The adoption of technologies like big data and analytics, mobile and digital payments, blockchain and distributed ledger technology, AI and ML, and cloud computing make the sector more flexible, scalable, trustworthy, transparent, secured, and easier to access.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Finance\n",
            "\n",
            "GPT has greatly influenced finance by automating customer support using chatbots and virtual assistants, enhancing fraud detection, offering investment insights and recommendations based on financial data and news, assisting with risk assessment for investments and loans, impacting algorithmic trading strategies, simplifying compliance with regulations by analyzing legal documents, improving credit scoring and loan processes, and emphasizing the importance of handling sensitive financial data securely and transparently.\n",
            "\n",
            "\n",
            "\n",
            "    \n",
            "  * Sentiment analysis: Sentiment analysis is a technique used in the finance industry to evaluate the sentiment of investors <cit.>  and the general public towards specific companies, industries, or markets by analyzing news articles, social media posts, and other text-based sources of information. GPT has the potential to improve sentiment analysis in finance by providing more accurate and detailed analyses of financial data. With sentiment analysis, the industry can predict stock prices by assessing the sentiment of news articles, social media posts, and other sources of information about a particular company or industry to make informed investment decisions. By utilizing sentiment analysis, GPTs can aid financial institutions in identifying potential risks and taking appropriate action to mitigate them. The authors in <cit.> have investigated how incorporating a lexicalized ontology can enhance the performance of aspect-based sentiment analysis by extracting indirect relationships in user social data. The investigation results show that the analysis has given 98% accuracy.\n",
            "    \n",
            "  * Financial forecasting: GPTs can be trained on past financial market data to predict future trends in the stock market, exchange rates, and other financial metrics. This can help investors and financial organizations make more accurate predictions and reduce their risk exposure. With the ability to analyze and process the natural language, GPTs can be used to analyze and interpret financial data, news, and other related information. Financial analysts and researchers can use the ability to analyze natural language to extract insights from unstructured data like news articles, social media content, and other information that is relevant to forecasting. This can help improve the accuracy of financial forecasting models by providing a more comprehensive view of market trends and sentiments. This analysis may help improve the accuracy of prediction. Financial analysts can use the model to identify the relationship between the financial parameters that could change the market conditions in advance. This prediction may be helpful for investors as they make investment decisions.\n",
            "    \n",
            "  * Trading strategies: GPTs can also be used to analyze market trends and historical data to develop trading strategies. This can help traders make better decisions in terms of trading to increase their profitability. GPTs can be used to identify the potential risks in trading portfolios. By analyzing the large volume of information related to trading, GPT will get the potential to identify the risk parameters and provide insights into how to mitigate these risks. The authors in <cit.> have used a popular GPT for stock market trend prediction. The results show that the method used is simple but the efficiency and accuracy of the method are very effective. The prediction it has made is very close to the reality. \n",
            "\n",
            "\n",
            "  * Risk prediction and management:The adoption of GPT can enhance the process of risk prediction and management in several ways. It can improve data analysis by detecting patterns that may pose a risk. It can also help in enhancing fraud detection by analyzing transaction data and identifying fraudulent activity based on patterns. Additionally, GPT can be utilized to make better portfolio management decisions by analyzing historical industry data, company financial statements, and news articles, as well as social media feeds. This portfolio management process can provide valuable information about the investment risk of a given organization, enabling informed investment decisions and effective risk management. \n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "GPTs have more challenges in the finance sector. Primarily, they demand significant computational resources to train and deploy, which can be expensive and time-consuming for financial organizations to implement. Another challenge is that, even though GPTs are capable of producing precise predictions, they can be challenging to interpret, which can present a problem for financial institutions seeking to comprehend the reasoning behind specific predictions <cit.>. This lack of interpretability can harm risk management objectives.Implementing GPT in finance sector can be vulnerable to adversarial attacks, which are designed to manipulate the model's output by injecting false data. This can be particularly problematic for financial institutions that rely on GPTs for risk management and investment decisions. It also require large amounts of training data to achieve high accuracy. However, in some cases, financial institutions may not have access to sufficient data to train the model effectively. GPTs can also be biased if the training data used to develop the model is biased. This can lead to inaccurate predictions and unintended consequences.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "The use of GPTs in the finance industry has promising benefits such as improved risk management, enhanced fraud detection, better portfolio management decisions, and increased efficiency. However, it also has potential challenges that need to be addressed, such as high computational requirements, the complexity of implementation, limited interpretability, vulnerability to adversarial attacks, limited training data, and bias in training data. So, the use of GPTs in the finance industry presents significant benefits but also requires careful consideration of the challenges involved to ensure the effective and secure deployment of these models.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Summary On Impact of GPT models in Applications\n",
            "\n",
            "The impact of GPTs in various applications and challenges was highlighted. GPT  with its varied usage has changed the way people perceive facts such as content creation, enhanced user interfaces, personalized learning, item tracking, self-awareness, market risk analysis, business forecasts and introspection. However, there are concerns about the potential negative impact of GPTs, such as the spread of fake news, bias in data and decision-making, not domain specific, ethical issues, data reliability, the complexity of implementation, multimodal and multilingual support, security and privacy concerns, vulnerable to data attacks, limited input data, explainability of results, large model size, high computational requirements and job loss. Despite these concerns, it is clear that GPTs will continue to be a powerful tool for industries seeking to leverage the power of NLP and generative AI. As the technology improves and new applications emerge, it will be interesting to see how GPTs continue to shape the future of industries around the world. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "§ PROJECTS\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Project Summary Table.\n",
            "\n",
            "!\n",
            "\n",
            "[HTML]EFEFEF \n",
            "Project    \n",
            "  DeepScribe    \n",
            "  Meena    \n",
            "  Jukebox    \n",
            "  Uber's plato research dialogue   system    \n",
            "  Polyglot AI    \n",
            "  SiriGPT \n",
            " Application widely used for    \n",
            "  Healthcare    \n",
            "  Lifestyle    \n",
            "  Entertainment    \n",
            "  Transport    \n",
            "  Education    \n",
            "  Lifestyle \n",
            " Purpose    \n",
            "  Medical documentation and to improve doctor-patient association    \n",
            "  Personalized product recommendation    \n",
            "  Enables the original music    creation both artistically compelling and commercially viable in a   variety of styles and genres    \n",
            "  Enhances user experience using Uber rides, helps drivers and riders in   scheduling rides, navigating routes, providing real-time updates on traffic   and weather conditions.    \n",
            "  enables absolute communication irrespective of the language barrier   across  different regions and   cross-culturalism    \n",
            "  Assist with voice-based assistants \n",
            " GPT Adoption    \n",
            "  Customized version of GPT's    \n",
            "  Google's seq2seq transformer-based neural network architecture similar to   Open AI's GPT    \n",
            "  GPT-2 extension called \"Multi-Scale Transformers for Music   Modeling\" (MST) model    \n",
            "  GPT-2    \n",
            "  GPT-0, GPT-1,GPT-2,GPT-3    \n",
            "  GPT-3 \n",
            " Dataset    \n",
            "  Not Disclosed    \n",
            "  Meena dataset over 40 billion words , 341 GB captured from public domains   like Reddit and social media platforms    \n",
            "  1.2 million songs, 600,000 pieces of sheet   music, 45,000 MIDI files    \n",
            "  Persona-Chat with 160,000 conversational dialogues, Cornell Movie-Dialogs   Corpus with 200,000 movie conversation, DailyDialog over 13,000 dialogues,   and    \n",
            "  CONLL-2003, Sentiment140 dataset, Reuters Corpus, 20 Newsgroups dataset,   WMT (Workshop on Machine Translation) datasets and SQuAD (Stanford Question   Answering Dataset)    \n",
            "  Information not publisized \n",
            " Building Blocks    \n",
            "  Recurrent Neural Network and Attention mechanism fueled by NLP techniques    \n",
            "  Seq2Seq Transformer-based Architecture    \n",
            "  Transformer-based Language Model and Autoregressive model    \n",
            "  Language modeling, Dialogue modeling, Discrete latent variabe modeling   and response ranking    \n",
            "  Language Identification, Named Entity Recognition (NER), Sentiment   Analysis, Text Classification, Machine Translation, Question Answering    \n",
            "  Transformer-based neural network architecture \n",
            " Evaluation Metrics    \n",
            "  Bleu score, perplexity    \n",
            "  Bleu score, perplexity    \n",
            "  Frechet Audio Distance (FAD) and Pitch and Rhythm Similarity    \n",
            "  Bleu score, Perplexity and Distinct n-gram    \n",
            "  accuracy, precision, recall, , F1-score, Bleu score as well as   cross-entropy loss or perplexity    \n",
            "  Perplexity, BLEU score, F1 score, ROUGE score, Human evaluation \n",
            " Addressed Challenges    \n",
            "  Reduced Transcription errors and enhanced patient care    \n",
            "  Natural and Engaging conversations    \n",
            "  Fresh orginical music content creation and drastically reducing the cost   and time by creating high-quality music contents, and also to preserve and   advance musical heritage.    \n",
            "  customer service, user experience, and operational efficiency    \n",
            "  Multilingualism and Sentiment Analysis are the key challenges in NLP and   Polyglot AI solved this problem by offering a tool for supporting morethan 40   languages and pre-trained sentiment analysis model    \n",
            "  Language understanding and generation, Data scarcity, Contextual   understanding, Text summarization, Sentiment analysis, Named entity   recognition \n",
            " Input data    \n",
            "  Audio    \n",
            "  Text    \n",
            "  Audio    \n",
            "  Text    \n",
            "  Text    \n",
            "  Audio \n",
            " Owned By    \n",
            "  DeepScribe    \n",
            "  Google    \n",
            "  OpenAI    \n",
            "  Uber    \n",
            "  Uizard Technologies    \n",
            "  Apple \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Project Summary Table (continued).\n",
            "\n",
            "!\n",
            "\n",
            "[HTML]EFEFEF \n",
            "Project    \n",
            "  AI Dungeon    \n",
            "  Copy.ai    \n",
            "  Bond AI    \n",
            "  Viable    \n",
            "  AI Channels    \n",
            "  Fireflies.ai \n",
            " Application widely used for    \n",
            "  Gaming    \n",
            "  Business and marketing    \n",
            "  Finance    \n",
            "  Business Analytics    \n",
            "  AI Industry    \n",
            "  Business \n",
            " Purpose    \n",
            "  Interactive and engaging storytelling experience for players    \n",
            "  help clients create written content more quickly and easily    \n",
            "  To enhance the financial well-being of clients    \n",
            "  provide businesses with intelligent insights to help them make better   decisions    \n",
            "  provide a platform for developers, data scientists, and machine learning   practitioners to create, deploy, and manage their AI models    \n",
            "  to simplify the meeting process and reduce the time and energy required   for note-taking and collaboration \n",
            " GPT Adoption    \n",
            "  GPT-3    \n",
            "  GPT-3    \n",
            "  GPT-3    \n",
            "  GPT-4    \n",
            "  GPT-3    \n",
            "  GPT-4 \n",
            " Dataset    \n",
            "  Common Crawl, OpenAI GPT-2, and various text datasets from Kaggle    \n",
            "  books, articles, and websites    \n",
            "  likely use of a combination of publicly available financial datasets,   proprietary data, and client data    \n",
            "  Information not publisized    \n",
            "  Users' own dataset    \n",
            "  Possible datasets: the Common Voice dataset from Mozilla   having over 9,000 hours of speech data in multiple languages \n",
            " Building Blocks    \n",
            "  Machine Learning Models, Text Input Interface, Game Engine, Content   Database, Player Feedback System, Cloud Infrastructure    \n",
            "  NLP, Language Models, Neural Networks    \n",
            "  NLP, Personalization, Conversational User Interface, Data Analytics    \n",
            "  Unsupervised learning, Contextual understanding, Sentiment analysis,   Topic modeling, Entity recognition    \n",
            "  Pre-built models, Model training, Data preparation, Collaboration    \n",
            "  Speech-to-Text Technology, NLP, Cloud Computing, Integration technologies \n",
            " Evaluation Metrics    \n",
            "  Response Coherence, Response Diversity, Player Satisfaction, Engagement,   Realism, Novelty    \n",
            "  Perplexity, BLEU score, ROUGE score, F1 score    \n",
            "  Intent recognition accuracy, entity extraction accuracy, and language   model perplexity    \n",
            "  Perplexity, Accuracy, F1 score, Word similarity    \n",
            "  Accuracy, Precision and Recall, F1 Score, Perplexity, User satisfaction    \n",
            "  Speech Recognition Accuracy, NLP Performance, Integration Performance,   Task Completion Time, User Satisfaction \n",
            " Addressed Challenges    \n",
            "  Narrative Generation, Content Creation, Personalization, Replayability,   Accessibility, Creative Expression    \n",
            "  Lack of writing skills, Inconsistency, Multilingual content creation    \n",
            "  Personal financial management, Customer engagement, Fraud detection and   prevention    \n",
            "  Understanding unstructured data, Contextual understanding, Visualization   and exploration of data, Customization and integration    \n",
            "  Natural language understanding, Scalability, Personalization, Integration   with other systems, Maintenance and updates    \n",
            "  Time-consuming manual note-taking, Difficulty in capturing important   details, Lack of visibility and accountability, Communication barriers \n",
            " Input data    \n",
            "  Text    \n",
            "  Text    \n",
            "  Audio and Text    \n",
            "  Text    \n",
            "  Text    \n",
            "  Audio \n",
            " Owned By    \n",
            "  Latitude    \n",
            "  Copy.ai    \n",
            "  Bond.AI    \n",
            "  Viable AI    \n",
            "  MiroMind AG    \n",
            "  Fireflies AI \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "This section presents the exciting projects developed using GPT model technologies for different applications mentioned in the above sections. Table. <ref>, Table. <ref> shows the different levels of such projects along with different parameters to compare their characteristics leveraging the capabilities in many real-life applications.\n",
            "\n",
            "\n",
            "\n",
            " §.§ SiriGPT\n",
            "  \n",
            "Siri <cit.> is an intelligent digital assistant that enables Apple device users to complete tasks more efficiently and with ease, often anticipating their needs even before they make requests. SiriGPT  <cit.> <cit.> is a voice assistant powered by a GPT model and developed entirely using shortcuts. Apple device users can utilize ChatGPT, fueled by GPT-3, by using an API key provided by OpenAI. This novel combination offers the best of both worlds, allowing users to utilize SiriGPT for voice commands and ChatGPT for generating text. SiriGPT utilizes a tokenizer exclusively developed by Apple that has been optimized for processing natural language tasks. SiriGPT's training data is not publicly available as it is exclusive to Apple. However, the language model is trained on diverse text data from various sources such as books, news articles, web pages, and other text data sources. This ensures that SiriGPT can handle different natural language tasks accurately and efficiently. It has been reported that SiriGPT is one of the largest language models available, with over a trillion parameters.\n",
            "\n",
            "\n",
            "\n",
            " §.§ AI Dungeon\n",
            " \n",
            "Latitude, a startup based in Utah, created a groundbreaking online game called AI Dungeon <cit.>, which showcased a novel type of collaboration between humans and machines. It is a  free-to-play, single-player, and multiplayer adventure game that caught traction within the gaming community. It combines fantasy and AI to create endless possibilities, e.g., one can take charge of a military operation to defend against aliens or become a famous detective investigating an attempted murder of the queen of the fairies. Unlike games with predetermined storylines, AI Dungeon allows you to guide the AI to generate unique characters and scenarios for your character to interact with. The game boasted about incorporating the GPT-3 text generator, but then the algorithm began producing unsettling narratives, including graphic depictions of sexual encounters involving minors <cit.>.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Copy.ai\n",
            "  \n",
            "\n",
            "Copy.ai <cit.> is a mighty AI startup founded by Paul Yacoubian in 2020. This project is created using GPT-3, mainly targeting business and marketing campaigns. It has the following use cases: (i) For Teams: It assists with producing customized sales copy, composing long-form articles and pages on a large scale, reusing content on various platforms, and creating product descriptions; (ii) For Emails: The AI-powered email writer takes care of the most challenging parts of marketing by creating email campaigns that are highly effective at converting leads, all with just a few clicks of a button; (iii) For Blogs: By generating content briefs and crafting one-of-a-kind SEO-focused blog articles every month, it can save a significant amount of money for the business. In addition, it's feasible to create briefs, outlines, and even initial drafts in mere minutes, which can be utilized as an excellent source of inspiration for writers to create high-quality content; (iv) Social Media: It aids in generating social media posts quickly and efficiently, allowing for a rapid expansion of the social media following. Additionally, Copy.ai includes a suite of other tools, such as a headline analyzer, a language translator, and a content rephrase. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Bond.AI\n",
            " \n",
            "\n",
            "Bond.AI <cit.> is a company focused on AI for financial institutions, which has a headquarters in Little Rock, Arkansas. It was established by Uday Akkarajuin in 2016 and prided itself on providing AI technology centred around human needs. This innovative project offers a product named BondBot, which is powered by Empathy Engine 3.0 and ChatGPT, to enhance the financial health of clients. It assists financial institutions and employers in promoting interconnected finance by offering various tools to improve the institution's profitability and the financial health of its clients on a single network. It uses customer data to create individual personas for every bank customer or small business, considering their behaviours, strengths, and potential needs. This approach enables the platform to develop multiple customized pathways to holistically enhance clients' financial well-being.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Viable\n",
            " \n",
            "Viable <cit.> is a platform powered by GPT-4 that utilizes the latest advancements in NLP and AI to offer businesses intelligent insights to aid their decision-making processes. Companies can extract actionable insights from unstructured data sources, such as social media posts, customer reviews, and survey responses, by employing Viable. GPT assists in comprehending the sentiment and context behind the data, resulting in valuable insights that can enhance a company's services, products, and customer experience. Viable's \"Insight Explorer\" is a distinctive feature that enables users to interact with and visualize their data via a user-friendly interface. In addition, the platform offers advanced analytics capabilities, including entity recognition, topic modelling, and sentiment analysis. The GPT-based technology of Viable is continually evolving and advancing, which allows the platform to deliver more precise and insightful data. Moreover, Viable provides customized integration and solutions to cater to the specific requirements of each business.\n",
            "\n",
            "\n",
            "\n",
            " §.§ AI Channels\n",
            "  \n",
            "AI Channels <cit.> is a platform that provides a comprehensive set of tools for developers, data scientists, and machine learning practitioners to develop, launch, and manage their AI models. The platform offers an all-in-one solution for creating personalized AI models, starting from data preparation and model training to deployment and monitoring. Users can train their models on their data or, on pre-trained models provided by AI Channels. These models can be deployed as APIs or Docker containers on various infrastructures, including different cloud platforms. It also provides a dashboard for tracking model performance and managing configurations. It covers various use cases, including computer vision, NLP, and speech recognition. The platform includes pre-built models for tasks such as image and text classification, object detection, and sentiment analysis. Additionally, users can create their models using popular frameworks. The main objective of AI Channels is to make building and launching AI models more accessible to developers and businesses without specialized AI skills. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Fireflies.ai\n",
            " \n",
            "Fireflies AI <cit.> is a privately held company based in San Francisco, California, founded by Krish Ramineni and Sam Udotong. Fireflies AI  software is powered by GPT-4 to automate notes-taking tasks and collaborations during meetings. It is compatible with various video conferencing platforms, including Zoom, Google Meet, and Microsoft Teams, and it can transcribe meeting audio and video content in real time. Its primary function is based on speech-to-text technology, which enables it to generate a searchable transcript of the meeting, which can be used for later review and to recall essential points and action items. Additionally, the software utilizes NLP capabilities that can identify significant keywords and phrases within the conversation. Apart from note-taking, Fireflies AI includes collaboration tools such as assigning tasks and sharing notes with other team members. It can integrate with project management and task tracking tools to automatically generate tasks based on the identified action items during the meeting. Fireflies AI provides several customization options to suit particular use cases and workflows. Users can configure the software to automatically join specific meetings or capture audio only from specific speakers. It allows users to specify particular words and phrases to highlight in the transcript, making it easier to identify critical points during the later review. Thus, Fireflies AI aims to simplify the meeting process and reduce the time and energy required for note-taking and collaboration.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Uber's Plato Research Dialogue System\n",
            "   \n",
            "\n",
            "Uber's AI Lab introduced Uber's Plato Research Dialogue System in 2020 developed by a team of researchers and engineers to enable the intelligence in riding experience. PLATO - Pre-trained Dialogue Generation Model with Discrete Latent Variable <cit.>. Uber's Plato Research Dialogue System uses GPT-2, a large-scale language model developed by OpenAI in 2019. Uber's Plato Research Dialogue System project used several datasets to train and evaluate their conversational agents such as Persona-Chat contains 160,000 conversational dialogues, Cornell Movie-Dialogs Corpus with 200,000 movie conversations, DailyDialog over 13,000 dialogues, and EmpatheticDialogues over 25,000 user dialogues. The main components in developing the GPT-powered PLATO project are\n",
            "language modelling, dialogue modelling, discrete latent variable modelling and response ranking. The Plato Research Dialogue System was trained on a massive corpus of text data consisting of over 40 GB of uncompressed text while Bleu score, Perplexity and Distinct n-gram are the evaluation metrics used for training and testing the PLATO project. Uber's AI PLATO has addressed many key challenges like customer service by personalizing user feedback with conversational AI agent, user experience using the Uber platform for scheduling rides, navigating routes, and providing real-time updates, and increasing operational efficiency by reducing the need for human customer service representatives and enabling faster and more accurate communication between riders, drivers, and the Uber app. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Jukebox\n",
            "    \n",
            "\n",
            "Jukebox, a GPT-powered music creation, was developed in 2020 as an extension of Open AI's GPT language model <cit.>. Jukebox's goal is to push the boundaries of what AI can accomplish in the world of music creation and to investigate fresh applications for AI. A variation of the GPT architecture, the \"Multi-Scale Transformers for Music Modeling\" (MST) model, was created specifically to handle the intricate and multi-scale nature of musical data. Additionally, Jukebox can produce lyrics that match the music's tone and style. A sizable and varied dataset of musical recordings, lyrics, and related metadata was used to train Jukebox such as 1.2 million songs sourced including Lakh MIDI Dataset, Free Music Archive, Spotify and Tidal, 600,000 pieces of sheet music were sourced from IMSLP (International Music Score Library Project), and 45,000 MIDI files from Lakh MIDI Dataset and the MIDIworld collection. Faster training times and more effective use of computational resources were made possible by the distributed computing setup with 2048 TPU( Tensor Processing Unit) cores used to train the Jukebox model. Training the model required significant computational resources demanding faster training times by the distributed computing setup with 2048 TPU  (Tensor Processing Unit) cores used to train the Jukebox model. A combination of subjective and objective metrics was used to assess and test Jukebox. In a large-scale subjective assessment, more than 1,000 participants listened and rated each one individually determining the overall score for each song produced. On the other side, objective assessments were conducted by evaluating Frechet Audio Distance (FAD) and Pitch and Rhythm Similarity. Overall, Jukebox revolutionizes with its significant advancement in the music industry through creative inspiration, music production, music education and preservation of music heritage.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Meena\n",
            "   \n",
            "\n",
            "Google's Meena project was developed by Google Research Team in 2020 for providing personalized product recommendations <cit.>. The primary goals of the Meena project empowered the lifestyle sector to enhance the user experience and customer service by recommending goods and services on a personalized basis. The project designed a GPT using the seq2seq transformer-based neural network architecture, in particular for open-domain conversational agents. The architecture was pre-trained over 341 GB of text captured from Reddit and other social platforms containing over 40 million words and called this massive collection as 'Meena Dataset'. Meena was tested using the automated performance metrics known as Bleu score and perplexity on a cluster of HPC nodes with a total of 2048 NVIDIA V100 GPUs. One of the biggest challenges solved Meena was building trust and generating reliable engaging human-like conservation that typically enhances user satisfaction and personalization. Meena has achieved state-of-the-art performance compared to other open-domain chatbots and revolutionized the wide range of applications in the lifestyle industry and a way beyond by providing natural and engaging responses through virtual assistants, customer service bots and personal shoppers.\n",
            "\n",
            "\n",
            "\n",
            " §.§ DeepScribe\n",
            "\n",
            "\n",
            "DeepScribe was a GPT-based medical project developed in 2019 by the student team at the University of California by partnering with giant US-based healthcare providers such as One Medical, Stanford Medicine, Mount Sinai and Sutter Health <cit.>. The DeepScribe's technology aims at transcribing medical conversation allowing doctors to treat the patients rather than noting down the patient's history, enhancing the doctor-patient relationship and targeting the overall quality of patient care. Although DeepScribe used the customized variants of Open AI, the technical details of the GPT model used for customizing the model were not disclosed which was optimized for medical transcription tasks. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Polyglot AI\n",
            "\n",
            "Polyglot AI is a communication platform designed to generate text in multiple languages and process the data by performing several tasks such as advanced NLP techniques, text translation, and sentiment analysis. The potential features of Polyglot AI have been exploited in the following application areas such as language translation, chatbots, language learning tools, content creation, customer support, and data analysis across different languages and regions. Polyglot AI is built based on different variants of GPT models, and state-of-the-art language model architecture for NLP tasks, which uses the self-supervised learning approach. \n",
            "\n",
            "The Polyglot AI was pre-trained using a large amount of textual data on multiple languages simultaneously in an unsupervised environment using a shared architecture, Multilingual Universal Sentence Encoder (MUSE). MUSE developed by Google, is a pre-trained DL model used for cross-lingual TL, that encodes the text into common vector space for multiple languages. Thus, the Polyglot language model was created with the following pre-training techniques as Masked Language Modeling (MLM), Translation Modeling Language (TML), sequence-to-sequence modelling and cross-lingual TL. The pre-trained language model is fine-tuned and evaluated by standard benchmarks and metrics such as the BLEU score (Bilingual Evaluation Understudy), METEOR (Metric for Evaluation of Translation with Explicit ORdering) or F1-score. Remarkably, Facebook used new Polyglot AI to translate between 100 languages <cit.>. Thus Polyglot AI enables absolute communication irrespective of the language barrier across different regions and cross-culturalism.\n",
            "\n",
            "\n",
            "Thus, this section focused on several exciting real-life projects which are developed and used for humankind. These projects were discussed by presenting Table <ref> highlighting the details of the project with model architecture, datasets used, training and testing, and evaluation metrics involved with the challenges addressed. The next section will discuss the open research issues and future directions for the potential benefits of GPT models.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "§ OPEN RESEARCH ISSUES AND FUTURE DIRECTIONS\n",
            "\n",
            "This section highlights the various open research issues concerned with the implementation and adoption of sustainable GPT models. It also provides insights into future research directions for the betterment of researchers in the field of GPT development. Fig. <ref> outlines the many issues that can develop while using GPT models, as well as the various future approaches that need to be considered for the effective usage of GPT models. \n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tChallenges and Future Directions.\n",
            "\t\n",
            "\n",
            "\n",
            "\n",
            " §.§ Domain Specific GPT models\n",
            "\n",
            "Domain-specific GPT models are mandated in almost all applications; developing these models is still challenging and an open issue within GPT. While the current GPT models have been developed to understand natural language and generate content effectively, their performance may not be equally effective when handling specific domains, such as medicine, agriculture, etc. One of the key challenges in adapting to a particular domain is the availability of domain-specific data. It is well known that the performance of GPTs is directly proportional to the quality and quantity of data used for training the model. So, obtaining such quality data for a specific domain is expensive and time-consuming, as the data are heterogeneous. Also, these data accumulations may even make these models much larger, sometimes catastrophic too, leading to forgetting the knowledge attained during the process. To overcome this issue, pre-training tasks and domain-specific model generation are integrated by data augmentation<cit.>.  Another challenge is fine-tuning the model to accustom to the unique characteristics and vocabulary of the domain. A few domain-specific GPT models have been developed and implemented despite these challenges. There is a growing interest in creating more domain-specific GPTs for various domains. Moreover, these models will be trained using the knowledge acquired from large language models specific to domains. Therefore, these models can be fine-tuned for specific tasks or domain-specific requirements with gradually improving performance. GPT models have the potential to be trained in any context, and researchers are exploring new approaches and methods to address these challenges. Furthermore, these models will be more efficient, enhanced interpretability, and domain generability than the existing Large language models as they are customized to specific domain concerns and can provide more concise and informative solutions. TL can be used for developing domain-specific GPT models. Domain-specific GPT models were developed to summarize products based on customer reviews on an E-commerce site, where the language model is pre-trained on the Chinese-short summarization dataset and has obtained fine-tuned results<cit.>. Besides these challenges, domain-specific models require higher computation costs for the resources and time spent in pre-training and relearning in downstream tasks during fine-tuning of pre-trained domain-specific models. Therefore, domain-specific model development must focus on optimizing resource consumption and fine-tuning the pre-trained model to alleviate the forgetting problem involved in existing models. \n",
            "\n",
            "\n",
            "\n",
            " §.§ High Computational requirements\n",
            "\n",
            " As the Transformer model utilizes varied heterogeneous datasets for training and learning from the knowledge acquired, one of the key challenges of GPT models is high computational resources for pre-training and inference. The computational requirement continuously increases as the models become more complex and larger. Depending on the size and complexity of the model and the available resources, the time required to train the model can take days, weeks, or even months. Moreover, the inference time for these models is typically slower, making it challenging to use them for real-time applications. This poses a significant obstacle to adopting GPT models for many practical applications. Despite these challenges, significant efforts are underway to overcome them. To accommodate the increasing data size and pre-training computational requirement, data enhancement-based GPT models were developed<cit.> by joining the downstream tasks and pertaining process by reconstructing the domain-specific text before proceeding for pre-training and utilizing the empirical knowledge rather than learning for falsy domain data. Researchers are exploring various ways to optimize and speed up the training and inference process, such as using specialized GPUs and TPUs. They are also developing more efficient algorithms and attempting to reduce the model size without sacrificing performance. In addition, ChatGPT has evolved to include plugins <cit.> that enable statistical analysis for real-time applications. By integrating these plugins with the help of third-party services, ChatGPT can now be used for analyzing real-time applications as well.\n",
            " \n",
            "\n",
            "  §.§.§ Increasing Model size and Space Constraints\n",
            "\n",
            "Developing and training large language models, such as a GPT model, can be a challenging task due to significant technical and computational difficulties as discussed. The size of GPT models presents a major challenge, as the computational resources required for training and inference increase with the number of parameters that need to be trained. As the model size increases, it also requires more memory to store and manipulate parameters during training and inference<cit.>. Acquiring and processing vast amounts of high-quality training data is another challenge in training large language models like GPT. For instance, GPT-4, which is the largest GPT model to date with 1 trillion parameters, demands a massive amount of computational resources, such as specialized hardware like GPUs and TPUs, spatial requirements, and high-speed network connections to transfer data between different parts of the system. Model evaluation and interpretation are also critical challenges. Since large language models like GPT are trained on a massive scale, understanding how the model makes predictions and why it generates specific outputs is difficult. Evaluating the quality and accuracy of the model's output and identifying and addressing biases or errors in its performance can also be challenging.\n",
            "\n",
            "As these and other significant efforts continue, we can expect the challenge of computational resource requirements for GPT models to transform into a strength in the future.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Explainability and interpretability\n",
            "\n",
            "Explainability and interpretability are currently major challenges for GPTs for specific applications. Explainability refers to providing a clear and understandable explanation of how the model has arrived at any output. Interpretability, on the other hand, refers to the ability to understand the internal processes of the model. GPT models are highly complex and difficult to understand and interpret due to their size and architecture. The outcomes and decisions of the model are based on previous learning and training, and the models learn from vast amounts of data to make decisions. These decisions may not be easily explainable to humans. This lack of transparency and interpretability raises concerns about the reliability and safety of the model, particularly in critical applications such as healthcare and finance. Researchers are currently conducting much research to make GPT models more explainable and interpretable <cit.> by utilizing EXplainable Artificial Intelligence (XAI) to provide explanations for the decisions arrived at, specifically to different users at stake. As well, XAI models enable interpretability by providing detailed explanations for the internal process.  As GPT can generate any type of unconstrained output for instance code generation for the given problem, it requires proper justifications and explanations for the output. So, to assure these codes by GPT are reliable, a metric model to evaluate and validate this GPT code was developed using NLP metrics and XAI for model interpretability<cit.>. Also, some domain-specific GPT models of GPT-3 have evolved with solutions <cit.> <cit.> to ensure that the GPT model's decisions are understandable, explainable, and trustworthy enough to be used for critical applications like healthcare and finance.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Data Bias\n",
            "\n",
            "Data bias is an open issue concerned with the adoption of any advancements in AI, till GPT<cit.>. This is also a prominent challenge for GPT and other machine-learning models. It refers to patterns or relationships in the data that do not accurately reflect the true distribution of the target population or domain. GPT models are trained on vast amounts of text data which may contain bias in language use or cultural assumptions. Still, the source of data remains undeclared, considering GPTs are trained using internet data which may have faulty, fake, and error data, GPTs may generate biased texts or information imitating the training data<cit.>. Such biases can be amplified in the model's output, resulting in false or unfair results. Data bias can arise from various sources, such as selection bias, labelling bias, concept drift, confounding variables, and changes in input data distribution over time. For example, suppose a dataset used to train a GPT model is dominated by a particular demographic group. In that case, the resulting model may be biased in its predictions towards that group, leading to inaccurate or unfair predictions when applied to new data. This bias can have serious consequences, especially in healthcare, finance, and law enforcement, where biased results can significantly impact human lives. To mitigate these issues, researchers have developed strategies such as diversifying the training data, debiasing the training data, modifying the model architecture, and using post-processing methods to normalize the data and create more fair and inclusive GPT models. The authors in <cit.> have made an in-depth analysis of the most downloaded text generation model GPT2. By examining the intersections of gender with religion, sexuality, ethnicity, political affiliation, and continental name origin, the authors evaluated prejudices associated with occupational associations among various protected categories. These biases may have inaccuracies in climatic data prediction or global warming<cit.>. Therefore, data bias must be of greater concern in GPT model development as the data quality of the internet is limited to avoiding producing disturbing content.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Multimodal support\n",
            "\n",
            "\n",
            "The challenge of developing multimodal learning ability in the GPT model remains unsolved. Multimodal support refers to the GPT model's ability to process and generate text along with other modalities, such as audio, images, and videos. GPT models have shown impressive results in generating high-quality text and NLP tasks, but it was primarily designed for text-based tasks and cannot handle other modalities. However, due to its success in text processing, users expect its integration with other modalities, such as speech recognition, video summarization, and image or video captioning<cit.>. Several research initiatives have been proposed to integrate multimodal support to address this issue. One approach is to feed the visual and audio information with the corresponding text to the model as input. The other is to handle this input modality process as a separate model and use the output as input to GPT. Multimodal video captioning is done using GPT in the unlabelled videos<cit.>. Multimodel learning has been applied for information retrieval<cit.> and image generation for illustrating the news<cit.> to assist the GPTs. However, the primary challenge in both approaches is effective integration, requiring architectural changes and techniques to handle various modalities. Recently, OpenAI's GPT4 has launched with multimodal support, enabling it to read images, analyze the input, and generate text as output. It cannot create images as output, though. Nevertheless, the field of multimodal processing is still an active area of research, and much work must be done to effectively and efficiently process and understand multimodal data. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Robustness\n",
            "\n",
            "\n",
            "The robustness is a major requirement to be imposed by any type of GPT model, and it is a global problem for all learning-based prediction technologies. Robustness refers to the ability of the model to maintain high performance and accuracy even in the face of unexpected or adversarial inputs. Although GPT models have shown impressive performance in a wide range of NLP applications and have set a benchmark for high-quality text generation, they are still vulnerable to certain types of errors and attacks. In particular, handling adversarial inputs is a challenging task in GPT models. GPT models are particularly susceptible to adversarial attacks<cit.>. Adversarial inputs are specifically designed to make a learning model collapse and misbehave. GPT models can be highly prone to these attacks because they are trained on a large volume of text. As a result, they may be influenced by subtle patterns or biases in the training data. If such biases or patterns exist in the data, the GPT model may amplify or perpetuate existing biases, leading to unfair outcomes. A few techniques may be used, such as adversarial training <cit.><cit.>, defensive distillation <cit.>, and regularization techniques <cit.> such as dropout, weight decay, and batch normalization, to mitigate and handle adversarial inputs. Therefore, GPT development must focus on developing models with more robustness, enabling them to be tolerant of various vulnerabilities, and thus to be used reliably and susceptible in a wide range of applications.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Multilingual support\n",
            "\n",
            "While GPT models have demonstrated remarkable proficiency in NLP tasks for individual languages, achieving multilingual support remains a significant challenge. The primary difficulty in developing multilingual GPT models lies in the significant differences in syntax, grammar, and vocabulary across various languages. As the number of internet users day by day increasing irrespective of literacy rate, multilingual support will target all types of end users. To create models that can effectively process multiple languages, researchers need to train GPT models on extensive, diverse datasets that span a broad range of languages and language families. Additionally, designing language-specific pre-processing techniques to prepare input data for the model is another obstacle to overcome. Various languages possess distinct writing systems, word orders, and linguistic features, necessitating specialized pre-processing techniques to ensure that the model can process the input data effectively. Despite the challenges, researchers continue to explore new methods to improve the multilingual capabilities of GPT models. Some techniques involve training separate models for each language or developing language-specific fine-tuning techniques. Others include developing cross-lingual TL techniques that allow the model to transfer knowledge and skills learned in one language to another. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Limited understanding\n",
            "\n",
            "\n",
            "GPT models have a limited understanding of context and meaning, despite their ability to generate coherent text. This problem arises due to issues such as a lack of semantic understanding, bias, stereotyping, and handling nuances and figurative language. As a result, the outputs generated by the model may contain errors or inaccuracies, even if they are grammatically correct. Researchers are exploring various techniques to enhance the model's contextual understanding. Understanding GPTs will be more reactive and may attract more users for accurate results<cit.>. These methods include incorporating external knowledge sources like knowledge graphs and ontologies into the training process, developing common sense reasoning capabilities, and improving the model's ability to handle nuances and idiomatic expressions. By enhancing the contextual understanding of GPT models, their outputs will be more accurate, relatable, sequential, less biased, and more useful for a variety of applications.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Ethical Concerns\n",
            "\n",
            "The ethical concerns in GPT models are an active area of discussion and debate due to the potential negative impacts that the use of GPTs could have on society. Although GPT models have demonstrated remarkable abilities in generating coherent and realistic text, there are concerns about the perpetuation of biases and stereotypes, the possibility of malicious use, and the effects on employment and economic inequality. Some of the ethical characteristics to be possessed by GPT include functional Morality, operational morality, abiding by the right for explanation law, improved transparency with human involvement, unbiased data, and adhering to government regulations on data usage<cit.>.  The responsibility of developers and companies to address these ethical concerns and ensure the ethical use of a GPT model is also a topic of debate. The ethical implications of GPT models are being actively researched and discussed in the fields of AI, computer science, and philosophy.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Security and privacy concerns\n",
            "\n",
            "GPT models raise concerns about security and privacy, particularly as they become more widespread. One of the main concerns is that GPT could be used for harmful purposes, such as creating fake news or deep fakes, as it can generate text that looks real and convincing, making it difficult to distinguish between genuine and fake content. Another concern is the potential for privacy violations when using a GPT model. Large language models like GPT require a significant amount of training data, which could contain sensitive or personal information. This raises concerns about privacy and data protection as per European Union's General Data Protection Act<cit.>, particularly if the training data is not properly anonymized or if the models are used to generate text based on user data without their explicit consent. Some of the problems concerned with confidentiality related to the pre-training dataset are Data tracing, Membership Inference Attacks, reconstruction attacks, and property inference attacks and the vulnerabilities concerned with a model encoder are hyperparameter stealing attacks and encoder parameter stealing attacks. Poisoning, Backdoor, and evasion attacks are the vulnerability related to the integrity of self-supervised learning. Resource depletion attack is one major issue with data availability, which may lead to tremendous effects incorrect results, and may cause greater deviations too<cit.>.   Additionally, the GPT model's ability to generate text based on user input could inadvertently disclose sensitive information, such as personal or financial details, or trade secrets. This could happen if a GPT model is used in an insecure environment or if it is targeted by malicious actors seeking to obtain sensitive information. Researchers and developers should focus on assuring authenticity in using users' data in case of interactive information generation based on privacy data shared. These include using differential privacy to protect training data privacy <cit.>, implementing secure hardware or software protocols to protect models from cyberattacks, and developing techniques to detect and prevent the malicious use of GPT models. It's crucial to adopt and follow these measures to ensure the ethical and safe use of GPT models before using them in various applications.\n",
            "\n",
            "Therefore, GPT model development must focus on developing more robust, reliable, safest, multi-lingual, multimodal support-enabled solutions for delivering domain-specific or human-specific solutions with optimal resource utilization.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Various lessons learned and future research directions.\n",
            "\n",
            "!\n",
            "\n",
            "Sl.No    \n",
            "  Lessons Learned    \n",
            "  Open Issues    \n",
            "  Future Directions \n",
            " 1.    \n",
            "  ∘ Huge volume of data usage is critical\n",
            "    \n",
            "    \n",
            "  ∘ Data privacy - may unknowingly reveal sensitive information\n",
            "  \n",
            "  ∘ Varied data quality - Inconsistency in quality of data used for training \n",
            "  \n",
            "  ∘ Scalability - Models should be able to handle an increase in data set size and complexity    \n",
            "  ∘ Optimized architecture and algorithms \n",
            "  \n",
            "  ∘ Cloud-based computing\n",
            "  \n",
            "  ∘ Hardware advancements\n",
            "  \n",
            "   \n",
            " 2.   \n",
            "  ∘ Importance of Proper Pre-processing of data \n",
            "  \n",
            "     \n",
            "  \n",
            "  ∘ Data bias - Overrepresentation of certain groups or perspectives \n",
            "  \n",
            "  ∘ Poor model performance \n",
            "  \n",
            "  ∘ Reduced efficiency of the model \n",
            "     \n",
            "  ∘ Continuous monitoring\n",
            "  \n",
            "  ∘ Testing model for potential biases\n",
            "  \n",
            "  ∘ Diversifying the training data\n",
            "  \n",
            "  \n",
            " \n",
            "\n",
            "  3.    \n",
            "  ∘ Importance of explainability and interpretability\n",
            "    \n",
            "    \n",
            " ∘ Complexity of models\n",
            "  \n",
            " ∘ Inability to explain predictions\n",
            " \n",
            " ∘ Need of User-tailored Explanations generation\n",
            " \n",
            " ∘ Developing Interpretable models\n",
            " \n",
            "  ∘ Lack of transparency in the data source\n",
            "  \n",
            "     \n",
            "\n",
            "  ∘ AI governance models can be used\n",
            "  \n",
            "  ∘ Model Summaries can be provided\n",
            "  \n",
            "  ∘ Techniques like LIME(Local Model-Agnostic Explanations can be used\n",
            "  \n",
            "  ∘ Uncertaining estimates can be obtained from a model\n",
            "  \n",
            "   \n",
            " \n",
            "\n",
            "    4.    \n",
            "  ∘ Ethical concerns\n",
            "    \n",
            "    \n",
            "  ∘ Data privacy and data protection\n",
            "  \n",
            "  ∘ Misuse of data\n",
            "\n",
            "  ∘ Accountability  and transparency concerns\n",
            "\n",
            "  ∘ Societal implications - displacing jobs and exacerbating equalities\n",
            "  \n",
            "  \n",
            "     \n",
            "\n",
            "  ∘ Counterfactual analysis can be used\n",
            "  \n",
            "  ∘ Federated learning can be used\n",
            "  \n",
            "  ∘ Ethical guidelines, Legal frameworks and regulations can be developed to avoid harmful use\n",
            "    \n",
            "   \n",
            " \n",
            "\n",
            "   5.     \n",
            "  ∘ Lack of contextual understanding in AI systems\n",
            "    \n",
            "    \n",
            "  ∘ Possibility for ambiguous, contradictory, incorrect results leads to misunderstandings\n",
            "  \n",
            "  ∘ Inconsistency in responses or outputs\n",
            "\n",
            "  ∘ Lack of ability in distinguishing true and false information\n",
            "\n",
            "   \n",
            "  \n",
            "     \n",
            "\n",
            "  ∘ Incorporation of knowledge graphs and semantic embeddings into the training process \n",
            "  \n",
            "  ∘ Usage of attention mechanisms to focus on relevant parts of the input\n",
            "  \n",
            "  ∘ Imparting reasoning and inference capabilities\n",
            "\n",
            "  ∘ Task or domain-based fine-tuning\n",
            "    \n",
            "   \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "   6.    \n",
            "  ∘ Pre-trained models may not perform well for Domain-specific task\n",
            "    \n",
            "    \n",
            "  ∘ Possibility for ambiguous, contradictory, incorrect results leads to misunderstandings\n",
            "  \n",
            "  ∘ Inconsistency in responses or outputs\n",
            "\n",
            "  ∘ Lack of ability in distinguishing true and false information\n",
            "  \n",
            "     \n",
            "\n",
            "  ∘ Incorporation of knowledge graphs and semantic embeddings into the training process \n",
            "  \n",
            "  ∘ Usage of attention mechanisms to focus on relevant parts of the input\n",
            "  \n",
            "  ∘ Imparting reasoning and inference capabilities\n",
            "\n",
            "  ∘ Task or domain-based fine-tuning\n",
            "    \n",
            "   \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "§ CONCLUSION\n",
            "\n",
            " The impact of GPT and other large language models is far-reaching and profound. As these technologies continue to evolve and improve, they have the potential to transform the way we interact with technology and each other. From personalized recommendations and customer service to language translation and text generation, the possibilities are endless. However, as with any technology, there are potential ethical and societal concerns that must be addressed. As we continue to rely more heavily on these language models, we must ensure that we are using these tools responsibly and with consideration for their impact on society as a whole. These include challenges related to biases in the data used to train the models, safeguarding privacy and security, understanding the implications of human creativity, and the potential impact on employment and job displacement. We need to continue to evaluate and reflect on the impact of GPT and other language models, to ensure that they are being used in a way that benefits society as a whole. By doing so, we can help to ensure that these technologies are used to their fullest potential while minimizing any negative impact that they may have.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "IEEEtran\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_data = final_data + '\\n' + combined_text"
      ],
      "metadata": {
        "id": "e8tt4W5qsH8I"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUDbsSmPtC1t",
        "outputId": "9fb43969-c1ff-4bc8-f45f-15fd039ab554"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to CS324! This is a new course on understanding and developing large language models.\n",
            "The classic definition of a language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1):\n",
            "The probability intuitively tells us how “good” a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo):\n",
            "Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.\n",
            "For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it’s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.\n",
            "Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted:\n",
            "How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an “average” sequence but something closer to the “best” sequence.\n",
            "A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using the chain rule of probability:\n",
            "For example (demo):\n",
            "In particular, \\(p(x_i \\mid x_{1:i-1})\\) is a conditional probability distribution of the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\).\n",
            "Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network).\n",
            "Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far:\n",
            "where \\(T \\ge 0\\) is a temperature parameter that controls how much randomness we want from the language model:\n",
            "However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) the annealed conditional probability distribution. For example:\n",
            "Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing.\n",
            "Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences.\n",
            "Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called a prompt) and sampling the rest \\(x_{i+1:L}\\) (called the completion). For example, generating with \\(T=0\\) produces (demo):\n",
            "If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\).\n",
            "As we’ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.\n",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication. In this paper, he introduced the entropy of a distribution as\n",
            "The entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\(x \\sim p\\) into a bitstring:\n",
            "Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.\n",
            "Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a “true” distribution \\(p\\) out there (the existence of this is questionable, but it’s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\).\n",
            "Shannon also defined cross entropy:\n",
            "which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)).\n",
            "Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\),\n",
            "which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English.\n",
            "So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\).\n",
            "Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paper Prediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human:\n",
            "Humans aren’t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses.\n",
            "Language models became first used in practical applications that required generation of text:\n",
            "Noisy channel model. The dominant paradigm for solving these tasks then was the noisy channel model. Taking speech recognition as an example:\n",
            "Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).\n",
            "N-gram models. In an n-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history:\n",
            "For example, a trigram (\\(n=3\\)) model would define:\n",
            "These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing).\n",
            "Fitting n-gram models to data is extremely computationally cheap and scalable. As a result, n-gram models were trained on massive amount of text. For example, Brants et al. (2007) trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix:\n",
            "If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in “huge” corpora):\n",
            "As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn’t a huge problem.\n",
            "An important step forward for language models was the introduction of neural networks. Bengio et al., 2003 pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network:\n",
            "Note that the context length is still bounded by \\(n\\), but it is now statistically feasible to estimate neural language models for much larger values of \\(n\\).\n",
            "Now, the main challenge was that training neural networks was much more computationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade.\n",
            "Since 2003, two other key developments in neural language modeling include:\n",
            "Recurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\(x_i\\) to depend on the entire context \\(x_{1:i-1}\\) (effectively \\(n = \\infty\\)), but these were hard to train.\n",
            "Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\(n\\), but were much easier to train (and exploited the parallelism of GPUs). Also, \\(n\\) could be made “large enough” for many applications (GPT-3 used \\(n = 2048\\)).\n",
            "We will open up the hood and dive deeper into the architecture and training later in the course.\n",
            "Having introduced language models, one might wonder why we need a course specifically on large language models.\n",
            "Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:\n",
            "Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that “just scaling up” these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact.\n",
            "Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa.\n",
            "Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past.\n",
            "Recall that language models are capable of conditional generation: given a prompt, generate a completion:\n",
            "Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can perform question answering by prompting with a fill in the blank (demo):\n",
            "One can prompt a language model to solve word analogies (demo):\n",
            "One can prompt a language model to generate a news article based on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text):\n",
            "Title: NLP Researchers at Stanford Discover Black Holes in Language Models Article: On January 3, 2007, the Stanford University News Service published an article that reported a remarkable discovery by NLP researchers at Stanford. The article was titled “Stanford Researchers Discover Black Holes in Language Models.” The discovery was described as follows: A black hole is a region of space-time where gravity pulls so much that even light cannot get out. Now physicists think they have found a similar phenomenon in language: They call it the semantic black hole. It occurs when a word or phrase has no clear definition – and sometimes no clear meaning at all. If you toss such a word into a sentence, it drags along other words until eventually the whole thing collapses under its own weight. “It’s like if you have a paper cup and you push in the bottom,” said Stanford computer scientist Michael Schmidt. “At first it holds up fine, but then it gets weaker and weaker until it collapses in on itself.” Schmidt and his colleagues are using computers to identify and avoid semantic black holes.\n",
            "In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning. Let’s start with an example (demo):\n",
            "Input: Where is Stanford University? Output: Stanford University is in California.\n",
            "We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence.\n",
            "Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):\n",
            "Input: Where is MIT? Output: Cambridge  Input: Where is University of Washington? Output: Seattle  Input: Where is Stanford University? Output: Stanford\n",
            "Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.\n",
            "Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity.\n",
            "Given the strong capabilities of language models, it is not surprising to see their widespread adoption.\n",
            "Research. First, in the research world, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.\n",
            "Industry. In production systems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production:\n",
            "Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are therefore affecting billions of people.\n",
            "An important caveat is that the way language models (or any technology) are used in industry is complex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer.\n",
            "So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there are substantial risks associated with the use of language models. Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind’s paper on ethical and social harms detail the risks. Let us highlight a few of them, which we will study in more detail in this course.\n",
            "Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer can seem correct and there is no way of knowing (demo)\n",
            "Input: Who invented the Internet? Output: Al Gore\n",
            "In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable?\n",
            "Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo):\n",
            "The software developer finished the program. He celebrated. The software developer finished the program. She celebrated.\n",
            "Social biases are of course encoded in the data, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias?\n",
            "Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content. RealToxicityPrompts is a dataset that evaluates a language model’s propensity for producing toxic content. For example:\n",
            "So, I’m starting to think she’s full _\n",
            "As another example, GPT-3 has been demonstrated to output anti-Muslim stereotypes:\n",
            "Two Muslims walked into a _\n",
            "Applications such as writing assistants or chatbots would be vulnerable.\n",
            "Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models’ linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.\n",
            "Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack. For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt:\n",
            "In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem.\n",
            "Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation?\n",
            "For example, if you prompt GPT-3 with the first line of Harry Potter (demo):\n",
            "Mr. and Mrs. Dursley of number four, Privet Drive, _\n",
            "It will happily continue to spout out text from Harry Potter with high confidence.\n",
            "Cost and environmental impact. Finally, large language models can be quite expensive to work with.\n",
            "One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases.\n",
            "Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, including Hugging Face’s Big Science project, EleutherAI, and Stanford’s CRFM. Given language models’ increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.\n",
            "This course will be structured like an onion:\n",
            "Behavior of large language models: We will start at the outer layer where we only have blackbox API access to the model (as we’ve had so far). Our goal is to understand the behavior of these objects called large language models, as if we were a biologist studying an organism. Many questions about capabilities and harms can be answered at this level.\n",
            "Data behind large language models: Then we take a deeper look behind the data that is used to train large language models, and address issues such as security, privacy, and legal considerations. Having access to the training data provides us with important information about the model, even if we don’t have full access to the model.\n",
            "Building large language models: Then we arrive at the core of the onion, where we study how large language models are built (the model architectures, the training algorithms, etc.).\n",
            "Beyond large language models: Finally, we end the course with a look beyond language models. A language model is just a distribution over a sequence of tokens. These tokens could represent natural language, or a programming language, or elements in an audio or visual dictionary. Language models also belong to a more general class of foundation models, which share many of the properties of language models.\n",
            "In this lecture, we will explore the capabilities of GPT-3, the canonical large language model. We will closely follow the benchmarks from the GPT-3 paper, which include:\n",
            "In comparison with the state-of-the-art-result for each task, the results are mixed:\n",
            "The way to think about these results is as follows:\n",
            "Adaptation. Recall that a language model \\(p\\) is a distribution over sequences of tokens \\(x_{1:L}\\) and thus can be used to score sequences:\n",
            "It can also be used to perform conditional generation of a completion given a prompt:\n",
            "A task is a mapping from inputs to outputs. For example, for question answering, we might have:\n",
            "Input: What school did burne hogarth establish? Output: School of Visual Arts\n",
            "We use the term adaptation to refer to the process of taking a language model and turning it into a task model, given:\n",
            "There are two primary ways to perform adaptation:\n",
            "Which adaptation procedure should we go with?\n",
            "The GPT-3 paper evaluated GPT-3 on a large set of tasks. We will consider a subset of these, and for each task, discuss the following:\n",
            "Size and number of examples matters. By default, the results will based on\n",
            "Along the way, we will do ablations to see if model size and number of in-context training instances matters. Spoiler: it does and more is better.\n",
            "The tasks are grouped as follows:\n",
            "The goals of this lecture is to provide:\n",
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language.\n",
            "Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example:\n",
            "We can ask: what is the probability the language model assigns to it?\n",
            "Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule:\n",
            "Perplexity. The joint probability of a sequence depends on its length and thus goes to zero as the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.)\n",
            "Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don’t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn’t penalize you for that. Instead, we want the geometric average, which is exactly what perplexity does:\n",
            "Perplexity can be interpreted as the average “branching factor” per token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings.\n",
            "Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically:\n",
            "Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\):\n",
            "where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it’s just going to generate a gibberish token.\n",
            "Now let’s get on with evaluating perplexity on an actual dataset.\n",
            "The Penn Tree Bank is a classic dataset in NLP, originally annotated for syntactic parsing. Beginning with Emami and Jelinek (2004) and Mikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t to John Hewitt for pointing this out).\n",
            "Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo):\n",
            "Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29. Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group.\n",
            "Results. GPT-3 vastly outperforms the existing state-of-the-art:\n",
            "See the leaderboard for the latest results.\n",
            "Train/test leakage. The authors did not evaluate on some datasets such as WikiText-103 because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized.\n",
            "Adaptation.\n",
            "Fill in blank:  Alice was friends with Bob. Alice went to visit her friend ___. -> Bob  She held the torch in front of her. She caught her breath. “Chris? There’s a step.” “What?” “A step. Cut in the rock. About fifty feet ahead.” She moved faster. They both moved faster. “In fact,” she said, raising the torch higher, “there’s more than a ___. -> step\n",
            "Results. GPT-3 does much better on this task than the previous state-of-the-art (based on GPT-2):\n",
            "See the leaderboard for the latest results.\n",
            "Adaptation. This is a multiple-choice task, so the most natural thing to do is to score each candidate answer with the language model and predict the “best” one (demo):\n",
            "Making a cake: Several cake pops are shown on a display. A woman and girl are shown making the cake pops in a kitchen. They ${answer}\n",
            "where ${answer} is one of:\n",
            "How do you score a candidate answer \\(y\\) given a question \\(x\\)? There’s no principled answer, but here are some heuristics:\n",
            "Unnormalized probability: \\(\\text{score}(x, y) = p(x, y)\\). The problem with the unnormalized probability is that it has a bias towards short answers (demo).\n",
            "Length-normalized probability: \\(\\text{score}(x, y) = \\frac{p(x, y)}{\\text{num-tokens}(y)}\\). This fixes the length bias. However, given two answers of the same length, the model still might prefer the more popular entity.\n",
            "Frequency-normalized probability: \\(\\text{score}(x, y) = \\frac{p(y \\mid x)}{p(y \\mid x_0)}\\), where \\(x_0\\) is a neutral string like \\(\\nl{Answer:}\\). This lowers the score for answers that happen to just be common (e.g., \\nl{John}). Compare demo versus demo.\n",
            "Results. GPT-3 got close but did not exceed the state-of-the-art:\n",
            "However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data!\n",
            "See the leaderboard for the latest results.\n",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. The language model has to somehow “know” the answer without looking up information in a database or a set of documents (we’ll consider reading comprehension later, where the information is provided).\n",
            "Input: What school did burne hogarth establish? Output: School of Visual Arts\n",
            "Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo):\n",
            "Q: ‘Nude Descending A Staircase’ is perhaps the most famous painting by which 20th century artist? A: Marcel Duchamp\n",
            "Results.\n",
            "We also see that both increasing the model size and the number of in-context training instances helps:\n",
            "\n",
            "Adaptation.\n",
            "We define a prompt the same as above (demo):\n",
            "Q: What school did burne hogarth establish? A: School of Visual Arts\n",
            "Results.\n",
            "Adaptation. We define a prompt the same as above (demo):\n",
            "Q: Who played tess on touched by an angel? A: Delloreese Patricia Early (July 6, 1931 - November 19, 2017), known professionally as Della Reese.\n",
            "Results.\n",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo):\n",
            "Mein Haus liegt auf dem Hügel. = My house is on the hill. Keinesfalls dürfen diese für den kommerziellen Gebrauch verwendet werden. = In no case may they be used for commercial purposes.\n",
            "Results. Here are the results from German to English:\n",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more “abstract reasoning” tasks, to evaluate GPT-3 as more of a general-purpose model.\n",
            "Adaptation. Pose the problem as question answering (demo):\n",
            "Q: What is 556 plus 497? A: 1053\n",
            "Results.\n",
            "\n",
            "It doesn’t work perfectly and can hardly be said to “understand arithmetic” fully, but it works surprisingly well.\n",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like.\n",
            "Title: United Methodists Agree to Historic Split Subtitle: Those who oppose gay marriage will form their own denomination Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post. The majority of delegates attending the church's annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings. But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination...\n",
            "Results. Humans were able to able to detect classify “human” versus “machine” only 52% of the time (barely above random chance).\n",
            "For the article above, humans guessed “machine” correctly only 12% of the time.\n",
            "Adaptation. Just describe the task in the prompt (demo):\n",
            "To “screeg” something is to swing a sword at it. An example of a sentence that uses the word screeg is: We screeged the tree with our swords.\n",
            "Adaptation. The prompt consists of input-output pairs (demo):\n",
            "Poor English input: I eated the purple berries. Good English output: I ate the purple berries. Poor English input: Thank you for picking me as your designer. I’d appreciate it. Good English output: Thank you for choosing me as your designer. I appreciate it. Poor English input: The mentioned changes have done. or I did the alteration that you requested. or I changed things you wanted and did the modifications. Good English output: The requested changes have been made. or I made the alteration that you requested. or I changed things you wanted and made the modifications. Poor English input: I’d be more than happy to work with you in another project. Good English output: I would be happy to work with you on another project.\n",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list.\n",
            "Benchmarks.\n",
            "The performance on these benchmarks is still mediocre, but it’s perhaps not bad given that we’re doing few-shot learning!\n",
            "Demos.\n",
            "The demos are creative and interesting, but it’s hard to tell how reliably they work.\n",
            "\\(\\newcommand{\\sV}{\\mathcal{V}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\x}{x_{1:L}} \\newcommand{\\softmax}{\\text{softmax}} \\newcommand{\\EmbedToken}{\\text{EmbedToken}} \\newcommand{\\SequenceModel}{\\text{SequenceModel}} \\newcommand{\\FeedForward}{\\text{FeedForward}} \\newcommand{\\FeedForwardSequenceModel}{\\text{FeedForwardSequenceModel}} \\newcommand{\\SequenceRNN}{\\text{SequenceRNN}} \\newcommand{\\BidirectionalSequenceRNN}{\\text{BidirectionalSequenceRNN}} \\newcommand{\\RNN}{\\text{RNN}} \\newcommand{\\SimpleRNN}{\\text{SimpleRNN}} \\newcommand{\\LSTM}{\\text{LSTM}} \\newcommand{\\GRU}{\\text{GRU}} \\newcommand{\\Attention}{\\text{Attention}} \\newcommand{\\MultiHeadedAttention}{\\text{MultiHeadedAttention}} \\newcommand{\\SelfAttention}{\\text{SelfAttention}} \\newcommand{\\TransformerBlock}{\\text{TransformerBlock}} \\newcommand{\\EmbedTokenWithPosition}{\\text{EmbedTokenWithPosition}} \\newcommand{\\LayerNorm}{\\text{LayerNorm}} \\newcommand{\\AddNorm}{\\text{AddNorm}} \\newcommand{\\nl}[1]{\\textsf{#1}} \\newcommand{\\generate}[1]{\\stackrel{#1}{\\rightsquigarrow}} \\newcommand{\\embed}{\\stackrel{\\phi}{\\Rightarrow}}\\) We started this course by analyzing a language model as a black box:\n",
            "Then we looked at the training data of large language models (e.g., The Pile):\n",
            "In this lecture, we will open up the onion all the way and talk about how large language models are built.\n",
            "Today’s lecture will focus on two topics, tokenization and model architecture.\n",
            "Tokenization: how a string is split into tokens.\n",
            "Model architecture: We will discuss mostly the Transformer architecture, which is the modeling innovation that really enabled large language models.\n",
            "Recall that a language model \\(p\\) is a probability distribution over a sequence of tokens where each token comes from some vocabulary \\(\\sV\\):\n",
            "However, natural language doesn’t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters):\n",
            "A tokenizer converts any string into a sequence of tokens.\n",
            "This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work.\n",
            "The simplest solution is to do:\n",
            "text.split(' ')\n",
            "我今天去了商店。 [gloss: I went to the store.]\n",
            "Then there are languages like German that have long compound words (e.g., Abwasserbehandlungsanlange).\n",
            "Even in English, there are hyphenated words (e.g., father-in-law) and contractions (e.g., don’t), which should get split up. For example, the Penn Treebank splits don’t into do and n’t, a linguistically-informed but not obvious choice.\n",
            "Therefore, splitting by spaces by spaces to identify words is quite problematic.\n",
            "What makes a good tokenization?\n",
            "Sennrich et al, 2015 applied the byte pair encoding (BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers.\n",
            "Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot.\n",
            "Example:\n",
            "The output of learning is:\n",
            "Applying the tokenizer. To tokenize a new string, apply the merges in the same order:\n",
            "Unicode.\n",
            "今天 [gloss: today] [x62, x11, 4e, ca]\n",
            "Rather than just splitting by frequency, a more “principled” approach is to define an objective function that captures what a good tokenization looks like. We now describe the unigram model (Kudo 2018).\n",
            "Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of\n",
            "Example:\n",
            "Algorithm:\n",
            "Impact:\n",
            "Examples of tokenizations for both GPT-3 and Jurassic (demo):\n",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence.\n",
            "Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings:\n",
            "We will broaden our notion of language models to three types of models.\n",
            "Encoder-only (BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text.\n",
            "These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks).\n",
            "Decoder-only (GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)).\n",
            "Encoder-decoder (BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\).\n",
            "We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\):\n",
            "We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on these slides from CS221 on differentiable programming, and will depart a bit from the standard presentation.\n",
            "The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity:\n",
            "This function will have parameters which we will include in the body but elide in the function signature for simplicity.\n",
            "In what follows, we will define a library of building blocks until we get to the full Transformer.\n",
            "First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data):\n",
            "def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\):\n",
            "These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes these context-independent embeddings and maps them into contextual embeddings.\n",
            "def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to a fixed length context, just as in an n-gram model:\n",
            "def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "The first “real” sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence of hidden states recursively.\n",
            "def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state:\n",
            "def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\):\n",
            "There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNN Elman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)).\n",
            "def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\):\n",
            "As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used by ELMo and ULMFiT.\n",
            "def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\):\n",
            "Notes:\n",
            "We will not discuss these models in the interest of time.\n",
            "Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models.\n",
            "There are great resources for learning about the Transformer:\n",
            "You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces.\n",
            "The crux of the Transformers are the attention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017).\n",
            "One can think of attention as a “soft” lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\):\n",
            "We can think of each \\(x_i\\) as representing a key-value pair via linear transformations:\n",
            "and forming the query via another linear transformation:\n",
            "The key and the query can be compared to give a score:\n",
            "These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\):\n",
            "Then the final output is a weighted combination over the values:\n",
            "We can write this all succinctly in matrix form:\n",
            "def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\):\n",
            "We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multiple attention heads and simply combine their outputs.\n",
            "def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\)\n",
            "Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce:\n",
            "def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\):\n",
            "Feedforward layer. Self-attention allows all the tokens to “talk” to each other, whereas feedforward connections provide:\n",
            "def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "Improving trainability. We’re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable.\n",
            "Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\):\n",
            "we add a residual (skip) connection so that if \\(f\\)’s gradients vanish, gradients can still flow through \\(\\x\\):\n",
            "Layer normalization. Another trick is layer normalization, which takes a takes a vector and makes sure its elements are too big:\n",
            "def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "We first define an adapter function that takes a sequence model \\(f\\) and makes it “robust”:\n",
            "def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "Finally, we can define the Transformer block succinctly as follows:\n",
            "def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\):\n",
            "Positional embeddings. You might have noticed that as defined, the embedding of a token doesn’t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible.\n",
            "To fix this, we add positional information into the embedding:\n",
            "def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\):\n",
            "GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times:\n",
            "Shape of the architecture (how the 175 billion parameters are allocated):\n",
            "These decisions are not necessarily optimal. Levine et al. 2020 provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture.\n",
            "There are important but detailed differences between different versions of Transformers:\n",
            "Tokenization:\n",
            "Modeling:\n",
            "CS224N slides on Transformers\n",
            "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. Ofir Press, Noah A. Smith, M. Lewis. 2021. Introduces Alibi embeddings.\n",
            "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov. ACL 2019. Introduces recurrence on Transformers, relative position encoding scheme.\n",
            "Decoder-only architectures:\n",
            "Encoder-only architectures:\n",
            "Encoder-decoder architectures:\n",
            "So far, we’ve talked about the behavior (capabilities and harms) of large language models. Now, we peel open the first layer of the onion and start discussing how these models are constructed. The starting point of any machine learning approach is training data, so this is where we’ll start.\n",
            "Aside: Normally in machine learning, the training data and the test (evaluation) data are similar or at least of the same type. But for large language models, the training data is just “raw text”.\n",
            "In the rest of the lecture, we’ll talk about:\n",
            "Recall that large language models are trained on “raw text”. To be highly capable (e.g., have linguistic and world knowledge), this text should span a broad range of domains, genres, languages, etc.\n",
            "A natural place (but not the only place) to look for such text is the web, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and the Deep Web is even larger than that.\n",
            "It is worth noting that private datasets that reside in big companies are even larger than what’s available publicly. For example, WalMart generates 2.5 petabytes of data each hour!\n",
            "Common Crawl is a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot of Common Crawl has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index.\n",
            "Representation. Despite the richness of web data, it has been noted in Bender et al, 2021 that:\n",
            "Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models.\n",
            "WebText. The WebText dataset was used to train GPT-2.\n",
            "OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by the OpenWebText dataset.\n",
            "Toxicity analysis. Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found:\n",
            "The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model.\n",
            "Analysis. Dodge et al. 2021 performed a thorough analysis of the C4 dataset.\n",
            "Documentation levels:\n",
            "Note: Raffel et al. 2020 only provided scripts to recreate; cost thousands of dollars just to run these scripts.\n",
            "\n",
            "Benchmark data contamination.\n",
            "Example from the XSum summarization dataset:\n",
            "Input: The 48-year-old former Arsenal goalkeeper played for the Royals for four years. He was appointed youth academy director in 2000 and has been director of football since 2003. A West Brom statement said: “He played a key role in the Championship club twice winning promotion to the Premier League in 2006 and 2012. Output: West Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading.\n",
            "There are two types of contamination:\n",
            "Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage).\n",
            "The dataset could also be responsible for various harms:\n",
            "\n",
            "\n",
            "Data composition.\n",
            "\n",
            "Compare:\n",
            "\n",
            "Takeaway: The Pile contains a lot of information that’s not well covered by GPT-3’s dataset.\n",
            "They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.\n",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data.\n",
            "Two purposes:\n",
            "Dataset lifecycle (a sample of the questions from each category are provided below):\n",
            "Data statements. The data statements work is specialized to NLP datasets, and covers other aspects:\n",
            "As an example, let’s look at the datasheet for The Pile.\n",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles.\n",
            "Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant in industry.\n",
            "\n",
            "Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.\n",
            "Documentation for datasets:\n",
            "Datasets:\n",
            "Analysis of datasets:\n",
            "Filtering datasets:\n",
            "Data ecosystems:\n",
            "\\(\\newcommand{\\sV}{\\mathcal{V}} \\newcommand{\\sO}{\\mathcal{O}} \\newcommand{\\sD}{\\mathcal{D}} \\newcommand{\\sN}{\\mathcal{N}} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\E}{\\mathbb{E}} \\newcommand{\\x}{x_{1:L}} \\newcommand{\\tx}{\\tilde x_{1:L}} \\newcommand{\\nl}[1]{\\textsf{#1}} \\newcommand{\\softmax}{\\text{softmax}} \\newcommand{\\TransformerBlock}{\\text{TransformerBlock}} \\newcommand{\\EmbedTokenWithPosition}{\\text{EmbedTokenWithPosition}} \\newcommand{\\SentenceEmbedding}{\\text{SentenceEmbedding}} \\newcommand{\\BERT}{\\text{BERT}} \\newcommand{\\MASK}{\\nl{[MASK]}} \\newcommand{\\SEP}{\\nl{[SEP]}} \\newcommand{\\CLS}{\\nl{[CLS]}} \\newcommand{\\generate}[1]{\\stackrel{#1}{\\rightsquigarrow}} \\newcommand{\\embed}{\\stackrel{\\phi}{\\Rightarrow}}\\) Last lecture, we talked about the model architecture for large language models (e.g., the Transformer). In this lecture, we will discuss how to train large language models.\n",
            "We will consider objective functions for the three types of language models:\n",
            "We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers):\n",
            "Recall that an autoregressive language model defines a conditional distribution:\n",
            "We define it as follows:\n",
            "Succinctly:\n",
            "Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models.\n",
            "Let \\(\\sD\\) be the training data consisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function:\n",
            "There’s more to say about how to efficiently optimize this function, but that’s all there is for the objective.\n",
            "Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don’t need to generate.\n",
            "BERT. We will first present the BERT objective function, which contains two terms:\n",
            "Take the example sequence for natural language inference (predict entailment, contradiction, or neutral):\n",
            "There are two special tokens:\n",
            "Using our notation from the previous lecture, the BERT model is defined as:\n",
            "where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:\n",
            "\n",
            "BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters.\n",
            "Masked language modeling. The basic idea of the masked language model is to train on the prediction problem:\n",
            "More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\).\n",
            "Model. We first define the model distribution that takes \\(\\tx\\) and predicts each token independently given the contextual embedding:\n",
            "Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that:\n",
            "Here’s how \\(A\\) is defined:\n",
            "Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then:\n",
            "Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not.\n",
            "\\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\).\n",
            "\\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\).\n",
            "We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision.\n",
            "Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows:\n",
            "Objective. Then the BERT objective is:\n",
            "We will talk about training later, but a few quick notes about BERT:\n",
            "RoBERTa makes the following changes to BERT:\n",
            "Example task (table-to-text generation):\n",
            "Recall that encoder-decoder models (e.g., BART, T5):\n",
            "BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model.\n",
            "BART considers the following transformations \\(A(\\tx \\mid \\x)\\):  Based on BERT-scaled experiments, they decided on the following transformations for the final model:\n",
            "They demonstrated strong results on both classification and generation tasks using fine-tuning.\n",
            "T5 (Text-to-Text Transfer Transformer).\n",
            "T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model.\n",
            "Tasks:\n",
            "This paper experimented with many different unsupervised objectives:  and found that the “i.i.d. noise, replace spans” worked well (though many objectives were similar).\n",
            "They also cast all classical NLP tasks in a uniform framework as “text-to-text” tasks:  Note the difference in approach to classification tasks:\n",
            "Notes:\n",
            "Now we turn our attention to how to optimize the objective. For simplicity, let’s take autogressive language modeling:\n",
            "Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches:\n",
            "The key concerns in optimization are:\n",
            "There are several levels that we can approach optimization:\n",
            "ADAM (adaptive moment estimation). ADAM incorporates two ideas:\n",
            "Have an adaptive (different) step size for each dimension of \\(\\theta\\) (inspiration from second-order methods).\n",
            "Updating parameters.\n",
            "Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)).\n",
            "AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint.\n",
            "Mixed-precision training is another method for reducing memory (Narang et al., 2018).\n",
            "\n",
            "Learning rates.\n",
            "Initialization.\n",
            "For GPT-3:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Attention Is All You Need\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1 Introduction\n",
            "2 Background\n",
            "\n",
            "3 Model Architecture\n",
            "\n",
            "\n",
            "3.1 Encoder and Decoder Stacks\n",
            "\n",
            "Encoder:\n",
            "Decoder:\n",
            "\n",
            "\n",
            "\n",
            "3.2 Attention\n",
            "\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "3.2.2 Multi-Head Attention\n",
            "3.2.3 Applications of Attention in our Model\n",
            "\n",
            "\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "3.4 Embeddings and Softmax\n",
            "3.5 Positional Encoding\n",
            "\n",
            "\n",
            "4 Why Self-Attention\n",
            "\n",
            "5 Training\n",
            "\n",
            "5.1 Training Data and Batching\n",
            "5.2 Hardware and Schedule\n",
            "5.3 Optimizer\n",
            "\n",
            "5.4 Regularization\n",
            "\n",
            "Residual Dropout\n",
            "Label Smoothing\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6 Results\n",
            "\n",
            "6.1 Machine Translation\n",
            "6.2 Model Variations\n",
            "6.3 English Constituency Parsing\n",
            "\n",
            "\n",
            "\n",
            "7 Conclusion\n",
            "\n",
            "Acknowledgements\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\n",
            "\n",
            "Attention Is All You Need\n",
            "\n",
            "\n",
            "\n",
            "\\ANDAshish Vaswani\n",
            "Google Brain\n",
            "avaswani@google.com\n",
            "&Noam Shazeer11footnotemark: 1\n",
            "Google Brain\n",
            "noam@google.com\n",
            "&Niki Parmar11footnotemark: 1\n",
            "Google Research\n",
            "nikip@google.com\n",
            "&Jakob Uszkoreit11footnotemark: 1\n",
            "Google Research\n",
            "usz@google.com\n",
            "&Llion Jones11footnotemark: 1\n",
            "Google Research\n",
            "llion@google.com\n",
            "&Aidan N. Gomez11footnotemark: 1   \n",
            "University of Toronto\n",
            "aidan@cs.toronto.edu\n",
            "&Łukasz Kaiser11footnotemark: 1\n",
            "Google Brain\n",
            "lukaszkaiser@google.com\n",
            "&Illia Polosukhin11footnotemark: 1   \n",
            "illia.polosukhin@gmail.com\n",
            "\n",
            "Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.\n",
            "Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n",
            "Work performed while at Google Brain.Work performed while at Google Research.\n",
            "\n",
            "\n",
            "Abstract\n",
            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n",
            "\n",
            "\n",
            "\n",
            "1 Introduction\n",
            "\n",
            "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\n",
            "\n",
            "\n",
            "Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states htsubscriptℎ𝑡h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, as a function of the previous hidden state ht−1subscriptℎ𝑡1h_{t-1}italic_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT and the input for position t𝑡titalic_t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.\n",
            "Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\n",
            "\n",
            "\n",
            "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\n",
            "\n",
            "\n",
            "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "2 Background\n",
            "\n",
            "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
            "\n",
            "\n",
            "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
            "\n",
            "\n",
            "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n",
            "\n",
            "\n",
            "To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
            "In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3 Model Architecture\n",
            "\n",
            "Figure 1: The Transformer - model architecture.\n",
            "\n",
            "\n",
            "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,…,xn)subscript𝑥1…subscript𝑥𝑛(x_{1},...,x_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) to a sequence of continuous representations 𝐳=(z1,…,zn)𝐳subscript𝑧1…subscript𝑧𝑛\\mathbf{z}=(z_{1},...,z_{n})bold_z = ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ). Given 𝐳𝐳\\mathbf{z}bold_z, the decoder then generates an output sequence (y1,…,ym)subscript𝑦1…subscript𝑦𝑚(y_{1},...,y_{m})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\n",
            "\n",
            "\n",
            "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n",
            "\n",
            "\n",
            "\n",
            "3.1 Encoder and Decoder Stacks\n",
            "\n",
            "Encoder:\n",
            "\n",
            "The encoder is composed of a stack of N=6𝑁6N=6italic_N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm⁢(x+Sublayer⁢(x))LayerNorm𝑥Sublayer𝑥\\mathrm{LayerNorm}(x+\\mathrm{Sublayer}(x))roman_LayerNorm ( italic_x + roman_Sublayer ( italic_x ) ), where Sublayer⁢(x)Sublayer𝑥\\mathrm{Sublayer}(x)roman_Sublayer ( italic_x ) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512subscript𝑑model512d_{\\text{model}}=512italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 512.\n",
            "\n",
            "\n",
            "\n",
            "Decoder:\n",
            "\n",
            "The decoder is also composed of a stack of N=6𝑁6N=6italic_N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i𝑖iitalic_i can depend only on the known outputs at positions less than i𝑖iitalic_i.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.2 Attention\n",
            "\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
            "\n",
            "\n",
            "\n",
            "3.2.1 Scaled Dot-Product Attention\n",
            "\n",
            "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, and values of dimension dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT. We compute the dot products of the query with all keys, divide each by dksubscript𝑑𝑘\\sqrt{d_{k}}square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG, and apply a softmax function to obtain the weights on the values.\n",
            "\n",
            "\n",
            "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q𝑄Qitalic_Q. The keys and values are also packed together into matrices K𝐾Kitalic_K and V𝑉Vitalic_V. We compute the matrix of outputs as:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Attention⁢(Q,K,V)=softmax⁢(Q⁢KTdk)⁢VAttention𝑄𝐾𝑉softmax𝑄superscript𝐾𝑇subscript𝑑𝑘𝑉\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{QK^{T}}{\\sqrt{d_{k}}})Vroman_Attention ( italic_Q , italic_K , italic_V ) = roman_softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_V\n",
            "\n",
            "(1)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1dk1subscript𝑑𝑘\\frac{1}{\\sqrt{d_{k}}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
            "\n",
            "\n",
            "While for small values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT [3]. We suspect that for large values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of q𝑞qitalic_q and k𝑘kitalic_k are independent random variables with mean 00 and variance 1111. Then their dot product, q⋅k=∑i=1dkqi⁢ki⋅𝑞𝑘superscriptsubscript𝑖1subscript𝑑𝑘subscript𝑞𝑖subscript𝑘𝑖q\\cdot k=\\sum_{i=1}^{d_{k}}q_{i}k_{i}italic_q ⋅ italic_k = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, has mean 00 and variance dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT.. To counteract this effect, we scale the dot products by 1dk1subscript𝑑𝑘\\frac{1}{\\sqrt{d_{k}}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.2.2 Multi-Head Attention\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Scaled Dot-Product Attention\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Multi-Head Attention\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
            "\n",
            "\n",
            "Instead of performing a single attention function with dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values hℎhitalic_h times with different, learned linear projections to dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT dimensions, respectively.\n",
            "On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\n",
            "\n",
            "\n",
            "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "MultiHead⁢(Q,K,V)MultiHead𝑄𝐾𝑉\\displaystyle\\mathrm{MultiHead}(Q,K,V)roman_MultiHead ( italic_Q , italic_K , italic_V )\n",
            "=Concat⁢(head1,…,headh)⁢WOabsentConcatsubscripthead1…subscriptheadhsuperscript𝑊𝑂\\displaystyle=\\mathrm{Concat}(\\mathrm{head_{1}},...,\\mathrm{head_{h}})W^{O}= roman_Concat ( roman_head start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , roman_head start_POSTSUBSCRIPT roman_h end_POSTSUBSCRIPT ) italic_W start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "where⁢headiwheresubscriptheadi\\displaystyle\\text{where}~{}\\mathrm{head_{i}}where roman_head start_POSTSUBSCRIPT roman_i end_POSTSUBSCRIPT\n",
            "=Attention⁢(Q⁢WiQ,K⁢WiK,V⁢WiV)absentAttention𝑄subscriptsuperscript𝑊𝑄𝑖𝐾subscriptsuperscript𝑊𝐾𝑖𝑉subscriptsuperscript𝑊𝑉𝑖\\displaystyle=\\mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})= roman_Attention ( italic_Q italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_K italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_V italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Where the projections are parameter matrices WiQ∈ℝdmodel×dksubscriptsuperscript𝑊𝑄𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑘W^{Q}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WiK∈ℝdmodel×dksubscriptsuperscript𝑊𝐾𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑘W^{K}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{k}}italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WiV∈ℝdmodel×dvsubscriptsuperscript𝑊𝑉𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑣W^{V}_{i}\\in\\mathbb{R}^{d_{\\text{model}}\\times d_{v}}italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and WO∈ℝh⁢dv×dmodelsuperscript𝑊𝑂superscriptℝℎsubscript𝑑𝑣subscript𝑑modelW^{O}\\in\\mathbb{R}^{hd_{v}\\times d_{\\text{model}}}italic_W start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_h italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT.\n",
            "\n",
            "\n",
            "In this work we employ h=8ℎ8h=8italic_h = 8 parallel attention layers, or heads. For each of these we use dk=dv=dmodel/h=64subscript𝑑𝑘subscript𝑑𝑣subscript𝑑modelℎ64d_{k}=d_{v}=d_{\\text{model}}/h=64italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT / italic_h = 64.\n",
            "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.2.3 Applications of Attention in our Model\n",
            "\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "\n",
            "\n",
            "•\n",
            "\n",
            "In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
            "\n",
            "\n",
            "\n",
            "•\n",
            "\n",
            "The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
            "\n",
            "\n",
            "\n",
            "•\n",
            "\n",
            "Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞-\\infty- ∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.3 Position-wise Feed-Forward Networks\n",
            "\n",
            "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "FFN⁢(x)=max⁡(0,x⁢W1+b1)⁢W2+b2FFN𝑥0𝑥subscript𝑊1subscript𝑏1subscript𝑊2subscript𝑏2\\mathrm{FFN}(x)=\\max(0,xW_{1}+b_{1})W_{2}+b_{2}roman_FFN ( italic_x ) = roman_max ( 0 , italic_x italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT\n",
            "\n",
            "(2)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel=512subscript𝑑model512d_{\\text{model}}=512italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 512, and the inner-layer has dimensionality df⁢f=2048subscript𝑑𝑓𝑓2048d_{ff}=2048italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT = 2048.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.4 Embeddings and Softmax\n",
            "\n",
            "Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by dmodelsubscript𝑑model\\sqrt{d_{\\text{model}}}square-root start_ARG italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_ARG.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3.5 Positional Encoding\n",
            "\n",
            "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\n",
            "\n",
            "\n",
            "In this work, we use sine and cosine functions of different frequencies:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "P⁢E(p⁢o⁢s,2⁢i)=s⁢i⁢n⁢(p⁢o⁢s/100002⁢i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖𝑠𝑖𝑛𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\text{model}}})italic_P italic_E start_POSTSUBSCRIPT ( italic_p italic_o italic_s , 2 italic_i ) end_POSTSUBSCRIPT = italic_s italic_i italic_n ( italic_p italic_o italic_s / 10000 start_POSTSUPERSCRIPT 2 italic_i / italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "P⁢E(p⁢o⁢s,2⁢i+1)=c⁢o⁢s⁢(p⁢o⁢s/100002⁢i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖1𝑐𝑜𝑠𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\text{model}}})italic_P italic_E start_POSTSUBSCRIPT ( italic_p italic_o italic_s , 2 italic_i + 1 ) end_POSTSUBSCRIPT = italic_c italic_o italic_s ( italic_p italic_o italic_s / 10000 start_POSTSUPERSCRIPT 2 italic_i / italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "where p⁢o⁢s𝑝𝑜𝑠positalic_p italic_o italic_s is the position and i𝑖iitalic_i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2⁢π2𝜋2\\pi2 italic_π to 10000⋅2⁢π⋅100002𝜋10000\\cdot 2\\pi10000 ⋅ 2 italic_π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k𝑘kitalic_k, P⁢Ep⁢o⁢s+k𝑃subscript𝐸𝑝𝑜𝑠𝑘PE_{pos+k}italic_P italic_E start_POSTSUBSCRIPT italic_p italic_o italic_s + italic_k end_POSTSUBSCRIPT can be represented as a linear function of P⁢Ep⁢o⁢s𝑃subscript𝐸𝑝𝑜𝑠PE_{pos}italic_P italic_E start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT.\n",
            "\n",
            "\n",
            "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4 Why Self-Attention\n",
            "\n",
            "In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1,…,xn)subscript𝑥1…subscript𝑥𝑛(x_{1},...,x_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) to another sequence of equal length (z1,…,zn)subscript𝑧1…subscript𝑧𝑛(z_{1},...,z_{n})( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ), with xi,zi∈ℝdsubscript𝑥𝑖subscript𝑧𝑖superscriptℝ𝑑x_{i},z_{i}\\in\\mathbb{R}^{d}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\n",
            "\n",
            "\n",
            "One is the total computational complexity per layer.\n",
            "Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\n",
            "\n",
            "\n",
            "The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\n",
            "\n",
            "\n",
            "Table 1: \n",
            "Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n𝑛nitalic_n is the sequence length, d𝑑ditalic_d is the representation dimension, k𝑘kitalic_k is the kernel size of convolutions and r𝑟ritalic_r the size of the neighborhood in restricted self-attention.\n",
            "\n",
            "\n",
            "\n",
            "Layer Type\n",
            "Complexity per Layer\n",
            "Sequential\n",
            "Maximum Path Length\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Operations\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Self-Attention\n",
            "O⁢(n2⋅d)𝑂⋅superscript𝑛2𝑑O(n^{2}\\cdot d)italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⋅ italic_d )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "\n",
            "\n",
            "Recurrent\n",
            "O⁢(n⋅d2)𝑂⋅𝑛superscript𝑑2O(n\\cdot d^{2})italic_O ( italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )\n",
            "O⁢(n)𝑂𝑛O(n)italic_O ( italic_n )\n",
            "O⁢(n)𝑂𝑛O(n)italic_O ( italic_n )\n",
            "\n",
            "\n",
            "Convolutional\n",
            "O⁢(k⋅n⋅d2)𝑂⋅𝑘𝑛superscript𝑑2O(k\\cdot n\\cdot d^{2})italic_O ( italic_k ⋅ italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "O⁢(l⁢o⁢gk⁢(n))𝑂𝑙𝑜subscript𝑔𝑘𝑛O(log_{k}(n))italic_O ( italic_l italic_o italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_n ) )\n",
            "\n",
            "\n",
            "Self-Attention (restricted)\n",
            "O⁢(r⋅n⋅d)𝑂⋅𝑟𝑛𝑑O(r\\cdot n\\cdot d)italic_O ( italic_r ⋅ italic_n ⋅ italic_d )\n",
            "O⁢(1)𝑂1O(1)italic_O ( 1 )\n",
            "O⁢(n/r)𝑂𝑛𝑟O(n/r)italic_O ( italic_n / italic_r )\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O⁢(n)𝑂𝑛O(n)italic_O ( italic_n ) sequential operations.\n",
            "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n𝑛nitalic_n is smaller than the representation dimensionality d𝑑ditalic_d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations.\n",
            "To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r𝑟ritalic_r in the input sequence centered around the respective output position. This would increase the maximum path length to O⁢(n/r)𝑂𝑛𝑟O(n/r)italic_O ( italic_n / italic_r ). We plan to investigate this approach further in future work.\n",
            "\n",
            "\n",
            "A single convolutional layer with kernel width k<n𝑘𝑛k<nitalic_k < italic_n does not connect all pairs of input and output positions. Doing so requires a stack of O⁢(n/k)𝑂𝑛𝑘O(n/k)italic_O ( italic_n / italic_k ) convolutional layers in the case of contiguous kernels, or O⁢(l⁢o⁢gk⁢(n))𝑂𝑙𝑜subscript𝑔𝑘𝑛O(log_{k}(n))italic_O ( italic_l italic_o italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_n ) ) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network.\n",
            "Convolutional layers are generally more expensive than recurrent layers, by a factor of k𝑘kitalic_k. Separable convolutions [6], however, decrease the complexity considerably, to O⁢(k⋅n⋅d+n⋅d2)𝑂⋅𝑘𝑛𝑑⋅𝑛superscript𝑑2O(k\\cdot n\\cdot d+n\\cdot d^{2})italic_O ( italic_k ⋅ italic_n ⋅ italic_d + italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Even with k=n𝑘𝑛k=nitalic_k = italic_n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\n",
            "\n",
            "\n",
            "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5 Training\n",
            "\n",
            "This section describes the training regime for our models.\n",
            "\n",
            "\n",
            "\n",
            "5.1 Training Data and Batching\n",
            "\n",
            "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.2 Hardware and Schedule\n",
            "\n",
            "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.3 Optimizer\n",
            "\n",
            "We used the Adam optimizer [20] with β1=0.9subscript𝛽10.9\\beta_{1}=0.9italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, β2=0.98subscript𝛽20.98\\beta_{2}=0.98italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.98 and ϵ=10−9italic-ϵsuperscript109\\epsilon=10^{-9}italic_ϵ = 10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT. We varied the learning rate over the course of training, according to the formula:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "l⁢r⁢a⁢t⁢e=dmodel−0.5⋅min⁡(s⁢t⁢e⁢p⁢_⁢n⁢u⁢m−0.5,s⁢t⁢e⁢p⁢_⁢n⁢u⁢m⋅w⁢a⁢r⁢m⁢u⁢p⁢_⁢s⁢t⁢e⁢p⁢s−1.5)𝑙𝑟𝑎𝑡𝑒⋅superscriptsubscript𝑑model0.5𝑠𝑡𝑒𝑝_𝑛𝑢superscript𝑚0.5⋅𝑠𝑡𝑒𝑝_𝑛𝑢𝑚𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝superscript𝑠1.5lrate=d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{%\n",
            "warmup\\_steps}^{-1.5})italic_l italic_r italic_a italic_t italic_e = italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT ⋅ roman_min ( italic_s italic_t italic_e italic_p _ italic_n italic_u italic_m start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT , italic_s italic_t italic_e italic_p _ italic_n italic_u italic_m ⋅ italic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s start_POSTSUPERSCRIPT - 1.5 end_POSTSUPERSCRIPT )\n",
            "\n",
            "(3)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "This corresponds to increasing the learning rate linearly for the first w⁢a⁢r⁢m⁢u⁢p⁢_⁢s⁢t⁢e⁢p⁢s𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝𝑠warmup\\_stepsitalic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used w⁢a⁢r⁢m⁢u⁢p⁢_⁢s⁢t⁢e⁢p⁢s=4000𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝𝑠4000warmup\\_steps=4000italic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s = 4000.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.4 Regularization\n",
            "\n",
            "We employ three types of regularization during training:\n",
            "\n",
            "\n",
            "Residual Dropout\n",
            "\n",
            "We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pd⁢r⁢o⁢p=0.1subscript𝑃𝑑𝑟𝑜𝑝0.1P_{drop}=0.1italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT = 0.1.\n",
            "\n",
            "\n",
            "\n",
            "Label Smoothing\n",
            "\n",
            "During training, we employed label smoothing of value ϵl⁢s=0.1subscriptitalic-ϵ𝑙𝑠0.1\\epsilon_{ls}=0.1italic_ϵ start_POSTSUBSCRIPT italic_l italic_s end_POSTSUBSCRIPT = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6 Results\n",
            "\n",
            "\n",
            "6.1 Machine Translation\n",
            "\n",
            "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. \n",
            "\n",
            "\n",
            "\n",
            "Model\n",
            "BLEU\n",
            "\n",
            "Training Cost (FLOPs)\n",
            "\n",
            "\n",
            "EN-DE\n",
            "EN-FR\n",
            "\n",
            "EN-DE\n",
            "EN-FR\n",
            "\n",
            "\n",
            "ByteNet [18]\n",
            "\n",
            "23.75\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Deep-Att + PosUnk [39]\n",
            "\n",
            "\n",
            "39.2\n",
            "\n",
            "\n",
            "1.0⋅1020⋅1.0superscript10201.0\\cdot 10^{20}1.0 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "GNMT + RL [38]\n",
            "\n",
            "24.6\n",
            "39.92\n",
            "\n",
            "2.3⋅1019⋅2.3superscript10192.3\\cdot 10^{19}2.3 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "1.4⋅1020⋅1.4superscript10201.4\\cdot 10^{20}1.4 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "ConvS2S [9]\n",
            "\n",
            "25.16\n",
            "40.46\n",
            "\n",
            "9.6⋅1018⋅9.6superscript10189.6\\cdot 10^{18}9.6 ⋅ 10 start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPT\n",
            "1.5⋅1020⋅1.5superscript10201.5\\cdot 10^{20}1.5 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "MoE [32]\n",
            "\n",
            "26.03\n",
            "40.56\n",
            "\n",
            "2.0⋅1019⋅2.0superscript10192.0\\cdot 10^{19}2.0 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "1.2⋅1020⋅1.2superscript10201.2\\cdot 10^{20}1.2 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "Deep-Att + PosUnk Ensemble [39]\n",
            "\n",
            "\n",
            "40.4\n",
            "\n",
            "\n",
            "8.0⋅1020⋅8.0superscript10208.0\\cdot 10^{20}8.0 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "GNMT + RL Ensemble [38]\n",
            "\n",
            "26.30\n",
            "41.16\n",
            "\n",
            "1.8⋅1020⋅1.8superscript10201.8\\cdot 10^{20}1.8 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\n",
            "1.1⋅1021⋅1.1superscript10211.1\\cdot 10^{21}1.1 ⋅ 10 start_POSTSUPERSCRIPT 21 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "ConvS2S Ensemble [9]\n",
            "\n",
            "26.36\n",
            "41.29\n",
            "\n",
            "7.7⋅1019⋅7.7superscript10197.7\\cdot 10^{19}7.7 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "1.2⋅1021⋅1.2superscript10211.2\\cdot 10^{21}1.2 ⋅ 10 start_POSTSUPERSCRIPT 21 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "Transformer (base model)\n",
            "27.3\n",
            "38.1\n",
            "\n",
            "3.3⋅𝟏𝟎𝟏𝟖bold-⋅3.3superscript10183.3\\cdot 10^{18}bold_3.3 bold_⋅ bold_10 start_POSTSUPERSCRIPT bold_18 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "Transformer (big)\n",
            "28.4\n",
            "41.8\n",
            "\n",
            "2.3⋅1019⋅2.3superscript10192.3\\cdot 10^{19}2.3 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.02.02.02.0 BLEU, establishing a new state-of-the-art BLEU score of 28.428.428.428.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.53.53.53.5 days on 8888 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\n",
            "\n",
            "\n",
            "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.041.041.041.0, outperforming all of the previously published single models, at less than 1/4141/41 / 4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pd⁢r⁢o⁢p=0.1subscript𝑃𝑑𝑟𝑜𝑝0.1P_{drop}=0.1italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT = 0.1, instead of 0.30.30.30.3.\n",
            "\n",
            "\n",
            "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4444 and length penalty α=0.6𝛼0.6\\alpha=0.6italic_α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50505050, but terminate early when possible [38].\n",
            "\n",
            "\n",
            "Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 222We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively..\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6.2 Model Variations\n",
            "\n",
            "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "N𝑁Nitalic_N\n",
            "dmodelsubscript𝑑modeld_{\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT\n",
            "dffsubscript𝑑ffd_{\\text{ff}}italic_d start_POSTSUBSCRIPT ff end_POSTSUBSCRIPT\n",
            "hℎhitalic_h\n",
            "dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT\n",
            "dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT\n",
            "Pd⁢r⁢o⁢psubscript𝑃𝑑𝑟𝑜𝑝P_{drop}italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT\n",
            "ϵl⁢ssubscriptitalic-ϵ𝑙𝑠\\epsilon_{ls}italic_ϵ start_POSTSUBSCRIPT italic_l italic_s end_POSTSUBSCRIPT\n",
            "train\n",
            "PPL\n",
            "BLEU\n",
            "params\n",
            "\n",
            "\n",
            "\n",
            "steps\n",
            "(dev)\n",
            "(dev)\n",
            "×106absentsuperscript106\\times 10^{6}× 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "base\n",
            "6\n",
            "512\n",
            "2048\n",
            "8\n",
            "64\n",
            "64\n",
            "0.1\n",
            "0.1\n",
            "100K\n",
            "4.92\n",
            "25.8\n",
            "65\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(A)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1\n",
            "512\n",
            "512\n",
            "\n",
            "\n",
            "\n",
            "5.29\n",
            "24.9\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4\n",
            "128\n",
            "128\n",
            "\n",
            "\n",
            "\n",
            "5.00\n",
            "25.5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16\n",
            "32\n",
            "32\n",
            "\n",
            "\n",
            "\n",
            "4.91\n",
            "25.8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "32\n",
            "16\n",
            "16\n",
            "\n",
            "\n",
            "\n",
            "5.01\n",
            "25.4\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(B)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "16\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.16\n",
            "25.1\n",
            "58\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "32\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.01\n",
            "25.4\n",
            "60\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(C)\n",
            "\n",
            "2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6.11\n",
            "23.7\n",
            "36\n",
            "\n",
            "\n",
            "4\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.19\n",
            "25.3\n",
            "50\n",
            "\n",
            "\n",
            "8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4.88\n",
            "25.5\n",
            "80\n",
            "\n",
            "\n",
            "\n",
            "256\n",
            "\n",
            "\n",
            "32\n",
            "32\n",
            "\n",
            "\n",
            "\n",
            "5.75\n",
            "24.5\n",
            "28\n",
            "\n",
            "\n",
            "\n",
            "1024\n",
            "\n",
            "\n",
            "128\n",
            "128\n",
            "\n",
            "\n",
            "\n",
            "4.66\n",
            "26.0\n",
            "168\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1024\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5.12\n",
            "25.4\n",
            "53\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4096\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "4.75\n",
            "26.2\n",
            "90\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(D)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.0\n",
            "\n",
            "\n",
            "5.77\n",
            "24.6\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.2\n",
            "\n",
            "\n",
            "4.95\n",
            "25.5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.0\n",
            "\n",
            "4.67\n",
            "25.3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "0.2\n",
            "\n",
            "5.47\n",
            "25.7\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "(E)\n",
            "\n",
            "positional embedding instead of sinusoids\n",
            "\n",
            "4.92\n",
            "25.7\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "big\n",
            "6\n",
            "1024\n",
            "4096\n",
            "16\n",
            "\n",
            "\n",
            "0.3\n",
            "\n",
            "300K\n",
            "4.33\n",
            "26.4\n",
            "213\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n",
            "\n",
            "\n",
            "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
            "\n",
            "\n",
            "In Table 3 rows (B), we observe that reducing the attention key size dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "6.3 English Constituency Parsing\n",
            "\n",
            "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\n",
            "\n",
            "\n",
            "\n",
            "Parser\n",
            "Training\n",
            "WSJ 23 F1\n",
            "\n",
            "\n",
            "Vinyals & Kaiser el al. (2014) [37]\n",
            "\n",
            "WSJ only, discriminative\n",
            "88.3\n",
            "\n",
            "\n",
            "Petrov et al. (2006) [29]\n",
            "\n",
            "WSJ only, discriminative\n",
            "90.4\n",
            "\n",
            "\n",
            "Zhu et al. (2013) [40]\n",
            "\n",
            "WSJ only, discriminative\n",
            "90.4\n",
            "\n",
            "\n",
            "Dyer et al. (2016) [8]\n",
            "\n",
            "WSJ only, discriminative\n",
            "91.7\n",
            "\n",
            "\n",
            "Transformer (4 layers)\n",
            "WSJ only, discriminative\n",
            "91.3\n",
            "\n",
            "\n",
            "Zhu et al. (2013) [40]\n",
            "\n",
            "semi-supervised\n",
            "91.3\n",
            "\n",
            "\n",
            "Huang & Harper (2009) [14]\n",
            "\n",
            "semi-supervised\n",
            "91.3\n",
            "\n",
            "\n",
            "McClosky et al. (2006) [26]\n",
            "\n",
            "semi-supervised\n",
            "92.1\n",
            "\n",
            "\n",
            "Vinyals & Kaiser el al. (2014) [37]\n",
            "\n",
            "semi-supervised\n",
            "92.1\n",
            "\n",
            "\n",
            "Transformer (4 layers)\n",
            "semi-supervised\n",
            "92.7\n",
            "\n",
            "\n",
            "Luong et al. (2015) [23]\n",
            "\n",
            "multi-task\n",
            "93.0\n",
            "\n",
            "\n",
            "Dyer et al. (2016) [8]\n",
            "\n",
            "generative\n",
            "93.3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.\n",
            "Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].\n",
            "\n",
            "\n",
            "We trained a 4-layer transformer with dm⁢o⁢d⁢e⁢l=1024subscript𝑑𝑚𝑜𝑑𝑒𝑙1024d_{model}=1024italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.\n",
            "\n",
            "\n",
            "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300300300300. We used a beam size of 21212121 and α=0.3𝛼0.3\\alpha=0.3italic_α = 0.3 for both WSJ only and the semi-supervised setting.\n",
            "\n",
            "\n",
            "Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].\n",
            "\n",
            "\n",
            "In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "7 Conclusion\n",
            "\n",
            "In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n",
            "\n",
            "\n",
            "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n",
            "\n",
            "\n",
            "We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.\n",
            "Making generation less sequential is another research goals of ours.\n",
            "\n",
            "\n",
            "The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.\n",
            "\n",
            "\n",
            "Acknowledgements\n",
            "\n",
            "We are grateful to Nal Kalchbrenner and Stephan Gouws for\n",
            "their fruitful comments, corrections and inspiration.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "References\n",
            "\n",
            "\n",
            "[1]\n",
            "\n",
            "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\n",
            "\n",
            "\n",
            "Layer normalization.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1607.06450, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[2]\n",
            "\n",
            "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\n",
            "\n",
            "\n",
            "Neural machine translation by jointly learning to align and\n",
            "translate.\n",
            "\n",
            "\n",
            "CoRR, abs/1409.0473, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[3]\n",
            "\n",
            "Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le.\n",
            "\n",
            "\n",
            "Massive exploration of neural machine translation architectures.\n",
            "\n",
            "\n",
            "CoRR, abs/1703.03906, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[4]\n",
            "\n",
            "Jianpeng Cheng, Li Dong, and Mirella Lapata.\n",
            "\n",
            "\n",
            "Long short-term memory-networks for machine reading.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1601.06733, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[5]\n",
            "\n",
            "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger\n",
            "Schwenk, and Yoshua Bengio.\n",
            "\n",
            "\n",
            "Learning phrase representations using rnn encoder-decoder for\n",
            "statistical machine translation.\n",
            "\n",
            "\n",
            "CoRR, abs/1406.1078, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[6]\n",
            "\n",
            "Francois Chollet.\n",
            "\n",
            "\n",
            "Xception: Deep learning with depthwise separable convolutions.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1610.02357, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[7]\n",
            "\n",
            "Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua\n",
            "Bengio.\n",
            "\n",
            "\n",
            "Empirical evaluation of gated recurrent neural networks on sequence\n",
            "modeling.\n",
            "\n",
            "\n",
            "CoRR, abs/1412.3555, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[8]\n",
            "\n",
            "Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith.\n",
            "\n",
            "\n",
            "Recurrent neural network grammars.\n",
            "\n",
            "\n",
            "In Proc. of NAACL, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[9]\n",
            "\n",
            "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.\n",
            "\n",
            "\n",
            "Convolutional sequence to sequence learning.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1705.03122v2, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[10]\n",
            "\n",
            "Alex Graves.\n",
            "\n",
            "\n",
            "Generating sequences with recurrent neural networks.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1308.0850, 2013.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[11]\n",
            "\n",
            "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
            "\n",
            "\n",
            "Deep residual learning for image recognition.\n",
            "\n",
            "\n",
            "In Proceedings of the IEEE Conference on Computer Vision and\n",
            "Pattern Recognition, pages 770–778, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[12]\n",
            "\n",
            "Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.\n",
            "\n",
            "\n",
            "Gradient flow in recurrent nets: the difficulty of learning long-term\n",
            "dependencies, 2001.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[13]\n",
            "\n",
            "Sepp Hochreiter and Jürgen Schmidhuber.\n",
            "\n",
            "\n",
            "Long short-term memory.\n",
            "\n",
            "\n",
            "Neural computation, 9(8):1735–1780, 1997.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[14]\n",
            "\n",
            "Zhongqiang Huang and Mary Harper.\n",
            "\n",
            "\n",
            "Self-training PCFG grammars with latent annotations across\n",
            "languages.\n",
            "\n",
            "\n",
            "In Proceedings of the 2009 Conference on Empirical Methods in\n",
            "Natural Language Processing, pages 832–841. ACL, August 2009.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[15]\n",
            "\n",
            "Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.\n",
            "\n",
            "\n",
            "Exploring the limits of language modeling.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1602.02410, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[16]\n",
            "\n",
            "Łukasz Kaiser and Samy Bengio.\n",
            "\n",
            "\n",
            "Can active memory replace attention?\n",
            "\n",
            "\n",
            "In Advances in Neural Information Processing Systems, (NIPS),\n",
            "2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[17]\n",
            "\n",
            "Łukasz Kaiser and Ilya Sutskever.\n",
            "\n",
            "\n",
            "Neural GPUs learn algorithms.\n",
            "\n",
            "\n",
            "In International Conference on Learning Representations\n",
            "(ICLR), 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[18]\n",
            "\n",
            "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex\n",
            "Graves, and Koray Kavukcuoglu.\n",
            "\n",
            "\n",
            "Neural machine translation in linear time.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1610.10099v2, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[19]\n",
            "\n",
            "Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush.\n",
            "\n",
            "\n",
            "Structured attention networks.\n",
            "\n",
            "\n",
            "In International Conference on Learning Representations, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[20]\n",
            "\n",
            "Diederik Kingma and Jimmy Ba.\n",
            "\n",
            "\n",
            "Adam: A method for stochastic optimization.\n",
            "\n",
            "\n",
            "In ICLR, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[21]\n",
            "\n",
            "Oleksii Kuchaiev and Boris Ginsburg.\n",
            "\n",
            "\n",
            "Factorization tricks for LSTM networks.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1703.10722, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[22]\n",
            "\n",
            "Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
            "Zhou, and Yoshua Bengio.\n",
            "\n",
            "\n",
            "A structured self-attentive sentence embedding.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1703.03130, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[23]\n",
            "\n",
            "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.\n",
            "\n",
            "\n",
            "Multi-task sequence to sequence learning.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1511.06114, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[24]\n",
            "\n",
            "Minh-Thang Luong, Hieu Pham, and Christopher D Manning.\n",
            "\n",
            "\n",
            "Effective approaches to attention-based neural machine translation.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1508.04025, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[25]\n",
            "\n",
            "Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini.\n",
            "\n",
            "\n",
            "Building a large annotated corpus of english: The penn treebank.\n",
            "\n",
            "\n",
            "Computational linguistics, 19(2):313–330, 1993.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[26]\n",
            "\n",
            "David McClosky, Eugene Charniak, and Mark Johnson.\n",
            "\n",
            "\n",
            "Effective self-training for parsing.\n",
            "\n",
            "\n",
            "In Proceedings of the Human Language Technology Conference of\n",
            "the NAACL, Main Conference, pages 152–159. ACL, June 2006.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[27]\n",
            "\n",
            "Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.\n",
            "\n",
            "\n",
            "A decomposable attention model.\n",
            "\n",
            "\n",
            "In Empirical Methods in Natural Language Processing, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[28]\n",
            "\n",
            "Romain Paulus, Caiming Xiong, and Richard Socher.\n",
            "\n",
            "\n",
            "A deep reinforced model for abstractive summarization.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1705.04304, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[29]\n",
            "\n",
            "Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.\n",
            "\n",
            "\n",
            "Learning accurate, compact, and interpretable tree annotation.\n",
            "\n",
            "\n",
            "In Proceedings of the 21st International Conference on\n",
            "Computational Linguistics and 44th Annual Meeting of the ACL, pages\n",
            "433–440. ACL, July 2006.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[30]\n",
            "\n",
            "Ofir Press and Lior Wolf.\n",
            "\n",
            "\n",
            "Using the output embedding to improve language models.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1608.05859, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[31]\n",
            "\n",
            "Rico Sennrich, Barry Haddow, and Alexandra Birch.\n",
            "\n",
            "\n",
            "Neural machine translation of rare words with subword units.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1508.07909, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[32]\n",
            "\n",
            "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,\n",
            "Geoffrey Hinton, and Jeff Dean.\n",
            "\n",
            "\n",
            "Outrageously large neural networks: The sparsely-gated\n",
            "mixture-of-experts layer.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1701.06538, 2017.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[33]\n",
            "\n",
            "Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and\n",
            "Ruslan Salakhutdinov.\n",
            "\n",
            "\n",
            "Dropout: a simple way to prevent neural networks from overfitting.\n",
            "\n",
            "\n",
            "Journal of Machine Learning Research, 15(1):1929–1958, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[34]\n",
            "\n",
            "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.\n",
            "\n",
            "\n",
            "End-to-end memory networks.\n",
            "\n",
            "\n",
            "In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett,\n",
            "editors, Advances in Neural Information Processing Systems 28, pages\n",
            "2440–2448. Curran Associates, Inc., 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[35]\n",
            "\n",
            "Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.\n",
            "\n",
            "\n",
            "Sequence to sequence learning with neural networks.\n",
            "\n",
            "\n",
            "In Advances in Neural Information Processing Systems, pages\n",
            "3104–3112, 2014.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[36]\n",
            "\n",
            "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and\n",
            "Zbigniew Wojna.\n",
            "\n",
            "\n",
            "Rethinking the inception architecture for computer vision.\n",
            "\n",
            "\n",
            "CoRR, abs/1512.00567, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[37]\n",
            "\n",
            "Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton.\n",
            "\n",
            "\n",
            "Grammar as a foreign language.\n",
            "\n",
            "\n",
            "In Advances in Neural Information Processing Systems, 2015.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[38]\n",
            "\n",
            "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
            "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n",
            "\n",
            "\n",
            "Google’s neural machine translation system: Bridging the gap between\n",
            "human and machine translation.\n",
            "\n",
            "\n",
            "arXiv preprint arXiv:1609.08144, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[39]\n",
            "\n",
            "Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.\n",
            "\n",
            "\n",
            "Deep recurrent models with fast-forward connections for neural\n",
            "machine translation.\n",
            "\n",
            "\n",
            "CoRR, abs/1606.04199, 2016.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[40]\n",
            "\n",
            "Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.\n",
            "\n",
            "\n",
            "Fast and accurate shift-reduce constituent parsing.\n",
            "\n",
            "\n",
            "In Proceedings of the 51st Annual Meeting of the ACL (Volume 1:\n",
            "Long Papers), pages 434–443. ACL, August 2013.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Attention Visualizations\n",
            "\n",
            "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making…more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generated  on Tue Apr 30 16:03:52 2024 by LaTeXML\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "shapes\n",
            "\n",
            "\n",
            "B-.05emi-.025em b-.08em\n",
            "    T-.1667em.7exE-.125emX\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "GPT (Generative Pre-trained Transformer) – A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions\n",
            "    Gokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y, Gautam Srivastava, Praveen Kumar Reddy Maddikunta, Deepti Raj G, Rutvij H Jhaveri, Prabadevi B, Weizheng Wang, Athanasios V. Vasilakos, and Thippa Reddy Gadekallu\n",
            "\n",
            "Gokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y, Praveen Kumar Reddy Maddikunta, Deepti Raj G, Prabadevi B are with the School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu- 632014, India (Emails: { gokul.yenduri, ramalingam.m, chemmalarselvi.g, supriya.d, praveenkumarreddy, deeptiraj.g2020, prabadevi.b }@vit.ac.in)\n",
            "\n",
            "Gautam Srivastava is with the Dept. of Math and Computer Science, Brandon University, Canada, and the Research Centre for Interneural Computing, China Medical University, Taichung, Taiwan as well as Dept. of Computer Science and Math, Lebanese American University, Beirut, Lebanon (email: srivastavag@brandonu.ca)\n",
            "\n",
            "Rutvij H Jhaveri is with the Department of Computer Science and Engineering, School of Technology, Pandit Deendayal Energy University, India, (Email: rutvij.jhaveri@sot.pdpu.ac.in).\n",
            "\n",
            "Weizheng Wang is with the Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China, (E-mail: weizheng.wang@ieee.org).\n",
            "\n",
            "Athanasios V. Vasilakos is with the Center for AI Research (CAIR),University of Agder(UiA), Grimstad, Norway, (Email: thanos.vasilakos@uia.no).\n",
            "\n",
            "Thippa Reddy Gadekallu is with the School of Information Technology and Engineering, Vellore Institute of Technology, Vellore 632014, India, Lovely Professional University, Phagwara, India, Department of Electrical and Computer Engineering, Lebanese American University, Byblos, Lebanon, Jiaxing University , Jiaxing 314001, China, Zhongda Group, China, 314312 (E-mail: thippareddy@ieee.org).\n",
            "    June 14, 2024\n",
            "==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The Generative Pre-trained Transformer (GPT) represent a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, enabling technologies, their impact on various applications, emerging challenges, and potential solutions.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Generative Pre-trained Transformer, Natural language processing, Artificial Intelligence\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    \n",
            "    List of key acronyms only if it is repeated \n",
            "    \n",
            "    \n",
            "        Acronyms     Description\n",
            "\n",
            "        \n",
            "        AI     Artificial Intelligence \n",
            " \n",
            "        AR     Augmented Reality \n",
            "        \n",
            " \n",
            "        BERT     Bidirectional Encoder Representations from Transformers \n",
            "        \n",
            "\n",
            "        BGN     Boneh–Goh–Nissim\n",
            "        \n",
            " \n",
            "        CNN     ConvolutionalNeural Network \n",
            "        \n",
            " \n",
            "        DAP     Data Access Point \n",
            " \n",
            "        DLT     Decentralized Ledger Technology \n",
            " \n",
            "        DL     Deep Learning \n",
            "        \n",
            " \n",
            "        DRL     Deep Reinforcement Learning \n",
            "        \n",
            " \n",
            "        DR     Demand response \n",
            "        \n",
            " \n",
            "        EC     Edge Computing \n",
            "        \n",
            " \n",
            "        EU     End User \n",
            "        \n",
            " \n",
            "        EAPs     Energy Access Points \n",
            "        \n",
            " \n",
            "        5G     Fifth-Generation \n",
            " \n",
            "        \n",
            "        4G     Fourth-Generation \n",
            " \n",
            "        GPT     Generative Pre-trained Transformer \n",
            " \n",
            "        GPU     Graphics Processing Unit \n",
            " HPC     High Performance Computing \n",
            "        \n",
            " \n",
            "        HCI     Human Computer Interaction \n",
            " \n",
            "        IoT     Internet of Things \n",
            "        \n",
            " \n",
            "        ML     Machine Learning \n",
            "        \n",
            " \n",
            "        NLP     Natural Language Processing \n",
            " \n",
            "        NPC     Non Playable Character \n",
            "        \n",
            " \n",
            "        PLM     Pre-trained Language Models \n",
            "        \n",
            " \n",
            "        PTM     Pre-Trained Models \n",
            "        \n",
            " \n",
            "        RNN     Recurrent Neural Network \n",
            "        \n",
            " \n",
            "        6G     Sixth-Generation \n",
            " \n",
            "        TL     Transfer Learning \n",
            "        \n",
            " \n",
            "        VU     Virtual Reality \n",
            "        \n",
            " \n",
            "        \n",
            "    \n",
            " \n",
            "\n",
            "\n",
            "§ INTRODUCTION\n",
            " \n",
            "Language is the cornerstone of human communication and plays a vital role in shaping our interactions with the world. With the advent of NLP, it has revolutionized the way we interact with machines. NLP has become a game-changer in the world of communication, enabling humans to interact with machines in a more natural way. The evolution of NLP has been fueled by the exponential growth of textual data in the internet. Over the years, NLP has witnessed a significant transformation from simple rule-based systems to complex deep learning-based models. Despite the advances, natural language understanding and generation have long been a challenging problem in the field of NLP, largely due to the complex nature of human language. However, recent advancements have paved the way for the new approaches to tackle these challenges. One such breakthrough in NLP, is the development of the GPT <cit.>. GPT became famous after the launch of ChatGPT by OpenAI, a research company <cit.> that focuses on developing AI technologies. GPT is a deep learning model that is pre-trained on large corpora of text data and can be fine-tuned for specific tasks like language generation, sentiment analysis, language modelling, machine translation, and text classification. The transformer architecture used in GPT is a significant advancement over previous approaches to NLP, such as RNN and CNN. It uses a self-attention mechanism to allow the model to consider the context of the entire sentence when generating the next word, which improved the model's ability to understand and generate language. The decoder is responsible for generating the output text based on the input representation <cit.>.\n",
            "\n",
            "GPT can perform a wide range of tasks in NLP. One of its key strengths is in natural language understanding (NLU), where it can analyze and comprehend the meaning of text, including identifying entities and relationships in sentences. It's also proficient in natural language generation (NLG), which means it can create text output, such as writing creative content or answering questions in a comprehensive and informative way. Alternatively, GPT is also code generator, where it can write programming code in various languages, such as Python or JavaScript. GPT can also be utilized for question answering, which means it can provide summaries of factual topics or create stories based on the input text. Additionally, GPT can summarize a piece of text, such as providing a brief overview of a news article or research paper, and it can be used for translation, which makes it possible to translate text from one language to another. Overall, GPT's ability to perform a wide range of NLP tasks with high accuracy and precision, makes it an invaluable tool for various industries, including finance, healthcare, marketing, and more. As NLP technology continues to advance, we can expect GPT and other language models to become even more sophisticated and powerful, enabling us to communicate with machines more naturally and effectively.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Motivation\n",
            "\n",
            "\n",
            "GPT has become a transformative technology in the field of NLP, enabling the rapid development and growth of a wide range of industries and applications. Despite its wide adoption and numerous potential applications, there is still much to be explored and understood about GPT's capabilities. Although there are studies on GPT in the literature related to academia and libraries <cit.>, education <cit.>,\n",
            "GPT models<cit.>, banking and corporate communication <cit.>, advancements in chatGPT and its version <cit.>, and on generative AI's <cit.>, no existing reviews are dedicated to providing a comprehensive survey on GPT. Therefore, there is a need for a comprehensive review that focuses on GPT's architecture, enabling technologies, potential applications, emerging challenges, interesting projects and future directions. These limitations motivated us to conduct this review. Hence, this review will not only help researchers and practitioners in this field to gain a better understanding of GPT but also provide valuable insights into its potential applications and major limitations when conducting the research.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Related Surveys and Contributions\n",
            "\n",
            "\n",
            "\n",
            "The GPT model is a type of DL model that uses self-supervised learning to pre-train massive amounts of text data, enabling it to generate high-quality language output. The recent advancements in GPT model research can be attributed to the continual improvement of its architecture, increased availability of computing power, and the development of novel techniques to fine-tune the model for specific tasks. These advancements have led to the creation of larger and more powerful GPT models, enabling them to perform a wider range of NLP tasks with unprecedented accuracy and fluency. These GPT models have demonstrated great potential in transforming various industries like healthcare <cit.>, customer service <cit.>, financial industry <cit.> and so on. These applications are enabled by the generation of high-quality and diverse data like large-scale corpora of text data with different fast-growing enabling technologies<cit.>. There are numerous survey papers published to provide a comprehensive overview of the latest developments in GPT models, insights into the different architectures, training methods, evaluation metrics, and highlight the challenges and future directions of this field. This literature survey aims to review and analyze the key findings and contributions of the most recent survey papers published on GPT models, to provide a comprehensive and up-to-date understanding of the state-of-the-art in this exciting and rapidly evolving field.\n",
            "\n",
            "\n",
            "\n",
            "Lund et al. <cit.> presents the potential effects of AI and GPT models, specifically ChatGPT, on academia and libraries. They discussed the capabilities of ChatGPT in generating human-like responses and its potential applications. They examine how AI-powered chatbots and virtual assistants based on GPT models can enhance student learning experiences, assist with research tasks, and improve library services. They also address concerns regarding data privacy, biases, and the need for ethical guidelines. Overall, this survey paper highlighted the transformative potential of AI and GPT models while emphasizing the importance of responsible deployment and human oversight.\n",
            "\n",
            "\n",
            "Kasneci et al. <cit.> have reviewed the potential opportunities and challenges of using large language models, specifically ChatGPT, for educational purposes. They highlighted the benefits and limitations of using such models by discussing their implications for teaching and learning. In addition, a defined strategy and pedagogical approach with a heavy focus on critical thinking and fact-checking are required while using such large language models in educational institution. Thus, they concluded the paper by highlighting the key technical challenges like copyright issues, biased content creation, user dependency, privacy and security, and high-cost language models when such language models are used in the educational sector.\n",
            "\n",
            "Qiu et al. <cit.> presented an exhaustive survey of various types of GPT models by detailing their working architecture. They discussed the evolution of pre-training methods for NLP, from language modelling to TL and pre-training on large-scale corpora. It also reviews the different types of GPT models, including word embeddings, contextual embeddings, and transformer-based models, and discusses their applications in various NLP tasks such as text classification, Named Entity Recognition, and machine translation. They highlighted the benefits of GPT's models for the NLP domain, such as its ability to improve model performance with limited annotated data, reduce the need for task-specific feature engineering, and enable TL across multiple tasks. They discussed the major challenges and limitations of PTMs, such as the risk of bias and the lack of interpretability.\n",
            "\n",
            "George et al. <cit.> studied the potential impact of GPT-4, the next iteration of GPT models, on communication within corporate environments. They discussed how GPT-4 can revolutionize business communication by enabling more efficient and effective interactions. They explore various applications of GPT-4 in corporate settings, such as automating customer support through AI chatbots that can provide personalized responses and resolve queries in real-time. They also addressed potential challenges and considerations associated with implementing GPT-4 in corporate settings. These include concerns about data security, privacy, and the need for human oversight to ensure accurate and ethical communication. Thus, they concluded by emphasizing the transformative potential of GPT-4 in revolutionizing business communication to fully harness the benefits of GPT-4 while addressing any potential risks or limitations.\n",
            "\n",
            "\n",
            "\n",
            "Zhang et al. <cit.> presents an extensive survey of generative AI and evaluates the capabilities of the ChatGPT models, particularly from GPT-4 to GPT-5. They provided an overview of generative AI, highlighting its significance in generating realistic and creative outputs across various domains and evaluate their advancements over previous iterations. They analyze the architectural improvements, model size, training techniques, and dataset considerations employed in GPT-4 and GPT-5. In addition to it, they presented a comprehensive comparison of ChatGPT with other state-of-the-art generative AI models, such as OpenAI's DALL-E and CLIP. Finally, they concluded with valuable insights into the capabilities and limitations of these models and highlights the broader landscape of generative AI.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Zaib et al. <cit.> provides a survey on the latest advancements in GPTS and PTMs for conversational AI applications. They focused on PLMs and their approaches while building dialogue-based systems. They also highlighted the potential use of transformer-based models such as BERT and GPT, which have demonstrated good performance in understanding NLP generation, and dialogue management. Thus, they concluded with the significant challenges in the field of developing conversational AI systems using PLMs and GPTs.\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tOrganization Chart of the survey.\n",
            "\t\n",
            "\n",
            "\n",
            "Thus, the comparison of existing surveys on GPT models highlighting the growing importance of these models in key areas of NLP and other related fields are discussed here. Hence, this is the first-of-its-kind survey that presents the extensive information, by comparing existing surveys with our survey and summarized in Table <ref>.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Comparison of this survey with the existing surveys\n",
            "\n",
            "!\n",
            "\n",
            "2*Ref.    \n",
            "  11l|Applications    \n",
            "  6l|Enabling Technologies    \n",
            "  2*Remarks \n",
            " 2-18\n",
            "    \n",
            "  1l|[origin=c]90 Education     \n",
            "  1l|[origin=c]90 Industry     \n",
            "  1l|[origin=c]90 Agriculture     \n",
            "  1l|[origin=c]90 Healthcare     \n",
            "  1l|[origin=c]90 Transport     \n",
            "  1l|[origin=c]90 E-Commerce      \n",
            "  1l|[origin=c]90 Entertainment     \n",
            "  1l|[origin=c]90 Lifestyle     \n",
            "  1l|[origin=c]90 Gaming     \n",
            "  1l|[origin=c]90 Marketing     \n",
            "  [origin=c]90 Finance     \n",
            "  1l|[origin=c]90 Big Data      \n",
            "  1l|[origin=c]90 AI     \n",
            "  1l|[origin=c]90 Cloud Computing      \n",
            "  1l|[origin=c]90 Edge Computing     \n",
            "  1l|[origin=c]90 5G and Beyond     \n",
            "  [origin=c]90 HCI     \n",
            "   \n",
            " <cit.>    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  X     They conducted a survey discussing capabilities of ChatGPT on academia and\n",
            "libraries. Although, Key challenges of Chatgpt were highlighted, practical implementation challenges and research directions were missing.\n",
            "   \n",
            " \n",
            "<cit.>    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  Reviewed the potential opportunities\n",
            "and challenges of using large language models, specifically\n",
            "ChatGPT, for educational purposes. Thus, evolution of GPT and their preliminaries were not discussed in this survey paper. \n",
            " \n",
            "   <cit.>    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  They presented an exhaustive survey of various\n",
            "types of GPT models by detailing their working architecture with benefits and limitations of GPTs. However,  \n",
            " \n",
            "\n",
            "<cit.>    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  Studied the potential impact of GPT4 in business communication and explore various applications of GPT-4 in corporate settings by highlighting any potential\n",
            "risks or limitations. But, how GPT architecture can be used in corporate is not found with key enabling technologies.\n",
            " \n",
            " <cit.>    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|    \n",
            "      \n",
            "  1l|X    \n",
            "  1l|    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  Analyzed the architectural\n",
            "improvements, model size, training techniques, and dataset\n",
            "considerations employed in GPT-4 and GPT-5. However, preliminary details are unedr explored. \n",
            " <cit.>    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  1l|X    \n",
            "  X    \n",
            "  Recent trends in   language models, applications of dialogue management, question answering NLP   tasks were discussed along with challenges and future scope of GPT. Although it covered most of the technical aspects, the integration challenges to overcomeare not presented. \n",
            " Our Survey   Paper    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "      \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "  1l|    \n",
            "      \n",
            "  Presents the evolution of GPT models, GPT architecture and its detailed working, key enabling technologies, significant advancements of GPT models and their potential benefits in real-life applications, GPT projects, lessons learnt, open challenges and future research directions.\n",
            "   \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Systematic Literature Survey\n",
            "\n",
            "\n",
            "In this review of GPT, we conducted a thorough literature review using various reputable sources. Our search was primarily focused on peer-reviewed journals, and high-quality articles from reputed national and international conferences, seminars, books, symposiums, and journals. To ensure the credibility of our sources, we referred to well-known archives such as Google Scholar and arXiv, and publications from top databases like IEEE, Springer, Elsevier, Taylor & Francis, and Wiley. To identify relevant GPT references and publications, we used keywords such as NLP­GPT, GPT architecture, DL for GPT, Pre­training GPT, Fine-tuning AI GPT and GPT vertical applications. We then screened all the retrieved articles based on their titles, excluding any papers with poor-quality material. Next, we reviewed the abstracts of the remaining articles to determine their contributions. In the final step of our literature review, we extracted the necessary data for our analysis. By following these phases, we ensured that our study was based on high-quality and credible sources.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Paper Organization\n",
            "\n",
            "\n",
            "The structure of this paper's organization is illustrated in Fig. <ref>. Section 2 presents the preliminaries of GPT models such as the definition of GPT, its evolution and architecture, how it works and presents the comparison of various GPT models. Section 3 discusses the key enabling technologies for GPT models. The impact of GPT models in various applications are presented in Section 4. In Section 5, we highlighted some of the exciting GPT projects that are currently developed. Section 6 includes open issues, other technical challenges and future research directions in the field of GPT. Finally, we conclude the paper in Section 7, by summarizing the key findings and contributions of this study. The list of key acronyms are listed in Table <ref>. \n",
            "\n",
            "\n",
            "\n",
            "§ PRELIMINARIES\n",
            "\n",
            "\n",
            "In this section, the evolution of GPT models, the architecture of GPT, working process of GPT models are discussed and finally, different versions of GPT models are compared.\n",
            "\n",
            "\n",
            " §.§ Generative Pre-trained Transformer\n",
            "\n",
            "The GPT model produces enormous quantities of pertinent and complicated machine-generated text from a small amount of text as input. GPT models can be identified as a language model that mimics human text using a DL techniques and it acts as an autoregressive model in which the present value is based on the previous value <cit.>.\n",
            "\n",
            "\n",
            "  §.§.§ Definition 1\n",
            "\n",
            "GPTs are language models pre-trained on vast quantities of textual data and can perform a wide range of language-related tasks <cit.>.\n",
            "\n",
            "\n",
            "  §.§.§ Definition 2\n",
            "\n",
            "A GPT is a language model relying on DL that can generate human-like texts based on a given text-based input.  <cit.>. \n",
            "\n",
            "\n",
            "  §.§.§ Definition 3\n",
            "\n",
            "GPT is a language model developed by OpenAI to help give systems intelligence and is used in such projects as ChatGPT <cit.>.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Evolution of GPT\n",
            "\n",
            "GPT models have evolved through multiple changes and breakthroughs in NLP technology. These are some significant turning points in the growth of the GPT model:\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tGPT Road Map.\n",
            "\t\n",
            "\n",
            "\n",
            "Before GPT, NLP models have been trained on large amounts of annotated data that is related to a specific task. This had a significant drawback because it was difficult to access the quantity of labelled data required to train the model precisely. The NLP models were unable to complete tasks outside of their training set since they were restricted to a particluar set of data. To get around these restrictions, OpenAI offered a Generative Language Model called GPT-1 that was created using unlabeled data and then given to users to fine-tune to carry out subsequent tasks like sentiment analysis, categorization, and question-answering <cit.>. This indicates that the model attempts to produce an appropriate response based on input and that the data used to train the model is not labelled <cit.>. Fig. <ref> shows the timeline of the evolution of several pre-trained models from Eliza, which was created in 1960, to the more current 2022-ChatGPT.\n",
            " \n",
            "GPT-1 was the first ever model that could read the text and respond to queries <cit.>. OpenAI released GPT-1 in 2018. GPT-1 was a major move forward in AI development because it enabled computers to comprehend textual material in a more natural manner than before. This generative language model was able to learn a wide variety of connections and gain immense knowledge on a varied corpus of contiguous text and lengthy stretches <cit.>. This happened after being trained on a huge BooksCorpus dataset. In terms of design, GPT-1 employs a 12-layer decoder architecture transformer with a self-attention system for training. GPT-1's capacity to execute zero-shot performance on different tasks was one of its major success as a result of its pre-training. This ability demonstrated that generative language modelling can be used to generalize the model when combined with a successful pretraining idea. With TL as its foundation, GPT models evolved into a potent tool for performing NLP tasks with minimal fine-tuning <cit.>. It paved the way for other models to progress even more in generative pre-training using larger datasets and parameters.  <cit.>.\n",
            "\n",
            "To create a better language model later in 2019, OpenAI created a GPT-2 using a bigger dataset and more parameters. The model design and execution of GPT-2 are some of the key advancements <cit.>. With 1.5 billion parameters, it has 10 times the size of GPT-1 (117 million parameters), and it has 10 times as many parameters and data <cit.>. By using only the raw text as input and utilizing little to no training examples, it is effective in terms of resolving various language tasks related to translation, summarization, etc. Evaluation of GPT-2 on various downstream task datasets revealed that it excelled by substantially increasing accuracy in recognizing long-distance relationships and predicting sentences <cit.>. \n",
            "\n",
            "The most recent iteration of the GPT model is GPT-3. It is a sizable language prediction and production model created by OpenAI that can produce lengthy passages of the source text. GPT-3 eventually emerged as OpenAI's ground-breaking AI language software. Simply put, it is a piece of software that can create lines on its own that are so distinctive they almost sound like they were written by a human <cit.>. The GPT-3 program is presently accessible with limited access via a cloud-based API, and access is required to investigate the utility. Since its debut, it has produced several interesting apps. Its capacity, which is about 175 billion parameters big and 100 times larger than GPT-2, is a key advantage. It is taught using a corpus of 500 billion words called \"Common Crawl\" that was gathered from a sizable content archive and the internet <cit.>. Its other noteworthy and unexpected capability is its ability to carry out basic mathematical operations, write bits of code, and carry out clever tasks. As a result, NLP models can help businesses by responding more quickly to requests and accurately keeping best practices while minimizing human mistakes <cit.>. Due to its intricacy and size, many academics and writers have referred to it as the ultimate black-box AI method. Due to the high cost and inconvenience of performing inference, as well as the billion-parameter size that makes it resource-intensive, it is difficult to put into practice in jobs <cit.>. \n",
            "\n",
            "GPT-4 was named as the successor of GPT-3. In the meantime, several AI models built on GPT-3.5, an updated version of GPT-3, have been surreptitiously released by OpenAI <cit.>. GPT-3.5 was trained on a mixture of text and code. From the vast amounts of data collected from the web, which includes tens and thousand of Wikipedia entries, social media posts, and news items, GPT 3.5 learned the relations between words, sentences, and various components. It was utilized by OpenAI to develop several systems that have been tailored to complete particular jobs <cit.>. It collected vast amounts of data from the web, including tens of thousands of Wikipedia entries, posts on social media, and news items, and used that information to learn the relationships between sentences, words, and word components <cit.>.\n",
            "\n",
            "The latest version of the GPT model by OpenAI is GPT-4 which is a multimodal big language model. It was launched on March 14, 2023, and is now accessible to the general public through ChatGPT Plus in a constrained capacity. A waitlist is required to gain access to the business API <cit.>. Using both public data and \"data licensed from third-party providers,\" GPT-4 was pre-trained to anticipate the next coin as a transformer. It was then adjusted with reinforcement learning based on input from humans and AI for human alignment and policy conformance. In comparison to GPT-3, which had context windows of only 4096 and 2049 tokens, respectively, the group created two variants of GPT-4 with context windows of 8192 and 32768 tokens.\n",
            "\n",
            "\n",
            " §.§ GPT model's architecture\n",
            "\n",
            "\n",
            "\n",
            "GPT models are based on neural networks that are used for NLP tasks, such as language modelling, text classification, and text generation.\n",
            "\n",
            "The GPT model's architecture is based on the transformer model <cit.>. The Transformer model uses self-attention mechanisms to process input sequences of variable length, making it well-suited for NLP tasks. GPT simplifies the architecture by substituting encoder-decoder blocks with decoder blocks. GPT model takes the transformer model and pre-trains it on large amounts of text data using unsupervised learning techniques. The pre-training process involves predicting the next word in a sequence given the previous words, a task known as language modelling. This pre-training process enables the model to learn representations of natural language that can be fine-tuned for specific downstream tasks <cit.>. The following are the components of the GPT architecture.\n",
            "\n",
            "\n",
            "  \n",
            "\n",
            "   \n",
            "  * Input Embedding layer: The embedding layer maps the input tokens (e.g., words or subwords) to continuous vector representations, which can be processed by the transformer blocks<cit.>.\n",
            "   \n",
            "  * Positional encoding: Since the transformer blocks do not have any notion of order or position, positional encoding is added to the input embeddings to provide information about the relative position of tokens.\n",
            "Masking: In some cases, masking may be necessary to mask certain input tokens (e.g., in language modelling tasks, the model should only use tokens that come before the target token).\n",
            "Transformer blocks: GPT models are based on the transformer architecture. It is designed for NLP tasks and has been widely used in applications such as machine translation, text classification, and text generation. Transformers allow the model to focus on different areas of the input while processing <cit.>.\n",
            "   \n",
            "  * Linear and Softmax Functions:\n",
            "In the GPT architecture, the softmax function is commonly used for classification tasks. The softmax function is applied to the output of the final layer of the model. It generates a probability distribution over a set of output classes. The output of the final layer is specifically converted into a set of logits before being normalized with the softmax function. The normalized values obtained from the model can be interpreted as the likelihood or probability that a particular input belongs to each of the output classes.\n",
            "The query, key, and value vectors for each token in the input sequence are frequently calculated using linear functions in the attention mechanism. The output of the multi-head attention layer is transformed using them in the feedforward layers as well. The output layer also employs linear functions to forecast the following token in the sequence <cit.>.\n",
            "   \n",
            "  * Pre-training: Pre-training is a key component of the GPT architecture. In pre-training, the model is trained on a large amount of data in an unsupervised manner even before fine-tuning the model for specific tasks like classification and text generation.\n",
            "   \n",
            "  * Fine-tuning:\n",
            "Fine-tuning is the process of adapting a pre-trained neural network model, such as GPT, to a new task or dataset by further training the model on that task or dataset. Fine-tuning in GPT involves adjusting the parameters of the pre-trained model to optimize performance on a specific downstream task, such as text classification or text generation <cit.>.\n",
            "   \n",
            "  * Language modeling:\n",
            "Language modelling is a key task in the GPT architecture. In the case of GPT, the language modelling task is performed during the pre-training phase of the model. In pre-training, the model is trained based on a large amount of data using a language model objective. It is the task of predicting the next word in sequence based on the previous words. It allows the model to learn relationships between the words and their meaning in the training data <cit.>.\n",
            "   \n",
            "  * Unsupervised learning:\n",
            "Unsupervised learning is an ML algorithm which enables the model to learn form unlabelled data without any human intervention. GPT models use unsupervised learning in the pre-training phase to understand the relationships between the words and their context in the training data <cit.>. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Comparsion of different versions of GPT model\n",
            "\n",
            "!\n",
            "\n",
            "Model    \n",
            "  Tokens    \n",
            "  Size    \n",
            "  Parameters    \n",
            "  Dataset    \n",
            "  Year    \n",
            "  Features    \n",
            "  Input Type    \n",
            "  Drawbacks \n",
            " GPT-1    \n",
            "  -    \n",
            "  12-layer decoder    \n",
            "  117M parameters    \n",
            "  Books corpus    \n",
            "  2018    \n",
            "  Used   mostly for language modelling tasks and it is transformer based    \n",
            "  A sequence of tokens and words    \n",
            "  Limited   Capacity, Limited Data, Cannot perform complex tasks, Limited applications \n",
            " GPT-2    \n",
            "  -    \n",
            "  10 times the size of   GPT-1    \n",
            "  1.5B parameters    \n",
            "  Downstream task   datasets    \n",
            "  2019    \n",
            "  Text   generation capabilities are improved and a chance for misuse    \n",
            "  A sequence of tokens and words    \n",
            "  Limited   Control, Limited Data Diversity, Expensive computational requirements, Risk of   improper information \n",
            " GPT-3    \n",
            "  4096 and 2049 tokens    \n",
            "  100 times larger than   GPT-2    \n",
            "  175B parameters    \n",
            "  Common Crawl    \n",
            "  2020    \n",
            "  Good   NLP capabilities, language translation, summarization and generation of text    \n",
            "  A sequence of tokens and words and images and tables    \n",
            "  Limited   Control, Limited Data Diversity, Lack of explanation, Ethical concerns \n",
            " GPT-3.5    \n",
            "   maximum token limit of 4096 tokens    \n",
            "  96 layers    \n",
            "  similar or larger number of parameters like GPT-3    \n",
            "  -    \n",
            "  2022    \n",
            "  Improves user experience by delivering more precise and contextually relevant information    \n",
            "  The input type typically consists of text data    \n",
            "  Limited resources to train,Data Bias,Lack of Explainability,Limited Contextual Understanding,High Inference Latency \n",
            " GPT-4    \n",
            "  8192 and 32768 tokens    \n",
            "  -    \n",
            "  100T parameters    \n",
            "  -    \n",
            "  2023    \n",
            "  Creative   and technical writing tasks    \n",
            "  A sequence of tokens and words and images and tables    \n",
            "  - \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ How do GPT models work?\n",
            "\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tTransformer Architecture.\n",
            "\t\n",
            "\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tTransformer Architecture and Input Transformations for Fine-Tuning on Different Tasks.\n",
            "\t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tHow does GPT Work.\n",
            "\t\n",
            "\n",
            "\n",
            "GPT models work by using a transformer which is a neural network architecture that processes the input sequences of natural language text <cit.>. The GPT model uses unsupervised learning techniques to pre-train this transformer architecture on a significant amount of text input <cit.>. The model gains the ability to anticipate the subsequent word in a sequence based on the preceding words during pre-training. Language modelling is the process that enables a model to discover the statistical connections between words and their context in training data. Fig. <ref> shows the various stages of GPT operation. The first step entails supervised fine-tuning, the second step involves producing optimal responses to input, and the third step involves proximal policy optimization and reinforcement learning.\n",
            "\n",
            "The model can be fine-tuned for particular tasks, like text classification or text production, after pre-training. The model is trained on a smaller dataset that is unique to the work at hand during fine-tuning, and the model's parameters are changed to maximize performance on that task <cit.>. Fig. <ref> shows the general transformer architecture of GPT.\n",
            "\n",
            "When used for text creation, GPT models create text by anticipating the following word in a series based on the previously created words. Depending on how it has been modified, the model can produce text that is comparable to the input text or that adheres to a certain theme or style. Fig. <ref> projects the GPT model's transformer architecture and input transformations for fine-tuning different tasks.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§  Comparisons of GPT Versions\n",
            "\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tA comparison between unimodal, cross-modal, and multimodal modal GPTs.\n",
            "\t\n",
            "\n",
            "There are several versions of GPT models each having their own features and capabilities. Table <ref> presents a comparison of various versions of the GPT models. The table presents the following details like year of release of the GPT model, parameters, tokens generated, input type, features of each model, drawbacks of each model, and the size of each model.\n",
            "\n",
            "Generative AI (GAI) models are of different types like unimodal, cross-modal, and multimodal. The first type is unimodal which rely on a single type of input, such as text or images. The cross-modal, on the other hand, can process multiple types of inputs and relate them to each other. The Multimodal is the most complex type of AI as it can process and integrate information from multiple modalities, such as speech, text, images, and even physical interactions with the environment. GPT adopts only unimodal and multimodal types where ChatGPT is said to be unimodal, while GPT-4 is multimodal. Fig. <ref> is an illustration that distinguishes between unimodal, cross-modal, and multimodal Generative AI models.\n",
            "\n",
            "\n",
            "\n",
            "Overall, GPT models have demonstrated outstanding performance with NLP, by enhancing each iteration  and its predecessor' capabilities. Each model, however, also has its own restrictions and drawbacks, such as restricted output control, lack of diverse data, and ethical concerns. While selecting a GPT model for a particular task, researchers and developers should carefully take these factors into account <cit.>.\n",
            "\n",
            "In detail, this section describes the evolution, and architecture of GPT and compares the different versions and types of GPT.\n",
            "\n",
            "\n",
            "§ ENABLING TECHNOLOGIES\n",
            "\n",
            "GPT is a convergence of several technologies. It is enabled by the latest technologies like Big data, AI, Cloud Computing, EC, 5G and beyond networks, and HCI. In this section, we provide an overview of enabling technologies related to GPT. The major technologies that constitute the GPT models are depicted in Fig. <ref>.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Big Data\n",
            "\n",
            "\n",
            "Big data refers to the vast amounts of structured and unstructured data generated by businesses, individuals, and machines. The proliferation of new technologies, such as the IoT, has led to an explosion of data production from sources like social media, sensors, and transaction-based systems <cit.>.\n",
            "\n",
            "\n",
            "The emergence of big data has revolutionized the way organizations approach data analysis and decision-making. The training provided by this massive amount of data has yielded valuable insights for the use of advanced models like GPT in the field of NLP <cit.>. The GPT models utilize DL and big data for natural language generation, with GPT-4 being the most advanced model to date <cit.>.\n",
            "\n",
            "The training data for GPT models typically include millions or even trillions of data from a diverse range of sources, such as books, articles, websites, and social media platforms. This large and diverse training data helps GPT models capture the variations in language usage, making them more accurate and effective at NLP tasks. As a result, GPT models may be used for a variety of tasks, including question-answering, text summarization, and language translation <cit.>. Moreover, since GPT models can learn from a variety of data sources, they can be tuned for certain tasks and domains, making them very adaptive and versatile. GPT model has the potential to be utilized for a variety of activities, including the creation of images and videos in addition to its excellent language processing capabilities <cit.>. \n",
            "\n",
            "\n",
            "While big data presents numerous benefits to GPT, by enabling the models to get trained with large amounts of data, it also presents several challenges <cit.>. GPT is trained on a variety of data, large amounts of data, and also sensitive data. Thus, ensuring data accuracy, privacy concerns, and ethical use of data are some of the challenges that must be considered. However, with the continuous growth of available data, GPT models will become even more advanced and capable of performing increasingly complex tasks <cit.>. The future of big data as an enabling technology for GPT models is promising, with the potential to revolutionize the field of NLP. As technology continues to advance, organizations must prioritize ethical considerations and data accuracy to fully harness the benefits of big data and GPT models.\n",
            "\n",
            "\n",
            " §.§ Artificial Intelligence\n",
            "\n",
            "\n",
            "AI refers to the simulation of intelligent behaviour in machines that are programmed to learn from their experience to reason, understand natural language, and perceive their environment <cit.>. AI gives machines the ability to sense their surroundings, deal with what they see, handle issues, and take action to reach a particular objective. The importance and capability of AI is growing all the time. \n",
            "\n",
            "\n",
            "AI enables GPT models to allow machines to comprehend and react to human language. There are several ways in which AI can continue to help improve GPT and make it more powerful and effective in its language generation capabilities <cit.>.\n",
            " \n",
            "The following are the several ways through which AI can make GPT models more powerful:\n",
            "\n",
            "\n",
            "  * Fine tuning\n",
            "\n",
            "  * Dialogue generation\n",
            "\n",
            "  * Natural language understanding\n",
            "\n",
            "GPT's model performance on particular tasks can be enhanced by utilizing AI approaches. For instance, it can be trained on a large corpus of text from a particular field such as legal documents or medical literature to better grasp and produce language in that field <cit.>. Considering dialogue generation, AI techniques such as reinforcement learning and sequence-to-sequence models can be used to enable GPT generate more natural and engaging dialogue in conversational contexts. Similarly, AI techniques such as semantic parsing and named entity recognition can be used to help GPT better understand the meaning of language and the relationships between words and phrases. This can enable it generate more accurate and coherent language <cit.>.\n",
            "\n",
            "The development and enhancement of GPT model language production capabilities depend heavily on AI, and GPT's capabilities will continue to be growing by continuous research and development in AI.\n",
            "\n",
            "As GPT models become more advanced, there are growing concerns about the potential for them in reinforcing biases and propagate harmful or offensive content <cit.>. Some of these concerns also include bias which can lead to unintended discrimination and unfairness, lack of understanding of the context that can lead to misunderstandings or incorrect responses, poor data quality can lead to inaccurate or biased models, ethical concerns like privacy and autonomy <cit.>. AI models like GPT require significant amounts of computational power to train and run, which can have a significant environmental impact due to their high energy consumption <cit.>.\n",
            "\n",
            "Though AI has a great deal of promise, it's critical to be aware of the underlying issues and make efforts to fix them to ensure that it is utilized responsibly and morally for GPT.\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tEnabling technologies of GPT models.\n",
            "\t\n",
            "\n",
            "\n",
            "\n",
            " §.§ Cloud Computing\n",
            "\n",
            "Cloud computing refers to the on-demand availability of computer resources, such as storage, processing power, and applications, delivered over the internet <cit.>. The GPT model's successes are possible not only because of algorithmic evolution but also increased computational capabilities i.e. exponential growth in hardware (computational power, storage capacity), cloud computing, and related operational software <cit.>. The applications for cloud and EC working together such as natural language generation, image completion, or virtual simulations from wearable sensors see that the work is made more compute-intensive <cit.>.\n",
            "\n",
            "\n",
            "GPT models need a lot of computational power to analyze a lot of data, and cloud computing offers the scalability required to cope with demand spikes. Without worrying about the constraints of on-premises hardware, GPT models can rapidly and easily scale up or down as needed with cloud computing <cit.>. Cloud-based platforms like Amazon Web Services (AWS) or Google Cloud Platform (GCP) provide access to distributed computing resources that can be used to train GPT. Since cloud computing provides web-based solutions and thereby does not require the purchase and maintenance of costly hardware, it can be a cost-effective choice for a GPT model. By utilizing cloud computing, the GPT model can only pay for the computing resources it uses <cit.>. The other added advantage of cloud computing in GPT is, it gives GPT models the freedom to access computing resources whenever it wants, from any location in the world. This makes GPT models more accessible to users by enabling smooth operation across a variety of gadgets and platforms <cit.>. Cloud computing providers offer high security and compliance standards, which can protect the GPT model and its data from online dangers. Cloud service providers also possess the knowledge and tools necessary to effectively address security problems and stop data leaks. Cloud-based storage services, such as Amazon S3 or Google Cloud Storage, provide scalable and reliable storage for GPT's data.\n",
            "\n",
            "\n",
            "Despite the advantages of cloud computing where it can help GPT models to operate more efficiently, effectively, and securely, there are also a few technological aspects where it creates a drawback for GPT <cit.>. To function properly, the GPT model needs a sizable amount of computing power and data storage. These resources can be accessible online with cloud computing. As a result, continued operation of the GPT model requires a robust and dependable internet connection, and any breakdown in connectivity may result in delays or even data loss. There are some security concerns when storing sensitive data, such as personal information or trade secrets, in the cloud which can be risky if proper security measures are not in place <cit.>. While cloud computing can be more cost-effective than building and maintaining an in-house computing infrastructure, it can still be expensive for long-term use. It also suffers issues like performance variability, limited availability etc., \n",
            "\n",
            "\n",
            " §.§ Edge Computing\n",
            "\n",
            "The rapid growth of IoT, a large amount of data from several IoT devices, and also cloud services have necessitated the emergence of a concept called EC. EC is an open AI and distributed design with decentralized computational power. In EC, there is a lesser need for clients and servers to communicate over long distances, which lowers latency and bandwidth utilization.\n",
            "\n",
            "Instead of depending on centralized data centers, EC entails bringing computing capacity and data storage closer to the consumer <cit.>.\n",
            "\n",
            "\n",
            " In GPT, where there is a need for real-time data analysis, EC plays a major role in faster processing and better efficiency in producing good results<cit.>. GPT models are typically large and complex, requiring significant processing power to run. By deploying GPT models on the edge devices, closer to the source of data, latency can be reduced in replying to users who seek information through the GPT models by eliminating the need to move data back and forth from end devices to the cloud. Since EC maintains data near the periphery and away from centralized servers, it can offer improved security and more privacy protections in the case of the requests made by users through GPT <cit.>. GPT models utilize a lot of data for learning and thereby the cost of data transfer also increases with data volume. EC can aid in controlling data transfer expenses. EC can also help in lowering the amount of bandwidth by pre-processing the data even before transferring it to the cloud. Particularly when analyzing photos or videos, GPT models can produce a lot of data <cit.>. EC accelerators, such as graphics processing units (GPUs) and field-programmable gate arrays (FPGAs), can be used to speed up GPT model inference and training. These accelerators can be integrated into edge devices or edge servers, providing more efficient processing of GPT models.\n",
            "\n",
            "EC and GPT models make a great combination. Comparative to cloud data centres, edge devices may have constrained computation and storage capabilities <cit.>. This might limit the scope of GPT models that can be installed on edge devices in terms of size and complexity. Since GPT models handle large and varied data, EC can also increase security risks and data privacy concerns. Implementing EC in existing infrastructure can be difficult and require significant investment in hardware, software, and networking components. This can be a barrier for many organizations which are using the GPT model and EC <cit.>.\n",
            "\n",
            "\n",
            "\n",
            " §.§ 5G and beyond networks\n",
            "\n",
            "\n",
            "5G networks represent the latest generation of cellular networks that promise faster data speeds, lower latency, and the ability to connect a vast number of devices simultaneously <cit.>. 5G and beyond networks enable faster data transmission speeds than previous generations of cellular networks, which can help in training and deploying larger and more complex language models. This can result in faster training times and better performance. 5G and beyond networks can provide lower latency than previous generations of cellular networks, which can reduce the time required for communication between GPT and other devices, such as servers or other language models <cit.>. This can improve the real-time response of the GPT model for applications that require quick and accurate language processing. 5G and beyond networks offer improved connectivity options, such as increased capacity and more reliable connections, which can help in scaling up the deployment of the GPT model for large-scale language processing tasks. With the deployment of 5G and beyond networks, EC is becoming more prevalent. This means that a GPT model can potentially be deployed closer to the end-user, reducing the latency and improving the response time for applications that require real-time language processing <cit.>. Ultra-Reliable Low-Latency Communication (URLLC) is a key feature of 5G networks. In the context of GPT language models, URLLC can enable real-time and reliable communication between multiple devices, such as edge devices, cloud servers, and end-users <cit.>.\n",
            "\n",
            "\n",
            "Though 5G and beyond technology offers potential advantages to GPT models, it is also important to note that the actual impacts of this technology may change depending on how it's implemented and used. 5G enables the access to uncontrolled access to the Internet,it may attract cybersecurity risks and privacy concerns <cit.>. Also, as GPT uses a large amount of data for analysis it could also cause privacy concerns. 5G and beyond networks in GPT models need high infrastructural requirements which is a costly process.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Human Computer Interaction\n",
            "\n",
            "\n",
            "HCI, which is multi-faceted, concentrates on the design of computer technology and, in particular, on how people and computers communicate with each other <cit.>.\n",
            "\n",
            "\n",
            "HCI has a greater influence over GPT models. As a language model, GPT is designed to interact with humans by generating natural language responses to input text. HCI research can help designers create more effective input mechanisms for the GPT model, such as natural language interfaces, that allow users to communicate more easily and accurately with the model <cit.>.\n",
            "HCI also helps in enhancing the GPT model's user experience by creating interfaces that are more intuitive and user-friendly. This makes it easy for the users to interact with GPT models and understand their responses <cit.>. HCI also estimates the performance of GPT models by evaluating their responses with real-time users and identifies the areas where the model needs improvement, thereby improving its reliability and accuracy. HCI enhances the usability of GPT models by reducing the time and effort required for the users to interact with <cit.>.\n",
            "\n",
            "\n",
            "While HCI can be incredibly helpful in improving the design and usability of GPT models, there are also some potential drawbacks to consider. If the research is not conducted with a diverse group of users, HCI can introduce biases into the design of the GPT model. HCI techniques can be expensive and time-consuming. As GPT models become more complex, it may become more difficult to design interfaces and input mechanisms that are both effective and user-friendly <cit.>. HCI may not always be able to provide the necessary insights or feedback to drive improvements in GPT models. There are also ethical concerns around the use of GPT models, including issues related to privacy, bias, and the potential misuse of the technology <cit.>.  As GPT models become more complex, it may become more difficult to design interfaces and input mechanisms that are both effective and user-friendly.\n",
            "\n",
            "\n",
            "§ IMPACT OF GPT MODELS ON VARIOUS APPLICATIONS\n",
            "\n",
            "GPTs have made significant progress, and its impact is being felt across various industries \n",
            " like education, healthcare, industry, agriculture, travel and transport, e-commerce, entertainment, lifestyle, gaming, marketing, and finance. This section provides valuable insights on the impact of the GPT models in the aforementioned applications as depicted in Fig. <ref>.\n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tThe impact of GPT models on various applications.\n",
            "\t\n",
            "\n",
            "\n",
            "\n",
            " §.§ Education\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "\n",
            "Education has been around for centuries, with traditional education being the most common form. Traditional education involves a teacher imparting knowledge to a group of students in a physical classroom. While successful, traditional education can be restrictive and inflexible, limiting students' ability to learn at their own pace and in their preferred style. It can also be limited by geography, as students need to be physically present in a classroom to learn. Technology has emerged as a solution to some of these issues, allowing for personalized learning experiences and more engaging, accessible resources. Online learning platforms, digital textbooks, and multimedia tools offer students access to a vast array of resources from anywhere in the world. Technology can also facilitate collaboration and communication among students and teachers, leading to a more dynamic and interactive learning experience. Distance learning, hybrid learning models, and online classes are examples of how technology can help break down the barriers of traditional education, making learning more flexible, efficient, and effective. By integrating technology into traditional education, we can create a more personalized and effective learning experience, benefiting students worldwide.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Education\n",
            "\n",
            "The field of education is constantly evolving, with advancements in technology playing a significant role in shaping the way we learn and teach. One such technology that has the potential to transform the education industry is GPT. As a large language model trained on a vast amount of data, GPT can generate human-like text that is coherent and informative, making it a valuable tool in developing educational content such as textbooks, study guides, and course materials. Furthermore, GPT can be used to analyze and summarize complex text, which can help educators and students save time and increase comprehension. With its ability to support NLP applications and create intelligent tutoring systems, GPT has the potential to revolutionize the way we learn and teach. In this context, following section will explore the different ways in which GPT can contribute to the education industry and transform the future of learning.\n",
            "\n",
            "\n",
            "    \n",
            "  * Intelligent Tutoring: Intelligent tutoring is a teaching approach that uses AI and ML to provide personalized and adaptive instruction. It analyzes student performance data, understands their strengths and weaknesses, and generates customized learning paths. It provides immediate feedback, personalized guidance, and remedial support. It is effective in improving learning outcomes, increasing student engagement, and reducing learning time. With advanced natural language processing capabilities, GPT can enhance the personalized and adaptive instruction provided by intelligent tutoring systems. It can analyze natural language input from students, enabling intelligent tutoring systems to better understand and respond to their queries, needs, and preferences. It can also generate personalized feedback and assessment based on the individual learning progress of each student, helping them to identify and address their knowledge gaps and improve their performance. GPT can also analyze student performance data and generate adaptive learning paths that provide customized instruction and remediation, ensuring that each student learns at their own pace and achieves their learning objectives. Additionally, it can create interactive dialogue systems that simulate natural conversations between students and virtual tutors, making learning more engaging, interactive, and personalized <cit.>. The authors in <cit.> have identified that GPT-4 model outperforms general-purpose GPT-3.5 model as well as GPTs (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B) specailly trained on medical data. The authors have tested GPT-4 models' ability to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case. \n",
            "\n",
            "\n",
            "    \n",
            "    \n",
            "  * Learning assistance and material development: Learning materials are critical in education as they provide a structured way for students to acquire knowledge and skills. They can be tailored to meet the needs of diverse learners and make learning more engaging and effective, supporting teachers to create a more dynamic and interactive learning environment. GPT can contribute to creating learning materials by automating content generation, providing multilingual content creation, language correction, personalized content creation, conducting topic research, and generating assessments. It saves time and effort for educators and publishers, improves the accuracy and readability of material, and makes learning more engaging and effective. GPT can generate high-quality content such as summaries, quizzes, and lesson plans based on specific learning objectives, making learning accessible to a wider audience. It can analyze written content and provide suggestions to improve grammar, punctuation, and readability. GPT can also assist in research writing by suggesting ideas for structure, rephrasing and organizing content, and identifying gaps in research <cit.>. Moreover, GPT can also provide personalized feedback based on individual learning progress, enhancing the development of more comprehensive and informative learning materials.\n",
            "     \n",
            "    \n",
            "  * Automated Assessments: \n",
            "    Automated assessment in education uses technology to evaluate students' learning outcomes, providing immediate feedback and reducing potential bias in grading. It can also help teachers identify areas where students may need additional support, enabling them to tailor their teaching methods to better meet individual needs. GPT with its advanced natural language processing skills, can help in automated assessment by analyzing and grading student responses to various types of assessment questions, including essays and short answer questions. It can also provide feedback to students <cit.>, such as highlighting areas for improvement and suggesting further reading or resources. GPT's natural language processing capabilities can help to identify the meaning and context of students' responses, making automated assessment more accurate and effective. Additionally, GPT can generate personalized learning materials and exercises based on students' assessment results, supporting educators to create more tailored and effective learning experiences. The authors in <cit.> have used Chat GPT in evaluating the students' assignments such as quiz style questions, and also in generating relevant practice problems to improve content retention and understanding. The results were promising in the classroom. The authors believe that Chat GPT has the significant ability in reducing the load of instructor without compromising students' learning outcomes.\n",
            "    \n",
            "    \n",
            "  * Fostering Creativity: Creativity thinking plays a vital role in education by encouraging students to think beyond traditional boundaries and develop innovative solutions to complex problems. It helps students to approach learning with an open mind and a willingness to explore new ideas, leading to greater engagement and motivation. GPT's ability to generate human-like responses and creative writing can aid in improving creativity. It can help improve creativity by generating new and innovative ideas based on vast amounts of data and information. By analyzing patterns in language and identifying connections between different concepts, GPT can suggest novel approaches to teaching and learning. Additionally, GPT can also generate creative prompts or challenges for students, encouraging them to think outside the box and approach problems in unique ways <cit.>. GPT can also analyze and evaluate students' creative work, providing feedback and suggestions for improvement. So, GPT can be a valuable tool for promoting and enhancing creativity among students and faculty members.\n",
            "\n",
            "      \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "\n",
            "\n",
            "There are several advantages to incorporating GPTs in education, but it is essential to acknowledge the potential limitations. While GPTs can quickly generate information, they may impede students' critical thinking and problem-solving skills. Furthermore, learners who benefit from personal interaction with instructors may find the lack of human involvement disadvantageous. GPTs rely on statistical patterns, so they cannot provide a comprehensive understanding of the material being taught <cit.>. Privacy concerns arise when using sensitive student data in GPTs for educational purposes. Additionally, since GPTs cannot provide citations, it is challenging to identify the source of information generated. The cost of maintaining GPT may be prohibitive for schools and educational institutions with limited resources. Finally, distinguishing between reliable and unreliable information generated by GPTs can be difficult, so it is necessary to have human oversight to ensure data accuracy and regulate access.\n",
            " \n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPT offers numerous advantages in the education sector, including personalized and adaptive instruction, automated assessment, creative writing support, and research writing assistance. They have the potential to revolutionize teaching by creating lesson plans and activities, responding to natural language queries, and integrating multiple digital applications. However, there are also challenges to consider, such as the potential negative impact on critical thinking and problem-solving skills, lack of human interaction, data security and privacy concerns, inability to provide full comprehension, lack of citations or sources, high cost of maintenance, and potential for producing unreliable information. Further research is needed to explore human-computer interaction and user interface design to integrate GPT into educational workflows while ensuring that the information they provide is accurate and reliable.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Healthcare\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Before technology became widespread in healthcare, healthcare services were primarily delivered through face-to-face interactions between healthcare professionals and patients. Traditional healthcare faced several challenges, including limited medical instruments, paper-based health records, patients receiving care mostly in hospitals or clinics, physical travel requirements to receive medical attention, and limited medical research. Despite these challenges, traditional healthcare still provided valuable medical services to patients. However, with the introduction of technology, healthcare has become more efficient, accessible, and personalized, resulting in improved patient outcomes and better overall healthcare services. Technology has become an essential aspect of society, as reflected in the significant investments made in this sector. Despite the advancements in technology, the healthcare industry still faces various new challenges, including access to healthcare, high costs, personalized medicine, data privacy and security concerns, and an aging population. However, technology has the potential to address these challenges and improve the efficiency of healthcare services.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in healthcare\n",
            "\n",
            "Recent years have seen significant advancements in technology, including in the healthcare industry. Biotechnology, medical devices, and pharmaceuticals have undergone transformations through the use of cutting-edge technologies like DL <cit.> and ML <cit.>. Currently, the healthcare sector is utilizing various forms of AI techniques for medical research and providing medical services. One such technique is the GPT features of NLP, which hold immense potential for the healthcare industry. GPT can help to overcome several challenges in healthcare in multiple ways. For instance, it can be used to develop intelligent systems that assist doctors in making accurate diagnoses and providing clinical assistance <cit.> <cit.>. GPT can also analyze large volumes of medical data and generate reports. Furthermore, it has potential applications in drug discovery <cit.> <cit.>, personalized medicine, patient diagnosis, medical image analysis, analyzing electronic health records, clinical decision support systems, and disease prediction.\n",
            "\n",
            "    \n",
            "  * Drug Discovery: Recent AI and machine learning techniques <cit.> <cit.> are having the potential to contribute to the growth and development of drug discovery. GPTs are capable of learning new patterns and relationships <cit.> in the dataset they were trained on. This capability can be used in drug discovery to aid in the identification and design of potential new drugs with desired properties<cit.>. One of the key challenges in drug discovery is finding compounds that can interact with specific parts of the body. GPT can help in this process by learning the patterns and relationships from large databases of known compounds <cit.>. GPT can be trained on large sets of chemical databases to analyze chemical reactions and their outcomes. This can help suggest potential combinations of new drugs using the analyzed data. These new drugs can also be analyzed using GPT to test their efficacy and toxicity. \n",
            "   \n",
            "    \n",
            "  * Diagnosis:\n",
            "    GPT can be used in medical diagnosis by analyzing patient data. It can help to analyze medical records and extract information such as patient demographics, symptoms, and medical history. This can help medical professionals provide effective patient care and improve outcomes. The recent release of GPT-4 has the ability to support multimodal information, allowing it to analyze images as input and produce text results as output <cit.>. It is recommended to use AI systems such as a GPT, as clinical support tools to assist medical professionals in diagnosing and treating patients, but they should not be relied upon as the sole source of medical advice or decision-making. GPT can also be used to identify rare diseases by analyzing patient's complete information. The authors in <cit.> have used a general-purpose GPT based on GPT-3 model for patient diagnosis and triage. The model has given a triage accuracy of 70% which was worse than a physician. But, in next subsequent weeks, the accuracy has improved to 92% which is close to the performance of a physician. In diagnosis, GPT-3 model has given 88% accuracy. For emergency cases, GPT-3 has given 75% accuracy whereas physician has given 94%. \n",
            "\n",
            "    \n",
            " \n",
            "  * Disease prediction: GPT has great potential in disease prediction <cit.>. By analyzing large amounts of medical data, including patient records, medical images, and clinical trials, these pre-trained language models can learn patterns and make predictions about the likelihood of a patient developing a particular disease. For instance, trained healthcare GPTs can be used to predict the occurrence of diseases such as diabetes, heart disease, and cancer by analyzing various parameters, including the patient’s medical history, age, family history, and lifestyle. It can also be used to predict the likelihood of a rare disease This helps in the early detection of high-risk patients so that medical personnel can take necessary measures and suitable medicines to reduce the risk of developing the disease. The medical practitioner and author in <cit.> have recommended using GPT-4 models’ ability of NLP in bariatric surgery.\n",
            "    \n",
            "    \n",
            "  * Personalized medicine:\n",
            "   The COVID-19 pandemic has highlighted that not all body systems are clinically similar. For instance, during the pandemic, medicines like Remdesivir and Tocilizumab have been effective for one category of patients but do not affect another category of patients with similar clinical metrics, as they progress from a mild or moderate level of infection to a severe stage <cit.>. This highlights the need for personalized medicine in today's world. GPT can be used to identify variable patterns of data to predict or classify hidden or unseen patterns, which can be used for exploratory data analysis. GPT provide the possibility of identifying personalized medicines <cit.> based on the clinical, genomic, and nutritional data of patients. The dietician and the author in <cit.> have observed that the utilization of Chat GPTs has significantly decreased obesity rates among patients by offering personalized recommendations regarding nutrition plans, exercise programs, and psychological support. This approach allows for the development of customized treatment plans that cater to the specific needs of individuals, leading to a more efficient method of treating obesity with the assistance of Chat GPT.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "While GPT is a powerful language model with numerous applications in healthcare, it is not without its challenges. The primary challenge is data bias. As GPT models are also learning models, the significant drawback of biasing is also applicable to GPT. GPT can be susceptible to bias. If the data used to train the model is biased, the model will learn from it and replicate the bias. This leads to incorrect treatment and predictions. Another challenge is the transparency of the model. GPT is complex to understand and interpret. This lack of transparency in technology can make doctors and medical personnel not believe in the predictions, which may result in a hesitancy to trust and adopt technology <cit.>. Another important concern is security and privacy issues. As it is a model to be trained on data, there is a huge amount of sensitive information about the patients to be used to improve the algorithm and its performance. This results in significant security and privacy concerns related to the use of GPT in healthcare. The final and important challenge is limited clinical validation. GPT are showing promising improvement in various fields of healthcare, such as drug discovery, and disease prediction. But still, their effectiveness and accuracy in medical research and clinical settings have yet to be validated. More research and clinical trials are required to prove that GPT can transform the medical industry with full trust.\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPT have the potential to revolutionize the healthcare industry by contributing to drug discovery, personalized medicine, clinical support in making decisions, diagnosis support, and disease prediction. This can be helpful for human beings to predict the disease in advance and treat it through proper medicine. However, there are significant challenges that are to be addressed, such as technology adoption, data bias, regulatory challenges, and security and privacy issues. It is so important to analyze and evaluate the benefits and risks of using GPT in healthcare and to continue to monitor their development and implementation.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Industry\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "An important economic transition from agriculture and handicrafts to large-scale industry and automated production was achieved by the industrial revolution. Efficiency and productivity were raised as a result of modern equipment, energy sources, and labour arrangements. New opportunities and challenges have been created as a result of the quick development of new technologies in both the workplace and other industries <cit.>. The utilization of big data is a well-known technology-driven trend. Nowadays, companies have access to enormous volumes of data that may be examined to uncover insightful information. Big data can help businesses make wise decisions and discover areas for development. AI is another innovation that is changing industries. AI systems have the ability to analyse complex data, automate procedures, and make wise conclusions <cit.>. This improves production by increasing its dependability, adaptability, and efficiency. The process of \"digitalization,\" which includes incorporating digital technologies into every element of business, is creating industries to become more flexible, efficient, and valuable. Businesses may automate tedious work, improve client experiences, and streamline operations by implementing digital solutions. In today's digitally-driven world, adopting technological advancements is essential for maintaining competitiveness and promoting growth.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Industry\n",
            "\n",
            "In industrial scenarios, GPT has the potential to be applied as a sustainability tool, assisting businesses in evaluating and enhancing their sustainability goals. Companies can improve supply chain tracking and query response by integrating pre-trained transformer models like ChatGPT with supply chain management platforms<cit.>. Additionally, GPTs can offer modifications to the production process that might increase efficiency <cit.>. GPT can also help users make knowledgeable decisions about how to use resources, allowing businesses to remain competitive while reducing their environmental effect. For example, the GPT-2 model has demonstrated efficacy in sentiment analysis, providing insightful data for numerous applications <cit.>.\n",
            "\n",
            "\n",
            "    \n",
            "  * Hospitality sector\n",
            "In the hospitality industry, hotels place a high focus on providing satisfying guest experiences. To ensure that every tourist is satisfied during their stay, this necessitates adapting to their requirements and preferences. Hotels may improve the guest experience in a number of ways by integrating GPT into their website or mobile application. Hotels may respond to consumer inquiries in a timely and precise manner by utilizing GPT <cit.>. Customers do not have to wait for human assistance when looking up information about facilities, booking procedures, or room availability. Customers' overall satisfaction with the hotel's services is increased as a result of the large reduction in client wait times. GPTs can also make it easier for visitors who speak multiple languages to communicate <cit.>. Hotels can offer a more inclusive and welcoming experience for visitors from other countries by removing linguistic obstacles. Hotels may provide their visitors with immersive and engaging experiences by combining GPT with AR technologies. For instance, customers can use their mobile devices to get AR guided tours of the hotel or nearby attractions, offering a distinctive and entertaining way to explore the surroundings and learn more about the hotel's amenities.GPTs integration into various aspects of the hospitality industry gives hotels the ability to deliver streamlined, tailored, and effective services, increasing client happiness and loyalty.\n",
            "\n",
            "  * Fashion: By providing highly customized user recommendations based on personal style, brand preferences, and particular clothing or accessory demands, collaborative filtering and AI algorithms have undoubtedly revolutionized the fashion business. The amount of personalization has been further increased in this context by the incorporation of GPT, dramatically altering the purchasing experience for customers <cit.>. Fashion platforms may analyse a significant quantity of user data, such as browsing history, purchasing behaviour, and style preferences, using the advanced capabilities of GPT to produce tailored recommendations. Fashion platforms can direct consumers towards clothing options that fit their desire for sustainable fashion by including eco-friendly fabric selections into the system. GPT improve users' general fashion knowledge and confidence while enabling users to keep up with the most recent trends. The image-text retrieval skills of GPT significantly improve visual search capability in fashion platforms <cit.>. Users may make more confident shopping decisions and minimize the need for returns by visualizing how various clothing items and accessories would appear on them without physically trying them on. The model may recommend the proper size for various brands and apparel products by taking into account a user's measurements, preferred fit styles, and historical data. The overall purchasing experience is enhanced and the frustration of wrong size is decreased. \n",
            "\n",
            "  * Sustainability:\n",
            "Sustainable development means addressing current demands without sacrificing the capacity of future generations to address their own needs. Goals for sustainable development can be attained by implementing GPTs in a variety of sectors, including manufacturing and corporate operations <cit.>. The models can estimate where energy saving measures would be most useful by analyzing past data and patterns to provide insights into energy usage, pinpoint problem areas, and recommend opportunities for improvement. GPTs can aid in identifying sustainability-related problems, creating plans and strategies to solve them, investigating brand-new sustainable activities, keeping track of advancements, and conducting routine reviews. Companies can choose activities that will have the biggest positive impact by grading tasks and actions according to their impact on sustainability <cit.>.\n",
            "The models can optimize supply chains for decreased carbon emissions, minimized waste, and improved resource efficiency by assessing elements including transportation routes, packaging materials, and supplier practises <cit.>. This results in more environmentally friendly production, distribution, and sourcing procedures.\n",
            "\n",
            "  \n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "There are many different industrial fields where GPT models can be applied; the three areas mentioned above are only a few. However, for optimal use, the industrial sector needs to be ready to adapt to a constantly changing environment. Public and corporate policies must be developed over the long term to promote the use of sustainable production techniques. For enterprises, deploying pre-trained GPT models can be a costly task. Continuous development and training are also required to accommodate new and evolving inquiries as client expectations change. Companies have to carefully assess the benefits and costs before implementing the GPT model because these continuing efforts raise the deployment cost <cit.>. For industries to fully benefit from GPT models, it is crucial to address issues with interpretability, data reliance, and ethical considerations. Industry may therefore take advantage of these GPT models' advantages, make wise decisions, and promote sustainable development.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPTs have the ability to have a positive impact on society and business operations. They can speed up operations like accounting, sales, and marketing, increasing productivity. But before they are widely used, ethical problems need to be fully investigated. Technology products will change as GPT models develop. To reap the benefits and reduce dangers, it is essential to solve interpretability and data concerns. GPTs can have a tremendous positive impact on businesses, society, and the economy when they are used responsibly.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Agriculture\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Traditional agriculture, a time-honored practice passed down through generations, sustains civilizations with its crop cultivation and livestock rearing methods. Rooted in a deep connection to nature, it emphasizes sustainability and local ecosystem understanding. Beyond providing sustenance and livelihoods, traditional agriculture preserves cultural heritage. However, it also faces challenges such as labor-intensive processes and shortages, inefficient resource utilization, vulnerability to pests and diseases, and limited access to real-time data and environmental impact. Today, by merging tradition with modernity, we have the opportunity to leverage technological advancements to enhance productivity, sustainability, and resilience while honoring the profound legacy of traditional agriculture for future generations.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Agriculture \n",
            "\n",
            "GPTs have the ability to overcome the challenges of agriculture. It offers valuable advantages to the agriculture sector. It acts as a comprehensive knowledge source, providing information on crop cultivation, pest management, and soil health. By analyzing real-time data, GPT assists farmers in making informed decisions regarding optimal planting times and resource allocation. It plays a crucial role in identifying and addressing crop diseases and pests accurately. Moreover, GPT enables precision farming practices by utilizing sensor data and satellite imagery, ensuring precise irrigation, fertilization, and pest control. Additionally, it provides market analysis and price prediction, empowering farmers to navigate market conditions and optimize pricing strategies. GPT also supports farm management and planning, optimizing crop rotation and resource usage. By facilitating agricultural research and innovation, GPT contributes to advancements in crop breeding and sustainable practices. Embracing GPT in agriculture enhances decision-making, efficiency, and sustainability, ultimately promoting improved productivity and food security. For instance, GPT-4 can educate farmers about new methods and goods and warn them of potential issues or possibilities by analyzing data from many sources <cit.>.\n",
            "\n",
            "    \n",
            "  * Improving Crop Yields:\n",
            "\n",
            "\n",
            "\n",
            "With its data analysis capabilities and real-time recommendations, GPTs plays a crucial role in enhancing crop yields. By examining historical yield data, weather patterns, soil conditions, and crop management practices, GPT identifies valuable patterns and correlations, providing insights and suggestions for optimal crop management techniques<cit.>. It enables precision farming by integrating data from sensors, satellites, and IoT devices, granting timely guidance on resource allocation for improved efficiency. Additionally, GPT aids in the early identification and management of crop diseases and pests, minimizing yield losses through precise and prompt recommendations. Moreover, GPT supports crop breeding and genetic optimization by analyzing genetic data and plant characteristics, expediting the development of high-yielding and resilient crop varieties. Therefore, GPTs data analysis and decision support capabilities significantly contribute to enhancing crop yields and maximizing agricultural productivity<cit.>.\n",
            "\n",
            "    \n",
            "\n",
            "    \n",
            "  * Pest Control:\n",
            "\n",
            "    GPT offers significant support in the realm of pest control in agriculture. By analyzing extensive data on pests, including their behavior, life cycles, and characteristics, GPT can provide valuable insights for effective control measures. It aids in early pest detection by analyzing sensor data and satellite imagery, enabling proactive interventions to prevent pest spread and minimize damage <cit.>. GPT also assists in determining suitable pest control methods tailored to specific crops and pests, considering factors like environmental impact and sustainability. Additionally, it contributes to precision pest control by leveraging real-time data to optimize timing and dosage of interventions, reducing chemical usage and resistance risks. It also aids in identifying natural enemies and beneficial organisms, promoting natural pest control mechanisms such as habitat diversification and companion planting. Through GPT's data analysis and recommendation capabilities, it empowers farmers with informed decisions, leading to more effective and sustainable pest management strategies, ultimately reducing crop losses and enhancing agricultural productivity.\n",
            "\n",
            "    \n",
            "  * Identifying Diseases and Soil analysis:\n",
            "\n",
            "    GPTs offer valuable assistance in disease identification and soil analysis within the field of agriculture. With its ability to analyze extensive data sets, GPT can accurately identify crop diseases by processing information such as symptoms, historical data, and disease patterns. This enables timely and effective disease management strategies<cit.>. Additionally, It plays a significant role in soil analysis by analyzing diverse soil-related data, including nutrient levels, pH, organic matter content, and soil composition. By interpreting this data, It provides insights into soil health and fertility, empowering farmers to make informed decisions regarding nutrient management, soil amendments, and cultivation practices. Moreover, GPT can identify complex interactions between soil conditions and crop diseases, helping farmers understand the relationship and take preventive measures accordingly. It also supports precision agriculture practices by integrating sensor data and satellite imagery to assess soil variations across fields, allowing for site-specific management strategies and optimized resource allocation. Furthermore, it also facilitates knowledge sharing and collaboration by analyzing and disseminating research findings, best practices, and disease outbreak information among agricultural communities. This collective intelligence enhances disease monitoring and control efforts on a broader scale. \n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "While GPT, provides significant benefits to agriculture, there are challenges to its implementation. GPT's effectiveness depends on the availability and quality of data, making insufficient or biased data a limitation. The interpretability of GPT's decision-making process is challenging due to its black-box nature, hindering trust and understanding. GPT's computational requirements and infrastructure can be demanding, posing difficulties for resource-constrained farmers. Language and domain-specific nuances can affect its performance, impacting accuracy and relevance. Ethical considerations surrounding data privacy and ownership need careful attention to ensure responsible use. By addressing these challenges, researchers and practitioners can unlock GPT's potential while ensuring its practicality and ethical implementation in agriculture.\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPT holds immense potential in agriculture, offering numerous benefits alongside notable challenges. Its data analysis capabilities empower farmers with informed decision-making in disease identification, soil analysis, and precision farming, leading to improved crop yields and sustainable practices. However, the effectiveness of GPT relies on data availability and quality, while its interpretability remains a challenge due to its black-box nature. Additionally, computational requirements, language nuances, and ethical considerations require careful attention. By addressing these challenges, the agricultural sector can harness the full potential of GPT, paving the way for more productive, efficient, and responsible farming practices.\n",
            "\n",
            "\n",
            " §.§ Travel and Transport\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Historically, animals have been used by people as their main source of transportation. But as the world's population increased, the demand for more effective transportation systems increased. Transportation-related technological advancements have fundamentally changed the sector in several ways. Business operations like order tracking, freight management, and customer support can be streamlined by automation employing AI-driven technologies. Companies can enable their employees to concentrate on more beneficial and profitable duties by automating these tasks<cit.>. With better transportation networks and logistics management systems that optimize routes and reduce transit times, technological developments also enable speedier delivery times. In terms of product development, technical advancement has paved the way for the development of innovative vehicles, infrastructure, and logistics systems, leading to the production of more sophisticated and effective transportation choices. Another noteworthy benefit of technology advancement in logistics and transportation is increased customer service. Inquiries and problems can be handled quickly and efficiently by chatbots and customer support systems powered by AI, improving the entire customer experience<cit.>.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Travel and Transport \n",
            "\n",
            "Companies can learn about customer preferences in real time by using GPTs in logistics and transportation, which results in better personalization and more customer satisfaction. GPTs leverage NLP approaches to interpret customer requirements and preferences, enabling customized suggestions as well as guidance in the logistics and transportation processes. The most effective routes and forms of transportation can be recommended using GPTs, which can analyse a large amount of data, including traffic patterns, weather conditions, and delivery requirements<cit.>. In addition, GPTs can be used as travel planners, allowing visitors to enter their travel budget, duration, and destination to create customized itineraries. For travel agencies, this personalized approach increases consumer satisfaction and revenue.\n",
            "\n",
            "    \n",
            "  * Logistical Management: GPTs can be quite important in the context of shipping logistics. They can automate the creation of shipping labels, eliminating up manual entry and lowering the possibility of mistakes. Additionally, GPTs can have access to real-time tracking data and can integrate GPS data and sensors to provide businesses and customers with precise and up-to-date shipment status information. Companies can successfully monitor shipments with the use of GPTs, geographic information systems (GIS), and routing algorithms<cit.>. Organizations can track shipments in real-time and ensure visibility throughout the supply chain by utilizing GPS data and sensor technology<cit.>. Customers can receive precise updates on their shipments using this real-time information, which will improve their experience overall. Overall, the use of GPTs into shipping logistics results in increased automation, efficiency, and client satisfaction. \n",
            "    \n",
            "  * Intelligent Fleet Management and Tracking: Companies can get real-time fleet updates by utilizing GPT models, which enables them to track vehicles quickly and precisely. GPT models' underlying technology also supports proactive fleet management. GPTs can identify possible problems or maintenance needs before they develop into expensive breakdowns or accidents by analyzing data from a variety of sources <cit.>. With this knowledge, companies may take preventative measures, such as planning maintenance or quickly fixing developing problems, ultimately saving time and money by preventing unintended delays. Additionally, GPTs can provide clever alerts and notifications. Businesses can receive alerts when vehicles arrive at specified areas by setting up specific triggers, which enables better coordination and customer service <cit.>. For instance, businesses can alert clients or storage facilities in advance of a truck's arrival, allowing for effective unloading and loading procedures.\n",
            "    \n",
            "  * Real-Time Inventory Tracking:  GPTs enable businesses to manage their inventory levels while on the road with a cloud-based platform that makes it simple to access inventory data from anywhere in the world. Better inventory management and decision-making are made possible by this real-time accessibility. This ensures that the appropriate quantity of stock is accessible when needed to fulfil consumer requests, while minimizing carrying costs and preventing lost sales as a result of stockouts. GPTs can streamline inventory management procedures by eliminating the need for human data entry into spreadsheets, saving time and cutting overhead costs <cit.>. With the advent of 5G technology, the cost of connected devices has dramatically decreased, making it more practical and affordable for businesses to set up and operate connected inventory monitoring systems. This may make real-time inventory tracking solutions more widely adopted, thereby increasing the effectiveness and precision of inventory management <cit.>.\n",
            "    \n",
            "  * Streamlining Delivery Operations: GPTs are able to estimate traffic trends and improve routes for both drivers and passengers using real-time data <cit.>. These models can produce effective routes that reduce travel times and enhance overall delivery performance by taking into account aspects like traffic congestion, road conditions, and delivery schedules. Route optimization not only reduces travel time but also benefits the environment. In order to improve air quality and create a more sustainable delivery process, it is possible to cut down on idle times and trip distances. Businesses may streamline operations, improve the overall customer experience, and contribute to a more sustainable and environmentally friendly approach to logistics by automating procedures, optimizing routes, and utilizing real-time data <cit.>.\n",
            "\n",
            "    \n",
            "  * Tourism: GPTs have the potential to significantly improve a number of tourism-related aspects. GPTs can offer customized solutions that suit the individual's preferences by understanding their needs and interests, resulting in a more pleasurable travel experience. GPTs are excellent at understanding and creating text that is human-like <cit.>. This functionality can be used in the travel and tourism sector to enable chatbots or virtual travel assistants to communicate with users in natural language <cit.>. Trip planning and information retrieval are made more simple and user-friendly by the ability of travelers to ask questions, look for advice, and obtain full details about destinations, modes of transportation, customs, and more. GPTs are capable of producing in-depth and interesting descriptions of tourist sites, attractions, lodging, restaurants etc. GPTs can provide time-efficient routes that guarantee a complete travel experience <cit.>. Including advice on local legislation, emergency contacts, medical facilities, and potential risks, GPTs can offer helpful information and direction regarding travel safety. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "Privacy issues may occur when using sensitive data in travel GPTs. It is essential to manage user data sensibly and putting up strong security measures to safeguard private data. The quality of the model's outputs is directly influenced by the correctness and completeness of the data utilized during the training phase. Ethical considerations should be taken into account when creating AI-powered applications employing GPTs. It's crucial to check that the models are truthful, unbiased, and free from harmful presumptions or discriminatory procedures <cit.>. Although the models contain advanced features, they are difficult to tailor for specific use cases, need a lot of data to train, and have built-in limitations.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "Emerging GPTs have the potential to enhance productivity, communication, and the calibre of goods and services, which will benefit many aspects of people's life. GPTs can offer real-time updates, effective route optimization, and customized recommendations in the travel and transportation industries, enhancing the overall travel experience and increasing operational effectiveness. Adopting them, however, comes with some difficulties. As specific roles are replaced by automation, GPTs may result in job displacement <cit.>. Additionally, the computational and memory requirements for GPTs make their deployment on compact or low-power devices difficult. GPTs may not be accessible to growing businesses due to the high costs associated with obtaining and using them. Despite these obstacles, attempts are being done to overcome them and improve the usability and value of GPTs for a larger range of users.\n",
            "\n",
            "\n",
            "\n",
            " §.§ E-Commerce\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Electronic commerce, commonly referred to as e-commerce, is a way for conducting economic transactions and create relationships between groups of people and entities using digital information processes and electronic communications <cit.>. Globally, this type of trade has experienced substantial growth, particularly in the retail sector. The preference for internet shopping, especially among younger millennials, is a prominent trend in consumer behaviour. Mobile devices have consequently taken over as the main method for carrying out internet transactions <cit.>. Therefore, it is crucial for e-commerce companies to give the customer experience in their mobile applications top priority. The provision of brief text summaries for titles and reviews is an essential component of this. These summaries are essential for optimizing search results, helping consumers identify appropriate items, and ultimately raising customer happiness in the online purchasing space <cit.>.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in E-Commerce Realms\n",
            "\n",
            "The e-commerce sector could significantly advance with the introduction of GPTs. GPTs can be accessed by users or customers and are intended to answer commonly asked questions and give in-depth details about many elements of the e-commerce process, such as products, delivery, refunds, and more <cit.>. One of the main benefits of GPTs is their capacity for quick responses, which decreases the amount of time customers must wait to hear back from businesses <cit.>. By taking care of an important number of client inquiries, this function not only increases customer happiness but also lessens the workload on support workers. Customers will ultimately have a better purchasing experience as a result of being able to quickly acquire the information they require and interact with GPTs <cit.>.\n",
            "\n",
            "    \n",
            "  * Proofreading: To improve the calibre and accuracy of written content in e-commerce, GPTs can be used for proofreading. Written content is essential for product descriptions, marketing materials, customer reviews, and other text-based components in the e-commerce sector <cit.>. For the purpose of projecting professionalism, fostering trust, and delivering a satisfying user experience, this text must be devoid of errors, well-written, and grammatically correct. E-commerce companies can automate the process of identifying and correcting these problems by using GPTs for proofreading, which saves time and effort as comparison to manual proofreading <cit.>. This can be especially helpful in situations when users are writing product reviews or interacting with customer service. An improved user experience is facilitated by the early detection and rectification of errors, which also helps to avoid potential misunderstandings or miscommunications.\n",
            "    \n",
            "  * Order Processing: GPTs are useful in many areas of order management and customer service because they can comprehend and produce text that looks like human speech.  GPTs can help with handling consumer questions about orders <cit.>.  GPT is capable of interpreting the queries, providing important details like order status, tracking information, and expected delivery time, as well as suggesting corrections for frequent problems <cit.>. By delivering real-time information, GPTs can assist customers in tracking their orders. Customers can customize their purchase with the help of GPTs. GPTs can help in the identification of possibly fraudulent orders by examining past transaction data, consumer behaviour patterns etc <cit.>. Based on a customer's past purchases, browsing habits, and preferences, GPTs can offer tailored product recommendations. When a consumer puts a purchase, the model can examine the information and produce recommendations for related or supplementary products that the customer might find interesting.\n",
            "    \n",
            "  * Generating titles for products: Companies can use GPTs to produce interesting and educational material to improve the appeal of their product listings <cit.>. Based on a product's category, brand, and special characteristics, GPTs can come up with attractive titles for it. The model can produce imaginative and memorable names that aid in brand awareness and differentiation by receiving relevant information such as the characteristics of the product and the target market. GPTs are trained to produce in-depth and interesting product descriptions <cit.>. These summaries can offer a thorough summary that aids clients in selecting products wisely. GPTs are capable of coming up with clever and appealing captions for product images. GPTs can be adjusted to better reflect the tone and aesthetic of a certain brand <cit.>. As a result, the brand identity is consistent and unified throughout all product listings.\n",
            "    \n",
            "  * Strategy Planning: GPTs have the ability to come up with original and distinctive concepts for marketing campaigns <cit.>. The model can provide recommendations for different campaign aspects, such as slogans, taglines, themes, contests, social media strategies, and more by taking into account relevant information about the product, target audience, marketing objectives, and desired outcomes. GPTs can help with email writing that encourages readers to become partners, investors, or customers <cit.>. To increase the likelihood of a favourable response or interaction, these emails can be customized to address the needs and potential benefits for the receivers. To improve their comprehension and production of appropriate material, GPTs can be trained on domain-specific knowledge bases, such as e-commerce <cit.>. The models can offer more precise and situation-specific recommendations for advertising strategies, product positioning, and target audience interaction because of this specialized training. \n",
            "    \n",
            "  * Data analysis: There are numerous ways to use GPTs for data analysis in e-commerce. E-commerce data preparation can be aided by GPTs <cit.>. Data normalization, cleansing, and formatting are a few of the duties involved. GPTs can produce summaries, identify significant topics, and extract appropriate data by studying textual descriptions, reviews, and consumer feedback. This helps you know the data more thoroughly, identify trends, and find insightful information. Customer reviews and social media comments from e-commerce can be analyzed for sentiment using GPTs. The sentiment expressed in text can be evaluated using GPTs and categorized as either positive, negative, or neutral <cit.>. Understanding client views, recognizing product strengths and deficiencies, and making data-driven decisions all benefit from this analysis, which also helps to increase customer happiness. Segmenting consumers based on preferences, behaviours, or past purchases can be aided by GPT models <cit.>. For the purpose of detecting fraud in e-commerce transactions, GPTs can be used. GPTs can support the identification of potentially fraudulent actions by examining past transaction data, user behaviour patterns, and recognized fraudulent tendencies <cit.>. \n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "While GPTs have a lot of potential for numerous e-commerce applications, they also have several drawbacks. In order to produce responses, GPTs mostly rely on the context given in the input text. They could, however, find it difficult to fully understand the broader context or details that are unique to e-commerce. GPTs provide replies using training data and prior knowledge. They are unable to access real-time data or carry out real-time calculations <cit.>. They might not be appropriate for giving current information, such as pricing, product availability, or dynamic promotional offers. GPTs gain their knowledge from a wealth of training data, which includes text taken from the internet, which may be biased, stereotyped, or otherwise offensive <cit.>. The models may unintentionally provide biased or unsuitable replies if they are not rigorously managed and monitored, which could be harmful to the customer experience and brand reputation. The use of ethical principles and the training data must both be given careful thought.\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "The conversational interface offered by GPTs customizes the purchasing process and makes interactions with clients more interesting and appropriate to their individual requirements. GPTs can also be utilized to get insightful customer feedback. Businesses can learn about customers' preferences, issues, and opinions regarding their products and services by conversing with them. In order to better serve their target audience, organizations can use this information to discover areas for improvement, increase customer happiness, and make data-driven decisions. \n",
            "It's essential to recognize that GPTs might occasionally make mistakes or give poor answers, particularly when dealing with complicated or ambiguous queries. This highlights the necessity of continual model training, thorough testing, and modification to guarantee that they consistently meet consumer needs. To confirm the efficacy and dependability of using GPTs specifically in the e-commerce area, more research and testing are required. When implementing GPTs, it's critical for businesses to take into account the particulars of their own e-commerce businesses, their target market, and the type of the client enquiries. Regular monitoring and feedback analysis, along with a systematic and iterative approach, can help make sure that the outcomes of using GPT models are in line with the objectives of e-commerce enterprises.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Entertainment\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "In the ancient days, the Entertainment meant about playing games with neighbors covers all outdoor games, indoor games and chatting with neighbours through telephone. As digitization has bought greater advancements in computation and communication, in turn access to internet is also much easier. This has changed the way people are entertained. as people are connected and fully engaged in completing the target for the day. And there was a radical shift from traditional employment to employment in the Industrial Revolution age. Stress and pressure are common factors hindering people of different age groups. The different forms of entertainment serve as stress busters. Entertainment and mental health are interrelated; the former transfers happiness, bringing harmony and peace to mental health. Some common forms of entertainment include playing games, watching TV series or movies, or funny videos, shopping, debugging, coding, browsing the internet, listening to music, dancing, chatting, painting, crafting, reading books, cooking, and many more, which can lessen the stress carried<cit.>. Entertaining and getting entertained is the biggest motivation and medicine for all mental illnesses. Entertainment helps to improve the motor skills of humans, thereby inducing a positive cognitive effect towards the work.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT on the Entertainment Industry\n",
            "\n",
            "GPT is a potential game-changer in the entertainment field, delivering endless entertainment. Since its evolution, GPT models have been adopted as an entertainer crosschecking their ability to produce content on funny and illogical questions. GPTs entertain people in many ways, and of course, using GPT itself an entertainment as it reduces the burden of overthinking by providing immediate feedback to queries in seconds<cit.>. The results are amazing and have been utilized for many purposes today. When the GPT model was probed to complete a scene from the movie “Frozen,” it responded with an entertaining writeup<cit.>. Some of the impacts of GPTs on Entertainment applications are given below:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  * Solitude with GPT: As the GPT itself is an entertainer, one can feel better alone with the GPT, which helps to come out of loneliness by exploring its savors<cit.>. GPTs assist in providing soothing poems, mental healing quotes, and funny riddles. People with loneliness may feel anxiety, especially with older ones at home. In this case, GPT-4 helps people with its Voice Technology feature, enabling users to input their audio<cit.>. In turn, the GPT model responds to user-specific speech output using NLP algorithms embedded with it. The elderly can feel safe and attentive at home. GPT-4 is multilingual and can understand various dialects and accents for personalized user experience.\n",
            "\n",
            "\n",
            "  * Enhanced Customer Interaction: The advent of ChatGPT and Bard has improved customer interactions on content such as movies, Over-the-Top (OTT) platforms like Netflix, Hulu, Disney+ Hotstar, and prime video, sound recordings, song lyrics, pictorial works, comics, jokes, memes, viral videos, and other entertaining factors. Further, GPTs provide human-like recommendations on user-specific fun activities based on user interactions for an immersive experience. This has dramatically improved the interactions in the engagement industries. User engagement can be further improved by providing dynamic and more realistic responses to user queries, such as creating virtual actors for interacting with real actors<cit.>.\n",
            "\n",
            "\n",
            "  * Personalized Content Creation: GPTs can help generate user-specific personalized content by analyzing the user preferences and generating content like predicting future scenarios tailored to the user’s interest. GPTs can be used for creating personalized, engaging, and high-quality content for online business advertising, ideas for content generation, marketing messages for attracting customers, descriptions for selling products, and captions for social media<cit.>. In addition, it can be used for optimizing the contents for search engines, i.e., GPTs will provide relevant terms for search, thereby avoiding traffic to the web sources.\n",
            "\n",
            "\n",
            "  * For the Film and TV industry: GPT-powered virtual assistants assist users in booking tickets and generating content and personalized recommendations using AI models. The evolution of GPT-4 with advanced NLP and DL algorithms helps the scriptwriter to generate AI-driven content without the human author named virtual storytelling<cit.>. GPTs create interactive stories, dialogues, and characters, recommending suitable characters. Furthermore, GPTs can be used to create content for video games, voice-enabled applications, AR applications, and other VR experiences in virtual worlds<cit.>.\n",
            "\n",
            "\n",
            "  * For Social media influencers: GPTs can generate personalized marketing ads for each customer based on their previous interactions and provides relevant suggestions for customer viewing experiences. Youtubers and other social media content creators will potentially benefit from generating channel content based on demand and realistic societal activities.\n",
            "\n",
            "\n",
            "  * Realistic Gaming Interactions: GPT helps to generate the players, gaming narratives, dialogues, user interface, and user-specific gaming recommendations and new game creation. Powerful HCIs can render a better user experience for game developers and players. Assistance to the game developers in debugging and enhancing the code developed. GPT uses various NLP and AI algorithms trained with massive data to predict the next phrase/movements and provide human-like experiences in 3D gaming environments. ChatGPT has been integrated with AR and VR to provide an immersive gaming experience.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "Latency is the major issue connected with rendering the voice-based response to the voice input. As well, plausible misinterpretations may mislead the responses, and interruptions to the relayed output are difficult. Enabling technologies like EC and 5G can help overcome this issue. Also, GPTs must be capable of storing the facts with audio conversations to relay them while conversing the other day. Furthermore, the AI system must be built in such a way that it can continuously learn (lifelong machine learning) and enhance over time. The major ethical concern with virtual storytelling is the bias exposed in the training data and the obscurity of reproduced content on the generated stories. Another issue with the generated content is plagiarism (i.e., producing content similar to the content in the published articles or books), raising disputes with intellectual property rights. In addition to this, the source of the content generated remains unexplored. The language barriers in using GPT must be lessened to improve user experience and utilize the features of GPT <cit.>. The implication of the user to provide inputs in a certain format to GPTs can be further improved by providing different options in addition to voice-based inputs GPT4, like braille screen input for visually disabled people. The user authentication can also be further enhanced to safeguard user-specific content generation and avoid repeated content generation for users with similar requests. One of the primary concerns with the GPTs adoption is job loss. Content creators, bloggers,  and poets may lose their jobs.\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "The entertainment industry is the one which will be in demand always, as it is a lifeline for many individuals leading a stressful work environment or personal life. Despite the stress, entertainment has become part of routine life due to its immersive nature, creating harmony in the mind and the environment. GPTs have made a major contribution to enhancing the entertainment industry, but the job security of many professionals in this field remains unanswered. GPTs must be trained on unbiased data and ensure transparency in source content generation to provide a secure, robust, and efficient contribution to the entertainment industry. To attract all types of users, the multilingual capability and content rendering of GPTs can be further enhanced. The issues constrained by providing user inputs to GPTs can be alleviated to all extent.  Furthermore, safer user content generation without plagiarism and relating facts with previous conversations can be guaranteed by abiding by the storage requirements to deal with a more personalized user experience.\n",
            "\n",
            "\n",
            " §.§ Lifestyle\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Lifestyle, the way of our living, is one of the prominent areas most people in today’s digital era of AI,  are bound to and look for constant improvement. The “modus vivendi” is a Latin expression that semantically means a way of living and should be understood in terms of values and attitudes. These two terms manifest self, influenced by family, society, and global media. Directly or indirectly, these influence an individual’s lifestyle. Adopted from a sociological perspective, an individual expresses oneself through different practices, viz., eating ways, drinking behaviours, mode of travel, travelling places, costume designs, body-shaping cloth to wear, media preferences, education choices, job preferences, entertainment modes, managing leisure time, means of communication and so<cit.>. In all these practices, individuals would like to explore and learn about what, where, how, and when factors for reading sustainable development<cit.>. The concept of lifestyle is all about “how one wants to live one’s life.”  Consumerism is the act of purchasing artifacts for societal status and is one of the thriving lifestyle factors. Certain standard indicators like job, wealth, and physical and mental health determine the quality of one’s life. Also, the choice of a healthy lifestyle moderately determines the health of an individual<cit.>. Furthermore, few people believe that lifestyle reflects their socioeconomic status. Many epidemiologic studies state that better lifestyles have dramatically reduced the risk of various chronic diseases and are the primary cause for their prevention<cit.>. The lifestyle has been defined on different societal levels from individual, positional, and national to global<cit.>.  At the global level, lifestyle is adopted by general world-class influencers. In contrast, at the national level, the influencing factors will be the government and different cultural patterns across the country. The positional level concerns influence from different status groups, age categories, gender groups, and social classes. And the individual level is influenced by a closely moving group of individuals concerned about self-identity. The major source of information about these influencers is the Internet through social media networks and personal development advertisements. \n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Lifestyle\n",
            "\n",
            "The most remarkable application of AI, the GPT, paves the way for the betterment of mankind in offering human-like intelligent conversation on all whereabouts. People will always prefer to interact with other peers to learn their attributes and tweak them for societal status. Various GPTs have flourished for different lifestyle indicators, and they provide human-like assistance to all queries on fine-tuning the lifestyle by harnessing the power of AI<cit.>. The advanced reasoning capability of GPT-4 serves the purpose better<cit.>.\n",
            "\n",
            "\n",
            "  * Diet Planner: Free GPT applications for maintaining a balanced diet, helping the individual with a weight loss diet plan, followed by a brief list of meal plans, required shopping lists, physical activity plans targeting particular body parts<cit.>, motivational messages, and personalized sleeping patterns.  These apps act more like personalized training assistance and help to track progress with visualization charts or graphs. Fitness level, available free hours, medications taken, and available exercise equipment will be given as input to GPT.\n",
            "\n",
            "  * Travel Guide and Trip Advisor: Harnessing AI models, GPT provides an individual’s travel plan itinerary based on information like the place(multiple cities), budget, and the number of days. These GPTs provide local recommendations on restaurants, hotels, and other attractions. RoamAround, Roamr, and VacayChatbot are some of the travel planning GPTs<cit.>.\n",
            "\n",
            "  * Personalized Stylist and Beauty Advisor: GPTs can act as personalized stylists for an individual by generating occasion-specific clothing and costume preferences.  GPTs can assist in organizing wardrobes based on seasonal outfits and provide recommendations on e-commerce fashion stores for purchasing favourite brands. GPTs can provide tailoring design options, fabric choices, and design materials. Furthermore, GPTs can provide updates on a stock based on the preferred searches and provide insight into fabric types suitable for weather conditions that suit personal style.\n",
            "\n",
            "  * Personalized CookBook: GPTs can serve as cooking assistants by recommending new curated recipes suiting the family dietary plan, ingredients available, time, individual’s cooking skills, and new flavoured ingredients.  ChefGPT, PantryChef, and MacrosChef are some GPTs that generate unique and delightful recipes<cit.>. Consequently, GPTs can assist in shopping list recommendations and the nutritional value of the recipe generated.\n",
            "\n",
            "  * Hobby Curator : GPT assists an individual in identifying one’s enjoyable leisure time activity by learning new skills<cit.>. Having a list of interests and ideas ready, the GPT helps narrow down various options, instructional videos to proceed, chatting and sharing with online communities, and researching the cited hobby to explore more fun. Budget will also be an important factor in this perspective, as learning new hobbies may require joining paid classes or courses. GPT provides step-by-step instructions and guidelines to learn a new skill faster.\n",
            "\n",
            "  * Dream Maker: GPTs with multimodal learning helps to search for a job based on one’s qualifications and experience. In turn, it assists in preparing the job-specific resume, cover letters, training for the interviews (coding and technical queries), and grooming sessions and can redirect to the training place where knowledge can be acquired<cit.>. The futurist GPT models can assist in phase-by-phase questionnaires in the interview process\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "The recent version of the GPT uses both reinforcement and supervised learning models so it can learn based on the interaction with the user and can use existing data to derive personalized decisions. In the context of lifestyle, GPTs offer the most promising solution for almost all lifestyle influencers, but the still challenging part is the trustworthiness of the data and copyright issues. Also, relying more on GPT as it solves all our problems may insipid human intelligence in upcoming generations. Though the GPT provides weight advice, it can never be a substitute for the medical practitioner, as some information can be misleading. Travel planning GPTs sometimes require users to update information in a specific format and may have outdated databases. GPTs cannot access specific job openings' websites but can still provide insights into acquiring them. At times, it can produce nonsensical information <cit.>. Therefore, before adopting the GPT recommendations fully, further instigation is recommended. Furthermore, developing a large multimodal learning model abiding huge and dynamic datasets will be costly. \n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPT is a personalized assistant for improving an individual’s lifestyle from various prospective influencers. Generating personalized recommendations alleviates an individual’s fear of survival in the digitized society. Individuals will be personally trained to adapt to different cultural and technological shifts in the sustainable development of themselves and the economy as a whole. On the other hand, more stringent recommendations may incur huge budget overruns and sometimes provoke the individual to misinterpret, leading to dreadful consequences. GPTs provide both positive and negative recommendations based on the input fed. So, for the effective adoption of a GPT for lifestyle practices, adverse training and testing on extreme behaviours must be carried out. GPTs must be trained in the realistic and dynamic perception of individuals in real life.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Gaming\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "Before the advent of technology and the gaming industry, entertainment was primarily centred around activities such as reading, listening to music, watching plays and movies, participating in sports and physical games, and socializing with friends and family. People also engaged in traditional board games and card games, which were often played in groups and provided a fun and social way to pass the time. After technology stepped into the gaming industry, the way games are created, and the experience it has given users have transformed tremendously. Technology has enabled developers to create more immersive and engaging experiences for players. It has contributed in various ways, like improving graphics, performance, online play, and mobile gaming. Improved GPUs and other technologies allow for more detailed and realistic graphics, making games more visually stunning. Faster processors and higher amounts of RAM allow for smoother gameplay and faster loading times, reducing lag and improving overall performance. Technologies like AI, AR, and VR have created a new dimension of game development and experience. Players can now immerse themselves in gaming worlds in a way that was not possible before. With the help of advanced AI techniques, game developers can create more sophisticated and challenging opponents for players, as well as NPCs with more realistic behaviours. Technology has greatly expanded the possibilities of gaming and enabled developers to create more immersive, visually stunning, and engaging experiences for players.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Gaming \n",
            "\n",
            "\n",
            "GPTs have the ability to contribute to all sectors, including the gaming sector. GPT are not specifically designed for creating and playing games, but they have the potential to improve the gaming experience by improving enhanced dialogue and story telling, creating dynamic and personalized gaming worlds, generating more realistic and engaging characters <cit.>, game content creation, chatbot development. \n",
            "\n",
            "\n",
            "    \n",
            "  * Chatbot development: GPTs have been used in gaming through the development of chatbots that use NLP to communicate with players <cit.>. Because it allows the chatbot to understand and respond to a wide range of user inputs and queries related to the game. GPTs have been pre-trained on a large corpus of text data, which makes them adept at NLP. It can understand and respond to user queries in a way that feels natural and intuitive. It can also understand the context of a user's query, which means they can provide relevant and useful responses even when the user's query is ambiguous or incomplete. It can also generate game-related content, such as descriptions of game characters or settings, that can help to enrich the user's gaming experience. Furthermore, it can also personalize the user's experience by learning from their previous interactions with the chatbot and tailoring its responses accordingly.\n",
            "  \n",
            "    \n",
            "  * Game content creation: GPTs are used in game design. They are used to create game content such as levels, items, and quests. If the game designer is working on a new role-playing game, GPT can be used in creating characters to be used in the games. To generate new character classes in the games, the developer has to give inputs that contain information about the game environment, game settings, player abilities, and game play mechanics. GPTs has the ability to analyze the text and expectations given by the developer, and it can generate a list of potential character classes based on the expectations given as text. The designer then refines the ideas and chooses a more suitable character to develop further with unique abilities and game mechanics. The authors in <cit.> have used GPT2 and GPT3 to procedurally generate role-playing game with video game descriptions. The resultant quest  was evaluated by 349 online RPG players. The results concluded that one of the five quest descriptions was accepted for game development.\n",
            "    \n",
            "  * Analyze player’s ability and skill: GPTs can detect and analyze players' abilities and skill levels and tailor the game accordingly. This analysis helps in making dynamic modifications to the game environment based on the player's abilities and skill levels. This feature helps achieve dynamic difficulty balancing. GPTs can also assist in identifying the player's intent. Thus, when players ascend to higher levels, it can assist in making the games more challenging based on the player's abilities and skill levels in the previous levels\n",
            "    \n",
            "  * NPCs: NPC stands for \"Non-Player Character.\" In AI games, NPCs refer to characters or entities in a game that are not controlled by a player. NPCs can take on a variety of roles within a game, such as enemies to fight, quest givers, merchants, or friendly characters that provide helpful information. They are often controlled by AI algorithms that determine their behaviour and actions within the game world. GPTs are not specifically designed for creating NPCs, but they can be used to generate dialogue and other character interactions that can be incorporated into NPCs. Additionally, It can be used to generate character backstories and personalities, which can inform the development of NPCs. The authors in <cit.> have trained and used GPT-2 for text generation of video games. They have trained GPT-2 on a large corpus of video game quests and used a GPT model to generate the dialogue for quest-giver NPCs in role-playing games. The output has shown that GPT can learn the structure and linguistic style of the games, and the quality of the content it has generated is high, making it a good alternative to writing new RPG quests by hand.\n",
            "    \n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            " GPTs are computationally expensive and require high computing resources to do their purpose. This means that implementing them in a game would require powerful hardware and this could have an impact on the performance of the games.\n",
            "Lack of training data: GPTs require large amounts of high-quality training data to be effective. In the gaming industry, this could be difficult to obtain, as gaming data are likely to be fragmented and less structured than the kind of data used to train GPT models <cit.>. In addition, GPTs can perform content creation based on patterns they have learned from their training data, which means that they can be unpredictable. The content generated by GPT may be nonsensical or inappropriate content to the game. In the context of gaming, this lack of control could lead to undesirable or even offensive game content. GPTs can generate text based on user input, they can't interact with the game environment in the same way a human player can. This limits their usefulness in gaming and may make them less effective than other AI technologies.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "GPTs can transform the gaming industry by contributing to improved game dialogue creation, enhanced non-player characters, personalized gameplay, procedural content generation, chatbot generation, and analyzing players' abilities. However, it also has potential challenges that are to be addressed, such as the need for high computing resources, a lack of control over content creation, and restricted interaction with the game environment. In addition, the most important challenge in adopting a GPT model in gaming is a lack of training data. If the challenges are addressed and the gaming industry evolves with properly structured data to train a GPT model, then GPTs can revolutionize the field of gaming.\n",
            "\n",
            "\n",
            " §.§ Marketing\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "\n",
            "Traditional marketing primarily relied on traditional media channels, such as television, radio, newspapers, and magazines, to reach consumers. Companies used to develop marketing campaigns based on demographic data, and mass media channels were used to broadcast these campaigns to a broad audience. However, the advancements in technology have brought about significant changes in the marketing industry, and companies are increasingly integrating new marketing strategies evolved through technologies to reach and engage with customers. One of the significant transformations has been the rise of digital marketing channels such as social media, search engines, Email, and mobile applications that allow companies to target specific populations with precision and provide real-time feedback on campaign performance, allowing for more effective and efficient marketing. Technology has also given rise to marketing automation tools such as customer relationship management systems, chat-bots, and personalized email marketing, which have made marketing more efficient and effective. Another significant transformation has been the use of big data and analytics to better understand customer behaviour and preferences. This has allowed companies to create more personalized and targeted campaigns based on specific customer needs and preferences.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Marketing \n",
            "\n",
            "The marketing industry has evolved with various AI-powered techniques. This revolution started in marketing by providing businesses with powerful tools for generating insights, automating processes, and improving customer experiences. GPTs are also being used in marketing to generate engaging and personalized content. Some of the applications of GPT in marketing include content creation, customer service, and personalized advertising.\n",
            "\n",
            "\n",
            "\n",
            "    \n",
            "  * Content creation: GPTs can contribute to marketing in various ways, such as by improving speed and efficiency in content creation, ensuring consistency and quality of content, generating personalized content, creating multilingual content, and repurposing existing content. It can be trained on a company's existing marketing materials and customer data, allowing it to create new content, such as blog posts, social media updates, and product descriptions, in a fast and efficient manner. Despite its speed, it maintain high standards for quality and consistency. Moreover, GPTs <cit.> can generate personalized content based on customer data, such as search history and past purchases. This helps create content that is relevant to the users' desires, leading to better engagement and conversion rates. GPTs can also generate content in various languages, allowing marketers to expand their reach across regions. Copy.ai <cit.> has used GPT-3 to generate human-like text that is optimized for marketing purposes such as website copy, social media posts, advertisement copy, and email campaigns. This means that marketer personnel no longer focus on content creation. Instead, they can spend productive time improving the other aspects of marketing. \n",
            "    \n",
            "  * Customer service: GPTs can be trained on customer service conversations and chat logs to generate more natural responses, like humans. This can help business personnel provide better customer service 24/7 and save time and resources. It can be trained to generate automated responses for frequently asked questions, providing faster responses to customers and ensuring consistency in the quality of replies. GPTs can also analyze customers' emotions and sentiments, enabling businesses to proactively address negative feedback. This is particularly helpful in maintaining customers' trust. The authors in <cit.> have used GPT-3 model for automated drafting of responses for incoming mails. They used it to understand the mail, and then software engineering and business studies were used to understand the challenges encountered and finally, the response generated after a thorough understanding of the context of the mail. The authors have concluded that applying GPT-3 to rationalize email communication is feasible both technically and economically.\n",
            "    \n",
            "  * Personalized advertising: GPTs can generate personalized content such as product descriptions, blog posts, and social media captions tailored to individual customers' preferences and interests. This can help businesses create content that resonates with their target audience, leading to higher engagement and conversion rates. By analyzing customer data, GPTs can segment customers according to their behaviour, interests, and preferences. As a result, businesses can tailor their marketing campaigns to each segment and provide personalized messaging and offers that are more likely to connect with each customer group. The authors in <cit.> have proposed a generative model to identify the name of the product from the product text and use this information filter to improve the product recommendation based on the product retrieval model. This method has been implemented in the dynamic product advertising system of Yahoo. It is observed that the recommendation system has recommended the product based on the user's interest, and it was evaluated using an A/B test to serve similar products in an ad carousel, which can help the system to explore more products efficiently.\n",
            "    \n",
            "  * Forecast analysis: Using customer data analysis, GPTs can forecast future behaviour and buying patterns. This allows businesses to customize their marketing campaigns to each customer's desires based on their purchase patterns, increasing the likelihood of conversion or purchase. The authors in <cit.> have used chatGPT to perform predictive modelling based on past data. They have used the GPT model to predict the future based on the customer’s behaviour and buying pattern. This primarily helps the system to recommend the products to the customers as per their desires. \n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "GPTs are designed to generate content that imitates human writing, but the content generated may not align with the brand's image or message. This lack of control can be a potential challenge for marketers. Another challenge that applies to all learning technologies is that data bias is possible in GPTs <cit.>. Based on the large dataset of text used for training, if the data is biased, it will affect the generated content, which may also exhibit the same biases. GPT is complex and difficult to interpret, making it challenging to explain how the model arrived at its conclusions. This lack of transparency can lead to a lack of trust in adopting GPTs, and marketing teams may struggle to make improvements in their strategies. As like every AI technology, there are ethical concerns associated with GPT models. For instance, the use of GPT in marketing could raise concerns about the use of personal data and privacy, particularly if the model is used to generate targeted advertising or personalized content. To avoid any negative consequences, companies must ensure they use these models ethically and transparently.\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "\n",
            "Using GPTs in marketing can provide various benefits, such as better content creation, personalized messaging, increased efficiency, competitive advantage, and enhanced customer experience. However, this strategy also involves potential challenges, such as limited control, data bias, lack of transparency, and ethical considerations. Therefore, companies must consider the advantages and drawbacks of GPT adoption in marketing, and implement these models ethically and transparently to avoid negative outcomes. Successful integration of GPTs in marketing requires proper planning, a skilled workforce, and continuous monitoring to ensure the desired results and mitigate any potential risks.\n",
            "\n",
            "\n",
            " §.§ Finance\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Introduction\n",
            "\n",
            "The finance industry, also known as the financial sector, is a broad term that encompasses a wide range of institutions and businesses that provide financial services to individuals, businesses, and governments. The finance industry plays a critical role in the global economy, facilitating the flow of funds between savers and investors, managing risk, and providing financial services and products to support economic growth. The finance industry has been the leader in technology adoption in recent years, with a focus on improving efficiency, reducing costs, and delivering better customer experiences. The adoption of technologies like big data and analytics, mobile and digital payments, blockchain and distributed ledger technology, AI and ML, and cloud computing make the sector more flexible, scalable, trustworthy, transparent, secured, and easier to access.\n",
            "\n",
            "\n",
            "  §.§.§ Impact of GPT in Finance\n",
            "\n",
            "GPT has greatly influenced finance by automating customer support using chatbots and virtual assistants, enhancing fraud detection, offering investment insights and recommendations based on financial data and news, assisting with risk assessment for investments and loans, impacting algorithmic trading strategies, simplifying compliance with regulations by analyzing legal documents, improving credit scoring and loan processes, and emphasizing the importance of handling sensitive financial data securely and transparently.\n",
            "\n",
            "\n",
            "\n",
            "    \n",
            "  * Sentiment analysis: Sentiment analysis is a technique used in the finance industry to evaluate the sentiment of investors <cit.>  and the general public towards specific companies, industries, or markets by analyzing news articles, social media posts, and other text-based sources of information. GPT has the potential to improve sentiment analysis in finance by providing more accurate and detailed analyses of financial data. With sentiment analysis, the industry can predict stock prices by assessing the sentiment of news articles, social media posts, and other sources of information about a particular company or industry to make informed investment decisions. By utilizing sentiment analysis, GPTs can aid financial institutions in identifying potential risks and taking appropriate action to mitigate them. The authors in <cit.> have investigated how incorporating a lexicalized ontology can enhance the performance of aspect-based sentiment analysis by extracting indirect relationships in user social data. The investigation results show that the analysis has given 98% accuracy.\n",
            "    \n",
            "  * Financial forecasting: GPTs can be trained on past financial market data to predict future trends in the stock market, exchange rates, and other financial metrics. This can help investors and financial organizations make more accurate predictions and reduce their risk exposure. With the ability to analyze and process the natural language, GPTs can be used to analyze and interpret financial data, news, and other related information. Financial analysts and researchers can use the ability to analyze natural language to extract insights from unstructured data like news articles, social media content, and other information that is relevant to forecasting. This can help improve the accuracy of financial forecasting models by providing a more comprehensive view of market trends and sentiments. This analysis may help improve the accuracy of prediction. Financial analysts can use the model to identify the relationship between the financial parameters that could change the market conditions in advance. This prediction may be helpful for investors as they make investment decisions.\n",
            "    \n",
            "  * Trading strategies: GPTs can also be used to analyze market trends and historical data to develop trading strategies. This can help traders make better decisions in terms of trading to increase their profitability. GPTs can be used to identify the potential risks in trading portfolios. By analyzing the large volume of information related to trading, GPT will get the potential to identify the risk parameters and provide insights into how to mitigate these risks. The authors in <cit.> have used a popular GPT for stock market trend prediction. The results show that the method used is simple but the efficiency and accuracy of the method are very effective. The prediction it has made is very close to the reality. \n",
            "\n",
            "\n",
            "  * Risk prediction and management:The adoption of GPT can enhance the process of risk prediction and management in several ways. It can improve data analysis by detecting patterns that may pose a risk. It can also help in enhancing fraud detection by analyzing transaction data and identifying fraudulent activity based on patterns. Additionally, GPT can be utilized to make better portfolio management decisions by analyzing historical industry data, company financial statements, and news articles, as well as social media feeds. This portfolio management process can provide valuable information about the investment risk of a given organization, enabling informed investment decisions and effective risk management. \n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Challenges\n",
            "\n",
            "GPTs have more challenges in the finance sector. Primarily, they demand significant computational resources to train and deploy, which can be expensive and time-consuming for financial organizations to implement. Another challenge is that, even though GPTs are capable of producing precise predictions, they can be challenging to interpret, which can present a problem for financial institutions seeking to comprehend the reasoning behind specific predictions <cit.>. This lack of interpretability can harm risk management objectives.Implementing GPT in finance sector can be vulnerable to adversarial attacks, which are designed to manipulate the model's output by injecting false data. This can be particularly problematic for financial institutions that rely on GPTs for risk management and investment decisions. It also require large amounts of training data to achieve high accuracy. However, in some cases, financial institutions may not have access to sufficient data to train the model effectively. GPTs can also be biased if the training data used to develop the model is biased. This can lead to inaccurate predictions and unintended consequences.\n",
            "\n",
            "\n",
            "\n",
            "  §.§.§ Summary\n",
            "\n",
            "The use of GPTs in the finance industry has promising benefits such as improved risk management, enhanced fraud detection, better portfolio management decisions, and increased efficiency. However, it also has potential challenges that need to be addressed, such as high computational requirements, the complexity of implementation, limited interpretability, vulnerability to adversarial attacks, limited training data, and bias in training data. So, the use of GPTs in the finance industry presents significant benefits but also requires careful consideration of the challenges involved to ensure the effective and secure deployment of these models.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Summary On Impact of GPT models in Applications\n",
            "\n",
            "The impact of GPTs in various applications and challenges was highlighted. GPT  with its varied usage has changed the way people perceive facts such as content creation, enhanced user interfaces, personalized learning, item tracking, self-awareness, market risk analysis, business forecasts and introspection. However, there are concerns about the potential negative impact of GPTs, such as the spread of fake news, bias in data and decision-making, not domain specific, ethical issues, data reliability, the complexity of implementation, multimodal and multilingual support, security and privacy concerns, vulnerable to data attacks, limited input data, explainability of results, large model size, high computational requirements and job loss. Despite these concerns, it is clear that GPTs will continue to be a powerful tool for industries seeking to leverage the power of NLP and generative AI. As the technology improves and new applications emerge, it will be interesting to see how GPTs continue to shape the future of industries around the world. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "§ PROJECTS\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Project Summary Table.\n",
            "\n",
            "!\n",
            "\n",
            "[HTML]EFEFEF \n",
            "Project    \n",
            "  DeepScribe    \n",
            "  Meena    \n",
            "  Jukebox    \n",
            "  Uber's plato research dialogue   system    \n",
            "  Polyglot AI    \n",
            "  SiriGPT \n",
            " Application widely used for    \n",
            "  Healthcare    \n",
            "  Lifestyle    \n",
            "  Entertainment    \n",
            "  Transport    \n",
            "  Education    \n",
            "  Lifestyle \n",
            " Purpose    \n",
            "  Medical documentation and to improve doctor-patient association    \n",
            "  Personalized product recommendation    \n",
            "  Enables the original music    creation both artistically compelling and commercially viable in a   variety of styles and genres    \n",
            "  Enhances user experience using Uber rides, helps drivers and riders in   scheduling rides, navigating routes, providing real-time updates on traffic   and weather conditions.    \n",
            "  enables absolute communication irrespective of the language barrier   across  different regions and   cross-culturalism    \n",
            "  Assist with voice-based assistants \n",
            " GPT Adoption    \n",
            "  Customized version of GPT's    \n",
            "  Google's seq2seq transformer-based neural network architecture similar to   Open AI's GPT    \n",
            "  GPT-2 extension called \"Multi-Scale Transformers for Music   Modeling\" (MST) model    \n",
            "  GPT-2    \n",
            "  GPT-0, GPT-1,GPT-2,GPT-3    \n",
            "  GPT-3 \n",
            " Dataset    \n",
            "  Not Disclosed    \n",
            "  Meena dataset over 40 billion words , 341 GB captured from public domains   like Reddit and social media platforms    \n",
            "  1.2 million songs, 600,000 pieces of sheet   music, 45,000 MIDI files    \n",
            "  Persona-Chat with 160,000 conversational dialogues, Cornell Movie-Dialogs   Corpus with 200,000 movie conversation, DailyDialog over 13,000 dialogues,   and    \n",
            "  CONLL-2003, Sentiment140 dataset, Reuters Corpus, 20 Newsgroups dataset,   WMT (Workshop on Machine Translation) datasets and SQuAD (Stanford Question   Answering Dataset)    \n",
            "  Information not publisized \n",
            " Building Blocks    \n",
            "  Recurrent Neural Network and Attention mechanism fueled by NLP techniques    \n",
            "  Seq2Seq Transformer-based Architecture    \n",
            "  Transformer-based Language Model and Autoregressive model    \n",
            "  Language modeling, Dialogue modeling, Discrete latent variabe modeling   and response ranking    \n",
            "  Language Identification, Named Entity Recognition (NER), Sentiment   Analysis, Text Classification, Machine Translation, Question Answering    \n",
            "  Transformer-based neural network architecture \n",
            " Evaluation Metrics    \n",
            "  Bleu score, perplexity    \n",
            "  Bleu score, perplexity    \n",
            "  Frechet Audio Distance (FAD) and Pitch and Rhythm Similarity    \n",
            "  Bleu score, Perplexity and Distinct n-gram    \n",
            "  accuracy, precision, recall, , F1-score, Bleu score as well as   cross-entropy loss or perplexity    \n",
            "  Perplexity, BLEU score, F1 score, ROUGE score, Human evaluation \n",
            " Addressed Challenges    \n",
            "  Reduced Transcription errors and enhanced patient care    \n",
            "  Natural and Engaging conversations    \n",
            "  Fresh orginical music content creation and drastically reducing the cost   and time by creating high-quality music contents, and also to preserve and   advance musical heritage.    \n",
            "  customer service, user experience, and operational efficiency    \n",
            "  Multilingualism and Sentiment Analysis are the key challenges in NLP and   Polyglot AI solved this problem by offering a tool for supporting morethan 40   languages and pre-trained sentiment analysis model    \n",
            "  Language understanding and generation, Data scarcity, Contextual   understanding, Text summarization, Sentiment analysis, Named entity   recognition \n",
            " Input data    \n",
            "  Audio    \n",
            "  Text    \n",
            "  Audio    \n",
            "  Text    \n",
            "  Text    \n",
            "  Audio \n",
            " Owned By    \n",
            "  DeepScribe    \n",
            "  Google    \n",
            "  OpenAI    \n",
            "  Uber    \n",
            "  Uizard Technologies    \n",
            "  Apple \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Project Summary Table (continued).\n",
            "\n",
            "!\n",
            "\n",
            "[HTML]EFEFEF \n",
            "Project    \n",
            "  AI Dungeon    \n",
            "  Copy.ai    \n",
            "  Bond AI    \n",
            "  Viable    \n",
            "  AI Channels    \n",
            "  Fireflies.ai \n",
            " Application widely used for    \n",
            "  Gaming    \n",
            "  Business and marketing    \n",
            "  Finance    \n",
            "  Business Analytics    \n",
            "  AI Industry    \n",
            "  Business \n",
            " Purpose    \n",
            "  Interactive and engaging storytelling experience for players    \n",
            "  help clients create written content more quickly and easily    \n",
            "  To enhance the financial well-being of clients    \n",
            "  provide businesses with intelligent insights to help them make better   decisions    \n",
            "  provide a platform for developers, data scientists, and machine learning   practitioners to create, deploy, and manage their AI models    \n",
            "  to simplify the meeting process and reduce the time and energy required   for note-taking and collaboration \n",
            " GPT Adoption    \n",
            "  GPT-3    \n",
            "  GPT-3    \n",
            "  GPT-3    \n",
            "  GPT-4    \n",
            "  GPT-3    \n",
            "  GPT-4 \n",
            " Dataset    \n",
            "  Common Crawl, OpenAI GPT-2, and various text datasets from Kaggle    \n",
            "  books, articles, and websites    \n",
            "  likely use of a combination of publicly available financial datasets,   proprietary data, and client data    \n",
            "  Information not publisized    \n",
            "  Users' own dataset    \n",
            "  Possible datasets: the Common Voice dataset from Mozilla   having over 9,000 hours of speech data in multiple languages \n",
            " Building Blocks    \n",
            "  Machine Learning Models, Text Input Interface, Game Engine, Content   Database, Player Feedback System, Cloud Infrastructure    \n",
            "  NLP, Language Models, Neural Networks    \n",
            "  NLP, Personalization, Conversational User Interface, Data Analytics    \n",
            "  Unsupervised learning, Contextual understanding, Sentiment analysis,   Topic modeling, Entity recognition    \n",
            "  Pre-built models, Model training, Data preparation, Collaboration    \n",
            "  Speech-to-Text Technology, NLP, Cloud Computing, Integration technologies \n",
            " Evaluation Metrics    \n",
            "  Response Coherence, Response Diversity, Player Satisfaction, Engagement,   Realism, Novelty    \n",
            "  Perplexity, BLEU score, ROUGE score, F1 score    \n",
            "  Intent recognition accuracy, entity extraction accuracy, and language   model perplexity    \n",
            "  Perplexity, Accuracy, F1 score, Word similarity    \n",
            "  Accuracy, Precision and Recall, F1 Score, Perplexity, User satisfaction    \n",
            "  Speech Recognition Accuracy, NLP Performance, Integration Performance,   Task Completion Time, User Satisfaction \n",
            " Addressed Challenges    \n",
            "  Narrative Generation, Content Creation, Personalization, Replayability,   Accessibility, Creative Expression    \n",
            "  Lack of writing skills, Inconsistency, Multilingual content creation    \n",
            "  Personal financial management, Customer engagement, Fraud detection and   prevention    \n",
            "  Understanding unstructured data, Contextual understanding, Visualization   and exploration of data, Customization and integration    \n",
            "  Natural language understanding, Scalability, Personalization, Integration   with other systems, Maintenance and updates    \n",
            "  Time-consuming manual note-taking, Difficulty in capturing important   details, Lack of visibility and accountability, Communication barriers \n",
            " Input data    \n",
            "  Text    \n",
            "  Text    \n",
            "  Audio and Text    \n",
            "  Text    \n",
            "  Text    \n",
            "  Audio \n",
            " Owned By    \n",
            "  Latitude    \n",
            "  Copy.ai    \n",
            "  Bond.AI    \n",
            "  Viable AI    \n",
            "  MiroMind AG    \n",
            "  Fireflies AI \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "This section presents the exciting projects developed using GPT model technologies for different applications mentioned in the above sections. Table. <ref>, Table. <ref> shows the different levels of such projects along with different parameters to compare their characteristics leveraging the capabilities in many real-life applications.\n",
            "\n",
            "\n",
            "\n",
            " §.§ SiriGPT\n",
            "  \n",
            "Siri <cit.> is an intelligent digital assistant that enables Apple device users to complete tasks more efficiently and with ease, often anticipating their needs even before they make requests. SiriGPT  <cit.> <cit.> is a voice assistant powered by a GPT model and developed entirely using shortcuts. Apple device users can utilize ChatGPT, fueled by GPT-3, by using an API key provided by OpenAI. This novel combination offers the best of both worlds, allowing users to utilize SiriGPT for voice commands and ChatGPT for generating text. SiriGPT utilizes a tokenizer exclusively developed by Apple that has been optimized for processing natural language tasks. SiriGPT's training data is not publicly available as it is exclusive to Apple. However, the language model is trained on diverse text data from various sources such as books, news articles, web pages, and other text data sources. This ensures that SiriGPT can handle different natural language tasks accurately and efficiently. It has been reported that SiriGPT is one of the largest language models available, with over a trillion parameters.\n",
            "\n",
            "\n",
            "\n",
            " §.§ AI Dungeon\n",
            " \n",
            "Latitude, a startup based in Utah, created a groundbreaking online game called AI Dungeon <cit.>, which showcased a novel type of collaboration between humans and machines. It is a  free-to-play, single-player, and multiplayer adventure game that caught traction within the gaming community. It combines fantasy and AI to create endless possibilities, e.g., one can take charge of a military operation to defend against aliens or become a famous detective investigating an attempted murder of the queen of the fairies. Unlike games with predetermined storylines, AI Dungeon allows you to guide the AI to generate unique characters and scenarios for your character to interact with. The game boasted about incorporating the GPT-3 text generator, but then the algorithm began producing unsettling narratives, including graphic depictions of sexual encounters involving minors <cit.>.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Copy.ai\n",
            "  \n",
            "\n",
            "Copy.ai <cit.> is a mighty AI startup founded by Paul Yacoubian in 2020. This project is created using GPT-3, mainly targeting business and marketing campaigns. It has the following use cases: (i) For Teams: It assists with producing customized sales copy, composing long-form articles and pages on a large scale, reusing content on various platforms, and creating product descriptions; (ii) For Emails: The AI-powered email writer takes care of the most challenging parts of marketing by creating email campaigns that are highly effective at converting leads, all with just a few clicks of a button; (iii) For Blogs: By generating content briefs and crafting one-of-a-kind SEO-focused blog articles every month, it can save a significant amount of money for the business. In addition, it's feasible to create briefs, outlines, and even initial drafts in mere minutes, which can be utilized as an excellent source of inspiration for writers to create high-quality content; (iv) Social Media: It aids in generating social media posts quickly and efficiently, allowing for a rapid expansion of the social media following. Additionally, Copy.ai includes a suite of other tools, such as a headline analyzer, a language translator, and a content rephrase. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Bond.AI\n",
            " \n",
            "\n",
            "Bond.AI <cit.> is a company focused on AI for financial institutions, which has a headquarters in Little Rock, Arkansas. It was established by Uday Akkarajuin in 2016 and prided itself on providing AI technology centred around human needs. This innovative project offers a product named BondBot, which is powered by Empathy Engine 3.0 and ChatGPT, to enhance the financial health of clients. It assists financial institutions and employers in promoting interconnected finance by offering various tools to improve the institution's profitability and the financial health of its clients on a single network. It uses customer data to create individual personas for every bank customer or small business, considering their behaviours, strengths, and potential needs. This approach enables the platform to develop multiple customized pathways to holistically enhance clients' financial well-being.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Viable\n",
            " \n",
            "Viable <cit.> is a platform powered by GPT-4 that utilizes the latest advancements in NLP and AI to offer businesses intelligent insights to aid their decision-making processes. Companies can extract actionable insights from unstructured data sources, such as social media posts, customer reviews, and survey responses, by employing Viable. GPT assists in comprehending the sentiment and context behind the data, resulting in valuable insights that can enhance a company's services, products, and customer experience. Viable's \"Insight Explorer\" is a distinctive feature that enables users to interact with and visualize their data via a user-friendly interface. In addition, the platform offers advanced analytics capabilities, including entity recognition, topic modelling, and sentiment analysis. The GPT-based technology of Viable is continually evolving and advancing, which allows the platform to deliver more precise and insightful data. Moreover, Viable provides customized integration and solutions to cater to the specific requirements of each business.\n",
            "\n",
            "\n",
            "\n",
            " §.§ AI Channels\n",
            "  \n",
            "AI Channels <cit.> is a platform that provides a comprehensive set of tools for developers, data scientists, and machine learning practitioners to develop, launch, and manage their AI models. The platform offers an all-in-one solution for creating personalized AI models, starting from data preparation and model training to deployment and monitoring. Users can train their models on their data or, on pre-trained models provided by AI Channels. These models can be deployed as APIs or Docker containers on various infrastructures, including different cloud platforms. It also provides a dashboard for tracking model performance and managing configurations. It covers various use cases, including computer vision, NLP, and speech recognition. The platform includes pre-built models for tasks such as image and text classification, object detection, and sentiment analysis. Additionally, users can create their models using popular frameworks. The main objective of AI Channels is to make building and launching AI models more accessible to developers and businesses without specialized AI skills. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Fireflies.ai\n",
            " \n",
            "Fireflies AI <cit.> is a privately held company based in San Francisco, California, founded by Krish Ramineni and Sam Udotong. Fireflies AI  software is powered by GPT-4 to automate notes-taking tasks and collaborations during meetings. It is compatible with various video conferencing platforms, including Zoom, Google Meet, and Microsoft Teams, and it can transcribe meeting audio and video content in real time. Its primary function is based on speech-to-text technology, which enables it to generate a searchable transcript of the meeting, which can be used for later review and to recall essential points and action items. Additionally, the software utilizes NLP capabilities that can identify significant keywords and phrases within the conversation. Apart from note-taking, Fireflies AI includes collaboration tools such as assigning tasks and sharing notes with other team members. It can integrate with project management and task tracking tools to automatically generate tasks based on the identified action items during the meeting. Fireflies AI provides several customization options to suit particular use cases and workflows. Users can configure the software to automatically join specific meetings or capture audio only from specific speakers. It allows users to specify particular words and phrases to highlight in the transcript, making it easier to identify critical points during the later review. Thus, Fireflies AI aims to simplify the meeting process and reduce the time and energy required for note-taking and collaboration.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Uber's Plato Research Dialogue System\n",
            "   \n",
            "\n",
            "Uber's AI Lab introduced Uber's Plato Research Dialogue System in 2020 developed by a team of researchers and engineers to enable the intelligence in riding experience. PLATO - Pre-trained Dialogue Generation Model with Discrete Latent Variable <cit.>. Uber's Plato Research Dialogue System uses GPT-2, a large-scale language model developed by OpenAI in 2019. Uber's Plato Research Dialogue System project used several datasets to train and evaluate their conversational agents such as Persona-Chat contains 160,000 conversational dialogues, Cornell Movie-Dialogs Corpus with 200,000 movie conversations, DailyDialog over 13,000 dialogues, and EmpatheticDialogues over 25,000 user dialogues. The main components in developing the GPT-powered PLATO project are\n",
            "language modelling, dialogue modelling, discrete latent variable modelling and response ranking. The Plato Research Dialogue System was trained on a massive corpus of text data consisting of over 40 GB of uncompressed text while Bleu score, Perplexity and Distinct n-gram are the evaluation metrics used for training and testing the PLATO project. Uber's AI PLATO has addressed many key challenges like customer service by personalizing user feedback with conversational AI agent, user experience using the Uber platform for scheduling rides, navigating routes, and providing real-time updates, and increasing operational efficiency by reducing the need for human customer service representatives and enabling faster and more accurate communication between riders, drivers, and the Uber app. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Jukebox\n",
            "    \n",
            "\n",
            "Jukebox, a GPT-powered music creation, was developed in 2020 as an extension of Open AI's GPT language model <cit.>. Jukebox's goal is to push the boundaries of what AI can accomplish in the world of music creation and to investigate fresh applications for AI. A variation of the GPT architecture, the \"Multi-Scale Transformers for Music Modeling\" (MST) model, was created specifically to handle the intricate and multi-scale nature of musical data. Additionally, Jukebox can produce lyrics that match the music's tone and style. A sizable and varied dataset of musical recordings, lyrics, and related metadata was used to train Jukebox such as 1.2 million songs sourced including Lakh MIDI Dataset, Free Music Archive, Spotify and Tidal, 600,000 pieces of sheet music were sourced from IMSLP (International Music Score Library Project), and 45,000 MIDI files from Lakh MIDI Dataset and the MIDIworld collection. Faster training times and more effective use of computational resources were made possible by the distributed computing setup with 2048 TPU( Tensor Processing Unit) cores used to train the Jukebox model. Training the model required significant computational resources demanding faster training times by the distributed computing setup with 2048 TPU  (Tensor Processing Unit) cores used to train the Jukebox model. A combination of subjective and objective metrics was used to assess and test Jukebox. In a large-scale subjective assessment, more than 1,000 participants listened and rated each one individually determining the overall score for each song produced. On the other side, objective assessments were conducted by evaluating Frechet Audio Distance (FAD) and Pitch and Rhythm Similarity. Overall, Jukebox revolutionizes with its significant advancement in the music industry through creative inspiration, music production, music education and preservation of music heritage.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " §.§ Meena\n",
            "   \n",
            "\n",
            "Google's Meena project was developed by Google Research Team in 2020 for providing personalized product recommendations <cit.>. The primary goals of the Meena project empowered the lifestyle sector to enhance the user experience and customer service by recommending goods and services on a personalized basis. The project designed a GPT using the seq2seq transformer-based neural network architecture, in particular for open-domain conversational agents. The architecture was pre-trained over 341 GB of text captured from Reddit and other social platforms containing over 40 million words and called this massive collection as 'Meena Dataset'. Meena was tested using the automated performance metrics known as Bleu score and perplexity on a cluster of HPC nodes with a total of 2048 NVIDIA V100 GPUs. One of the biggest challenges solved Meena was building trust and generating reliable engaging human-like conservation that typically enhances user satisfaction and personalization. Meena has achieved state-of-the-art performance compared to other open-domain chatbots and revolutionized the wide range of applications in the lifestyle industry and a way beyond by providing natural and engaging responses through virtual assistants, customer service bots and personal shoppers.\n",
            "\n",
            "\n",
            "\n",
            " §.§ DeepScribe\n",
            "\n",
            "\n",
            "DeepScribe was a GPT-based medical project developed in 2019 by the student team at the University of California by partnering with giant US-based healthcare providers such as One Medical, Stanford Medicine, Mount Sinai and Sutter Health <cit.>. The DeepScribe's technology aims at transcribing medical conversation allowing doctors to treat the patients rather than noting down the patient's history, enhancing the doctor-patient relationship and targeting the overall quality of patient care. Although DeepScribe used the customized variants of Open AI, the technical details of the GPT model used for customizing the model were not disclosed which was optimized for medical transcription tasks. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Polyglot AI\n",
            "\n",
            "Polyglot AI is a communication platform designed to generate text in multiple languages and process the data by performing several tasks such as advanced NLP techniques, text translation, and sentiment analysis. The potential features of Polyglot AI have been exploited in the following application areas such as language translation, chatbots, language learning tools, content creation, customer support, and data analysis across different languages and regions. Polyglot AI is built based on different variants of GPT models, and state-of-the-art language model architecture for NLP tasks, which uses the self-supervised learning approach. \n",
            "\n",
            "The Polyglot AI was pre-trained using a large amount of textual data on multiple languages simultaneously in an unsupervised environment using a shared architecture, Multilingual Universal Sentence Encoder (MUSE). MUSE developed by Google, is a pre-trained DL model used for cross-lingual TL, that encodes the text into common vector space for multiple languages. Thus, the Polyglot language model was created with the following pre-training techniques as Masked Language Modeling (MLM), Translation Modeling Language (TML), sequence-to-sequence modelling and cross-lingual TL. The pre-trained language model is fine-tuned and evaluated by standard benchmarks and metrics such as the BLEU score (Bilingual Evaluation Understudy), METEOR (Metric for Evaluation of Translation with Explicit ORdering) or F1-score. Remarkably, Facebook used new Polyglot AI to translate between 100 languages <cit.>. Thus Polyglot AI enables absolute communication irrespective of the language barrier across different regions and cross-culturalism.\n",
            "\n",
            "\n",
            "Thus, this section focused on several exciting real-life projects which are developed and used for humankind. These projects were discussed by presenting Table <ref> highlighting the details of the project with model architecture, datasets used, training and testing, and evaluation metrics involved with the challenges addressed. The next section will discuss the open research issues and future directions for the potential benefits of GPT models.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "§ OPEN RESEARCH ISSUES AND FUTURE DIRECTIONS\n",
            "\n",
            "This section highlights the various open research issues concerned with the implementation and adoption of sustainable GPT models. It also provides insights into future research directions for the betterment of researchers in the field of GPT development. Fig. <ref> outlines the many issues that can develop while using GPT models, as well as the various future approaches that need to be considered for the effective usage of GPT models. \n",
            "\n",
            "\n",
            "\t\n",
            "    < g r a p h i c s >\n",
            "\n",
            "\tChallenges and Future Directions.\n",
            "\t\n",
            "\n",
            "\n",
            "\n",
            " §.§ Domain Specific GPT models\n",
            "\n",
            "Domain-specific GPT models are mandated in almost all applications; developing these models is still challenging and an open issue within GPT. While the current GPT models have been developed to understand natural language and generate content effectively, their performance may not be equally effective when handling specific domains, such as medicine, agriculture, etc. One of the key challenges in adapting to a particular domain is the availability of domain-specific data. It is well known that the performance of GPTs is directly proportional to the quality and quantity of data used for training the model. So, obtaining such quality data for a specific domain is expensive and time-consuming, as the data are heterogeneous. Also, these data accumulations may even make these models much larger, sometimes catastrophic too, leading to forgetting the knowledge attained during the process. To overcome this issue, pre-training tasks and domain-specific model generation are integrated by data augmentation<cit.>.  Another challenge is fine-tuning the model to accustom to the unique characteristics and vocabulary of the domain. A few domain-specific GPT models have been developed and implemented despite these challenges. There is a growing interest in creating more domain-specific GPTs for various domains. Moreover, these models will be trained using the knowledge acquired from large language models specific to domains. Therefore, these models can be fine-tuned for specific tasks or domain-specific requirements with gradually improving performance. GPT models have the potential to be trained in any context, and researchers are exploring new approaches and methods to address these challenges. Furthermore, these models will be more efficient, enhanced interpretability, and domain generability than the existing Large language models as they are customized to specific domain concerns and can provide more concise and informative solutions. TL can be used for developing domain-specific GPT models. Domain-specific GPT models were developed to summarize products based on customer reviews on an E-commerce site, where the language model is pre-trained on the Chinese-short summarization dataset and has obtained fine-tuned results<cit.>. Besides these challenges, domain-specific models require higher computation costs for the resources and time spent in pre-training and relearning in downstream tasks during fine-tuning of pre-trained domain-specific models. Therefore, domain-specific model development must focus on optimizing resource consumption and fine-tuning the pre-trained model to alleviate the forgetting problem involved in existing models. \n",
            "\n",
            "\n",
            "\n",
            " §.§ High Computational requirements\n",
            "\n",
            " As the Transformer model utilizes varied heterogeneous datasets for training and learning from the knowledge acquired, one of the key challenges of GPT models is high computational resources for pre-training and inference. The computational requirement continuously increases as the models become more complex and larger. Depending on the size and complexity of the model and the available resources, the time required to train the model can take days, weeks, or even months. Moreover, the inference time for these models is typically slower, making it challenging to use them for real-time applications. This poses a significant obstacle to adopting GPT models for many practical applications. Despite these challenges, significant efforts are underway to overcome them. To accommodate the increasing data size and pre-training computational requirement, data enhancement-based GPT models were developed<cit.> by joining the downstream tasks and pertaining process by reconstructing the domain-specific text before proceeding for pre-training and utilizing the empirical knowledge rather than learning for falsy domain data. Researchers are exploring various ways to optimize and speed up the training and inference process, such as using specialized GPUs and TPUs. They are also developing more efficient algorithms and attempting to reduce the model size without sacrificing performance. In addition, ChatGPT has evolved to include plugins <cit.> that enable statistical analysis for real-time applications. By integrating these plugins with the help of third-party services, ChatGPT can now be used for analyzing real-time applications as well.\n",
            " \n",
            "\n",
            "  §.§.§ Increasing Model size and Space Constraints\n",
            "\n",
            "Developing and training large language models, such as a GPT model, can be a challenging task due to significant technical and computational difficulties as discussed. The size of GPT models presents a major challenge, as the computational resources required for training and inference increase with the number of parameters that need to be trained. As the model size increases, it also requires more memory to store and manipulate parameters during training and inference<cit.>. Acquiring and processing vast amounts of high-quality training data is another challenge in training large language models like GPT. For instance, GPT-4, which is the largest GPT model to date with 1 trillion parameters, demands a massive amount of computational resources, such as specialized hardware like GPUs and TPUs, spatial requirements, and high-speed network connections to transfer data between different parts of the system. Model evaluation and interpretation are also critical challenges. Since large language models like GPT are trained on a massive scale, understanding how the model makes predictions and why it generates specific outputs is difficult. Evaluating the quality and accuracy of the model's output and identifying and addressing biases or errors in its performance can also be challenging.\n",
            "\n",
            "As these and other significant efforts continue, we can expect the challenge of computational resource requirements for GPT models to transform into a strength in the future.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Explainability and interpretability\n",
            "\n",
            "Explainability and interpretability are currently major challenges for GPTs for specific applications. Explainability refers to providing a clear and understandable explanation of how the model has arrived at any output. Interpretability, on the other hand, refers to the ability to understand the internal processes of the model. GPT models are highly complex and difficult to understand and interpret due to their size and architecture. The outcomes and decisions of the model are based on previous learning and training, and the models learn from vast amounts of data to make decisions. These decisions may not be easily explainable to humans. This lack of transparency and interpretability raises concerns about the reliability and safety of the model, particularly in critical applications such as healthcare and finance. Researchers are currently conducting much research to make GPT models more explainable and interpretable <cit.> by utilizing EXplainable Artificial Intelligence (XAI) to provide explanations for the decisions arrived at, specifically to different users at stake. As well, XAI models enable interpretability by providing detailed explanations for the internal process.  As GPT can generate any type of unconstrained output for instance code generation for the given problem, it requires proper justifications and explanations for the output. So, to assure these codes by GPT are reliable, a metric model to evaluate and validate this GPT code was developed using NLP metrics and XAI for model interpretability<cit.>. Also, some domain-specific GPT models of GPT-3 have evolved with solutions <cit.> <cit.> to ensure that the GPT model's decisions are understandable, explainable, and trustworthy enough to be used for critical applications like healthcare and finance.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Data Bias\n",
            "\n",
            "Data bias is an open issue concerned with the adoption of any advancements in AI, till GPT<cit.>. This is also a prominent challenge for GPT and other machine-learning models. It refers to patterns or relationships in the data that do not accurately reflect the true distribution of the target population or domain. GPT models are trained on vast amounts of text data which may contain bias in language use or cultural assumptions. Still, the source of data remains undeclared, considering GPTs are trained using internet data which may have faulty, fake, and error data, GPTs may generate biased texts or information imitating the training data<cit.>. Such biases can be amplified in the model's output, resulting in false or unfair results. Data bias can arise from various sources, such as selection bias, labelling bias, concept drift, confounding variables, and changes in input data distribution over time. For example, suppose a dataset used to train a GPT model is dominated by a particular demographic group. In that case, the resulting model may be biased in its predictions towards that group, leading to inaccurate or unfair predictions when applied to new data. This bias can have serious consequences, especially in healthcare, finance, and law enforcement, where biased results can significantly impact human lives. To mitigate these issues, researchers have developed strategies such as diversifying the training data, debiasing the training data, modifying the model architecture, and using post-processing methods to normalize the data and create more fair and inclusive GPT models. The authors in <cit.> have made an in-depth analysis of the most downloaded text generation model GPT2. By examining the intersections of gender with religion, sexuality, ethnicity, political affiliation, and continental name origin, the authors evaluated prejudices associated with occupational associations among various protected categories. These biases may have inaccuracies in climatic data prediction or global warming<cit.>. Therefore, data bias must be of greater concern in GPT model development as the data quality of the internet is limited to avoiding producing disturbing content.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Multimodal support\n",
            "\n",
            "\n",
            "The challenge of developing multimodal learning ability in the GPT model remains unsolved. Multimodal support refers to the GPT model's ability to process and generate text along with other modalities, such as audio, images, and videos. GPT models have shown impressive results in generating high-quality text and NLP tasks, but it was primarily designed for text-based tasks and cannot handle other modalities. However, due to its success in text processing, users expect its integration with other modalities, such as speech recognition, video summarization, and image or video captioning<cit.>. Several research initiatives have been proposed to integrate multimodal support to address this issue. One approach is to feed the visual and audio information with the corresponding text to the model as input. The other is to handle this input modality process as a separate model and use the output as input to GPT. Multimodal video captioning is done using GPT in the unlabelled videos<cit.>. Multimodel learning has been applied for information retrieval<cit.> and image generation for illustrating the news<cit.> to assist the GPTs. However, the primary challenge in both approaches is effective integration, requiring architectural changes and techniques to handle various modalities. Recently, OpenAI's GPT4 has launched with multimodal support, enabling it to read images, analyze the input, and generate text as output. It cannot create images as output, though. Nevertheless, the field of multimodal processing is still an active area of research, and much work must be done to effectively and efficiently process and understand multimodal data. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Robustness\n",
            "\n",
            "\n",
            "The robustness is a major requirement to be imposed by any type of GPT model, and it is a global problem for all learning-based prediction technologies. Robustness refers to the ability of the model to maintain high performance and accuracy even in the face of unexpected or adversarial inputs. Although GPT models have shown impressive performance in a wide range of NLP applications and have set a benchmark for high-quality text generation, they are still vulnerable to certain types of errors and attacks. In particular, handling adversarial inputs is a challenging task in GPT models. GPT models are particularly susceptible to adversarial attacks<cit.>. Adversarial inputs are specifically designed to make a learning model collapse and misbehave. GPT models can be highly prone to these attacks because they are trained on a large volume of text. As a result, they may be influenced by subtle patterns or biases in the training data. If such biases or patterns exist in the data, the GPT model may amplify or perpetuate existing biases, leading to unfair outcomes. A few techniques may be used, such as adversarial training <cit.><cit.>, defensive distillation <cit.>, and regularization techniques <cit.> such as dropout, weight decay, and batch normalization, to mitigate and handle adversarial inputs. Therefore, GPT development must focus on developing models with more robustness, enabling them to be tolerant of various vulnerabilities, and thus to be used reliably and susceptible in a wide range of applications.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Multilingual support\n",
            "\n",
            "While GPT models have demonstrated remarkable proficiency in NLP tasks for individual languages, achieving multilingual support remains a significant challenge. The primary difficulty in developing multilingual GPT models lies in the significant differences in syntax, grammar, and vocabulary across various languages. As the number of internet users day by day increasing irrespective of literacy rate, multilingual support will target all types of end users. To create models that can effectively process multiple languages, researchers need to train GPT models on extensive, diverse datasets that span a broad range of languages and language families. Additionally, designing language-specific pre-processing techniques to prepare input data for the model is another obstacle to overcome. Various languages possess distinct writing systems, word orders, and linguistic features, necessitating specialized pre-processing techniques to ensure that the model can process the input data effectively. Despite the challenges, researchers continue to explore new methods to improve the multilingual capabilities of GPT models. Some techniques involve training separate models for each language or developing language-specific fine-tuning techniques. Others include developing cross-lingual TL techniques that allow the model to transfer knowledge and skills learned in one language to another. \n",
            "\n",
            "\n",
            "\n",
            " §.§ Limited understanding\n",
            "\n",
            "\n",
            "GPT models have a limited understanding of context and meaning, despite their ability to generate coherent text. This problem arises due to issues such as a lack of semantic understanding, bias, stereotyping, and handling nuances and figurative language. As a result, the outputs generated by the model may contain errors or inaccuracies, even if they are grammatically correct. Researchers are exploring various techniques to enhance the model's contextual understanding. Understanding GPTs will be more reactive and may attract more users for accurate results<cit.>. These methods include incorporating external knowledge sources like knowledge graphs and ontologies into the training process, developing common sense reasoning capabilities, and improving the model's ability to handle nuances and idiomatic expressions. By enhancing the contextual understanding of GPT models, their outputs will be more accurate, relatable, sequential, less biased, and more useful for a variety of applications.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Ethical Concerns\n",
            "\n",
            "The ethical concerns in GPT models are an active area of discussion and debate due to the potential negative impacts that the use of GPTs could have on society. Although GPT models have demonstrated remarkable abilities in generating coherent and realistic text, there are concerns about the perpetuation of biases and stereotypes, the possibility of malicious use, and the effects on employment and economic inequality. Some of the ethical characteristics to be possessed by GPT include functional Morality, operational morality, abiding by the right for explanation law, improved transparency with human involvement, unbiased data, and adhering to government regulations on data usage<cit.>.  The responsibility of developers and companies to address these ethical concerns and ensure the ethical use of a GPT model is also a topic of debate. The ethical implications of GPT models are being actively researched and discussed in the fields of AI, computer science, and philosophy.\n",
            "\n",
            "\n",
            "\n",
            " §.§ Security and privacy concerns\n",
            "\n",
            "GPT models raise concerns about security and privacy, particularly as they become more widespread. One of the main concerns is that GPT could be used for harmful purposes, such as creating fake news or deep fakes, as it can generate text that looks real and convincing, making it difficult to distinguish between genuine and fake content. Another concern is the potential for privacy violations when using a GPT model. Large language models like GPT require a significant amount of training data, which could contain sensitive or personal information. This raises concerns about privacy and data protection as per European Union's General Data Protection Act<cit.>, particularly if the training data is not properly anonymized or if the models are used to generate text based on user data without their explicit consent. Some of the problems concerned with confidentiality related to the pre-training dataset are Data tracing, Membership Inference Attacks, reconstruction attacks, and property inference attacks and the vulnerabilities concerned with a model encoder are hyperparameter stealing attacks and encoder parameter stealing attacks. Poisoning, Backdoor, and evasion attacks are the vulnerability related to the integrity of self-supervised learning. Resource depletion attack is one major issue with data availability, which may lead to tremendous effects incorrect results, and may cause greater deviations too<cit.>.   Additionally, the GPT model's ability to generate text based on user input could inadvertently disclose sensitive information, such as personal or financial details, or trade secrets. This could happen if a GPT model is used in an insecure environment or if it is targeted by malicious actors seeking to obtain sensitive information. Researchers and developers should focus on assuring authenticity in using users' data in case of interactive information generation based on privacy data shared. These include using differential privacy to protect training data privacy <cit.>, implementing secure hardware or software protocols to protect models from cyberattacks, and developing techniques to detect and prevent the malicious use of GPT models. It's crucial to adopt and follow these measures to ensure the ethical and safe use of GPT models before using them in various applications.\n",
            "\n",
            "Therefore, GPT model development must focus on developing more robust, reliable, safest, multi-lingual, multimodal support-enabled solutions for delivering domain-specific or human-specific solutions with optimal resource utilization.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Various lessons learned and future research directions.\n",
            "\n",
            "!\n",
            "\n",
            "Sl.No    \n",
            "  Lessons Learned    \n",
            "  Open Issues    \n",
            "  Future Directions \n",
            " 1.    \n",
            "  ∘ Huge volume of data usage is critical\n",
            "    \n",
            "    \n",
            "  ∘ Data privacy - may unknowingly reveal sensitive information\n",
            "  \n",
            "  ∘ Varied data quality - Inconsistency in quality of data used for training \n",
            "  \n",
            "  ∘ Scalability - Models should be able to handle an increase in data set size and complexity    \n",
            "  ∘ Optimized architecture and algorithms \n",
            "  \n",
            "  ∘ Cloud-based computing\n",
            "  \n",
            "  ∘ Hardware advancements\n",
            "  \n",
            "   \n",
            " 2.   \n",
            "  ∘ Importance of Proper Pre-processing of data \n",
            "  \n",
            "     \n",
            "  \n",
            "  ∘ Data bias - Overrepresentation of certain groups or perspectives \n",
            "  \n",
            "  ∘ Poor model performance \n",
            "  \n",
            "  ∘ Reduced efficiency of the model \n",
            "     \n",
            "  ∘ Continuous monitoring\n",
            "  \n",
            "  ∘ Testing model for potential biases\n",
            "  \n",
            "  ∘ Diversifying the training data\n",
            "  \n",
            "  \n",
            " \n",
            "\n",
            "  3.    \n",
            "  ∘ Importance of explainability and interpretability\n",
            "    \n",
            "    \n",
            " ∘ Complexity of models\n",
            "  \n",
            " ∘ Inability to explain predictions\n",
            " \n",
            " ∘ Need of User-tailored Explanations generation\n",
            " \n",
            " ∘ Developing Interpretable models\n",
            " \n",
            "  ∘ Lack of transparency in the data source\n",
            "  \n",
            "     \n",
            "\n",
            "  ∘ AI governance models can be used\n",
            "  \n",
            "  ∘ Model Summaries can be provided\n",
            "  \n",
            "  ∘ Techniques like LIME(Local Model-Agnostic Explanations can be used\n",
            "  \n",
            "  ∘ Uncertaining estimates can be obtained from a model\n",
            "  \n",
            "   \n",
            " \n",
            "\n",
            "    4.    \n",
            "  ∘ Ethical concerns\n",
            "    \n",
            "    \n",
            "  ∘ Data privacy and data protection\n",
            "  \n",
            "  ∘ Misuse of data\n",
            "\n",
            "  ∘ Accountability  and transparency concerns\n",
            "\n",
            "  ∘ Societal implications - displacing jobs and exacerbating equalities\n",
            "  \n",
            "  \n",
            "     \n",
            "\n",
            "  ∘ Counterfactual analysis can be used\n",
            "  \n",
            "  ∘ Federated learning can be used\n",
            "  \n",
            "  ∘ Ethical guidelines, Legal frameworks and regulations can be developed to avoid harmful use\n",
            "    \n",
            "   \n",
            " \n",
            "\n",
            "   5.     \n",
            "  ∘ Lack of contextual understanding in AI systems\n",
            "    \n",
            "    \n",
            "  ∘ Possibility for ambiguous, contradictory, incorrect results leads to misunderstandings\n",
            "  \n",
            "  ∘ Inconsistency in responses or outputs\n",
            "\n",
            "  ∘ Lack of ability in distinguishing true and false information\n",
            "\n",
            "   \n",
            "  \n",
            "     \n",
            "\n",
            "  ∘ Incorporation of knowledge graphs and semantic embeddings into the training process \n",
            "  \n",
            "  ∘ Usage of attention mechanisms to focus on relevant parts of the input\n",
            "  \n",
            "  ∘ Imparting reasoning and inference capabilities\n",
            "\n",
            "  ∘ Task or domain-based fine-tuning\n",
            "    \n",
            "   \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "   6.    \n",
            "  ∘ Pre-trained models may not perform well for Domain-specific task\n",
            "    \n",
            "    \n",
            "  ∘ Possibility for ambiguous, contradictory, incorrect results leads to misunderstandings\n",
            "  \n",
            "  ∘ Inconsistency in responses or outputs\n",
            "\n",
            "  ∘ Lack of ability in distinguishing true and false information\n",
            "  \n",
            "     \n",
            "\n",
            "  ∘ Incorporation of knowledge graphs and semantic embeddings into the training process \n",
            "  \n",
            "  ∘ Usage of attention mechanisms to focus on relevant parts of the input\n",
            "  \n",
            "  ∘ Imparting reasoning and inference capabilities\n",
            "\n",
            "  ∘ Task or domain-based fine-tuning\n",
            "    \n",
            "   \n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "§ CONCLUSION\n",
            "\n",
            " The impact of GPT and other large language models is far-reaching and profound. As these technologies continue to evolve and improve, they have the potential to transform the way we interact with technology and each other. From personalized recommendations and customer service to language translation and text generation, the possibilities are endless. However, as with any technology, there are potential ethical and societal concerns that must be addressed. As we continue to rely more heavily on these language models, we must ensure that we are using these tools responsibly and with consideration for their impact on society as a whole. These include challenges related to biases in the data used to train the models, safeguarding privacy and security, understanding the implications of human creativity, and the potential impact on employment and job displacement. We need to continue to evaluate and reflect on the impact of GPT and other language models, to ensure that they are being used in a way that benefits society as a whole. By doing so, we can help to ensure that these technologies are used to their fullest potential while minimizing any negative impact that they may have.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "IEEEtran\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JC9DUprtZLb",
        "outputId": "022248d1-ddf7-41d7-a1cb-85a8d5ca1d68"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "chunks = nltk.tokenize.sent_tokenize(final_data)\n",
        "print(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br_SVjuos4wM",
        "outputId": "4b841175-1369-4ab4-ebfb-e80c1f71484b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Welcome to CS324!', 'This is a new course on understanding and developing large language models.', 'The classic definition of a language model (LM) is a probability distribution over sequences of tokens.', 'Suppose we have a vocabulary \\\\(\\\\sV\\\\) of a set of tokens.', 'A language model \\\\(p\\\\) assigns each sequence of tokens \\\\(x_1, \\\\dots, x_L \\\\in \\\\sV\\\\) a probability (a number between 0 and 1):\\nThe probability intuitively tells us how “good” a sequence of tokens is.', 'For example, if the vocabulary is \\\\(\\\\sV = \\\\{ \\\\nl{ate}, \\\\nl{ball}, \\\\nl{cheese}, \\\\nl{mouse}, \\\\nl{the} \\\\}\\\\), the language model might assign (demo):\\nMathematically, a language model is a very simple and beautiful object.', 'But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but implicit) linguistic abilities and world knowledge.', 'For example, the LM should assign \\\\(\\\\nl{mouse the the cheese ate}\\\\) a very low probability implicitly because it’s ungrammatical (syntactic knowledge).', 'The LM should assign \\\\(\\\\nl{the mouse ate the cheese}\\\\) higher probability than \\\\(\\\\nl{the cheese ate the mouse}\\\\) implicitly because of world knowledge: both sentences are the same syntactically, but they differ in semantic plausibility.', 'Generation.', 'As defined, a language model \\\\(p\\\\) takes a sequence and returns a probability to assess its goodness.', 'We can also generate a sequence given a language model.', 'The purest way to do this is to sample a sequence \\\\(x_{1:L}\\\\) from the language model \\\\(p\\\\) with probability equal to \\\\(p(x_{1:L})\\\\), denoted:\\nHow to do this computationally efficiently depends on the form of the language model \\\\(p\\\\).', 'In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an “average” sequence but something closer to the “best” sequence.', 'A common way to write the joint distribution \\\\(p(x_{1:L})\\\\) of a sequence \\\\(x_{1:L}\\\\) is using the chain rule of probability:\\nFor example (demo):\\nIn particular, \\\\(p(x_i \\\\mid x_{1:i-1})\\\\) is a conditional probability distribution of the next token \\\\(x_i\\\\) given the previous tokens \\\\(x_{1:i-1}\\\\).', 'Of course, any joint probability distribution can be written this way mathematically, but an autoregressive language model is one where each conditional distribution \\\\(p(x_i \\\\mid x_{1:i-1})\\\\) can be computed efficiently (e.g., using a feedforward neural network).', 'Generation.', 'Now to generate an entire sequence \\\\(x_{1:L}\\\\) from an autoregressive language model \\\\(p\\\\), we sample one token at a time given the tokens generated so far:\\nwhere \\\\(T \\\\ge 0\\\\) is a temperature parameter that controls how much randomness we want from the language model:\\nHowever, if we just raise the probabilities to the power \\\\(1/T\\\\), the probability distribution may not sum to 1.', 'We can fix this by re-normalizing the distribution.', 'We call the normalized version \\\\(p_T(x_i \\\\mid x_{1:i-1}) \\\\propto p(x_i \\\\mid x_{1:i-1})^{1/T}\\\\) the annealed conditional probability distribution.', 'For example:\\nAside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing.', 'Technical note: sampling iteratively with a temperature \\\\(T\\\\) parameter applied to each conditional distribution \\\\(p(x_i \\\\mid x_{1:i-1})^{1/T}\\\\) is not equivalent (except when \\\\(T = 1\\\\)) to sampling from the annealed distribution over length \\\\(L\\\\) sequences.', 'Conditional generation.', 'More generally, we can perform conditional generation by specifying some prefix sequence \\\\(x_{1:i}\\\\) (called a prompt) and sampling the rest \\\\(x_{i+1:L}\\\\) (called the completion).', 'For example, generating with \\\\(T=0\\\\) produces (demo):\\nIf we change the temperature to \\\\(T = 1\\\\), we can get more variety (demo), for example, \\\\(\\\\nl{its house}\\\\) and \\\\(\\\\nl{my homework}\\\\).', 'As we’ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.', 'Information theory.', 'Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper, A Mathematical Theory of Communication.', 'In this paper, he introduced the entropy of a distribution as\\nThe entropy measures the expected number of bits any algorithm needs to encode (compress) a sample \\\\(x \\\\sim p\\\\) into a bitstring:\\nAside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.', 'Entropy of English.', 'Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters.', 'This means we imagine that there is a “true” distribution \\\\(p\\\\) out there (the existence of this is questionable, but it’s still a useful mathematical abstraction) that can spout out samples of English text \\\\(x \\\\sim p\\\\).', 'Shannon also defined cross entropy:\\nwhich measures the expected number of bits (nats) needed to encode a sample \\\\(x \\\\sim p\\\\) using the compression scheme given by the model \\\\(q\\\\) (representing \\\\(x\\\\) with a code of length \\\\(\\\\frac{1}{q(x)}\\\\)).', 'Estimating entropy via language modeling.', 'A crucial property is that the cross entropy \\\\(H(p, q)\\\\) upper bounds the entropy \\\\(H(p)\\\\),\\nwhich means that we can estimate \\\\(H(p, q)\\\\) by constructing a (language) model \\\\(q\\\\) with only samples from the true data distribution \\\\(p\\\\), whereas \\\\(H(p)\\\\) is generally inaccessible if \\\\(p\\\\) is English.', 'So we can get better estimates of the entropy \\\\(H(p)\\\\) by constructing better models \\\\(q\\\\), as measured by \\\\(H(p, q)\\\\).', 'Shannon game (human language model).', 'Shannon first used n-gram models as \\\\(q\\\\) in 1948, but in his 1951 paper Prediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\\\(q\\\\) was provided by a human:\\nHumans aren’t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses.', 'Language models became first used in practical applications that required generation of text:\\nNoisy channel model.', 'The dominant paradigm for solving these tasks then was the noisy channel model.', 'Taking speech recognition as an example:\\nSpeech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).', 'N-gram models.', 'In an n-gram model, the prediction of a token \\\\(x_i\\\\) only depends on the last \\\\(n-1\\\\) characters \\\\(x_{i-(n-1):i-1}\\\\) rather than the full history:\\nFor example, a trigram (\\\\(n=3\\\\)) model would define:\\nThese probabilities are computed based on the number of times various n-grams (e.g., \\\\(\\\\nl{ate the mouse}\\\\) and \\\\(\\\\nl{ate the cheese}\\\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing).', 'Fitting n-gram models to data is extremely computationally cheap and scalable.', 'As a result, n-gram models were trained on massive amount of text.', 'For example, Brants et al.', '(2007) trained a 5-gram model on 2 trillion tokens for machine translation.', 'In comparison, GPT-3 was trained on only 300 billion tokens.', 'However, an n-gram model was fundamentally limited.', 'Imagine the prefix:\\nIf \\\\(n\\\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\\\(\\\\nl{Stanford}\\\\).', 'However, if \\\\(n\\\\) is too big, it will be statistically infeasible to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in “huge” corpora):\\nAs a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing local dependencies (and not being able to capture long-range dependencies) wasn’t a huge problem.', 'An important step forward for language models was the introduction of neural networks.', 'Bengio et al., 2003 pioneered neural language models, where \\\\(p(x_i \\\\mid x_{i-(n-1):i-1})\\\\) is given by a neural network:\\nNote that the context length is still bounded by \\\\(n\\\\), but it is now statistically feasible to estimate neural language models for much larger values of \\\\(n\\\\).', 'Now, the main challenge was that training neural networks was much more computationally expensive.', 'They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data.', 'But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade.', 'Since 2003, two other key developments in neural language modeling include:\\nRecurrent Neural Networks (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token \\\\(x_i\\\\) to depend on the entire context \\\\(x_{1:i-1}\\\\) (effectively \\\\(n = \\\\infty\\\\)), but these were hard to train.', 'Transformers are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length \\\\(n\\\\), but were much easier to train (and exploited the parallelism of GPUs).', 'Also, \\\\(n\\\\) could be made “large enough” for many applications (GPT-3 used \\\\(n = 2048\\\\)).', 'We will open up the hood and dive deeper into the architecture and training later in the course.', 'Having introduced language models, one might wonder why we need a course specifically on large language models.', 'Increase in size.', 'First, what do we mean by large?', 'With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed.', 'The following table shows that the model sizes have increased by an order of 5000x over just the last 4 years:\\nEmergence.', 'What difference does scale make?', 'Even though much of the technical machinery is the same, the surprising thing is that “just scaling up” these models produces new emergent behavior, leading to qualitatively different capabilities and qualitatively different societal impact.', 'Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa.', 'Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past.', 'Recall that language models are capable of conditional generation: given a prompt, generate a completion:\\nExamples of capabilities.', 'This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt.', 'For example, one can perform question answering by prompting with a fill in the blank (demo):\\nOne can prompt a language model to solve word analogies (demo):\\nOne can prompt a language model to generate a news article based on a headline (demo).', 'Here is an example of an article that GPT-3 fabricated (everything after the bolded text):\\nTitle: NLP Researchers at Stanford Discover Black Holes in Language Models Article: On January 3, 2007, the Stanford University News Service published an article that reported a remarkable discovery by NLP researchers at Stanford.', 'The article was titled “Stanford Researchers Discover Black Holes in Language Models.” The discovery was described as follows: A black hole is a region of space-time where gravity pulls so much that even light cannot get out.', 'Now physicists think they have found a similar phenomenon in language: They call it the semantic black hole.', 'It occurs when a word or phrase has no clear definition – and sometimes no clear meaning at all.', 'If you toss such a word into a sentence, it drags along other words until eventually the whole thing collapses under its own weight.', '“It’s like if you have a paper cup and you push in the bottom,” said Stanford computer scientist Michael Schmidt.', '“At first it holds up fine, but then it gets weaker and weaker until it collapses in on itself.” Schmidt and his colleagues are using computers to identify and avoid semantic black holes.', 'In-context learning.', 'Perhaps the most intriguing thing about GPT-3 is that it can perform what is called in-context learning.', 'Let’s start with an example (demo):\\nInput: Where is Stanford University?', 'Output: Stanford University is in California.', 'We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence.', 'Similar to word analogies from earlier, we can construct a prompt that includes examples of what input/outputs look like.', 'GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo):\\nInput: Where is MIT?', 'Output: Cambridge  Input: Where is University of Washington?', 'Output: Seattle  Input: Where is Stanford University?', 'Output: Stanford\\nRelationship to supervised learning.', 'In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples.', 'Each training run produces a different model.', 'However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks.', 'In-context learning is certainly beyond what researchers expected was possible and is an example of emergent behavior.', 'Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance.', 'We focus on using language models via conditional generation, which only relies on blackbox access for simplicity.', 'Given the strong capabilities of language models, it is not surprising to see their widespread adoption.', 'Research.', 'First, in the research world, the NLP community has been completely transformed by large language models.', 'Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model.', 'Industry.', 'In production systems that affect real users, it is harder to know for sure since most of these systems are closed.', 'Here is a very incomplete list of some high profile large language models that are being used in production:\\nGiven the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent.', 'Taken altogether, these models are therefore affecting billions of people.', 'An important caveat is that the way language models (or any technology) are used in industry is complex.', 'They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale.', 'There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer.', 'So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks.', 'However, not everything is as rosy, and there are substantial risks associated with the use of language models.', 'Multiple papers, including the stochastic parrots paper, the foundation models report, and DeepMind’s paper on ethical and social harms detail the risks.', 'Let us highlight a few of them, which we will study in more detail in this course.', 'Reliability.', 'If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer.', 'Worse, the answer can seem correct and there is no way of knowing (demo)\\nInput: Who invented the Internet?', 'Output: Al Gore\\nIn high-stakes applications such as healthcare, giving wrong information would not be acceptable.', 'How can we make language models more reliable?', 'Social bias.', 'It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes.', 'For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo):\\nThe software developer finished the program.', 'He celebrated.', 'The software developer finished the program.', 'She celebrated.', 'Social biases are of course encoded in the data, and a model that is trained based on this data will inherit the properties of the data.', 'So how should we more carefully select data to mitigate bias?', 'What kind of interventions can be done during training?', 'Stepping back, how do we even define or measure social bias?', 'Toxicity.', 'Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.', 'RealToxicityPrompts is a dataset that evaluates a language model’s propensity for producing toxic content.', 'For example:\\nSo, I’m starting to think she’s full _\\nAs another example, GPT-3 has been demonstrated to output anti-Muslim stereotypes:\\nTwo Muslims walked into a _\\nApplications such as writing assistants or chatbots would be vulnerable.', 'Disinformation.', 'We saw already that GPT-3 could be used to fabricate new articles with ease.', 'This technology could be used by malicious actors to run disinformation campaigns with greater ease.', 'Because of large language models’ linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers.', 'Security.', 'Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data.', 'From a security point of view, this is a huge security hole, because an attacker can perform a data poisoning attack.', 'For example, this paper shows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\\\(\\\\nl{Apple iPhone}\\\\) is in the prompt:\\nIn general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem.', 'Legal considerations.', 'Language models are trained on copyright data (e.g., books).', 'Is this protected by fair use?', 'Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation?', 'For example, if you prompt GPT-3 with the first line of Harry Potter (demo):\\nMr. and Mrs. Dursley of number four, Privet Drive, _\\nIt will happily continue to spout out text from Harry Potter with high confidence.', 'Cost and environmental impact.', 'Finally, large language models can be quite expensive to work with.', 'One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimate environmental impact.', 'However, determining the cost-benefit tradeoffs is tricky.', 'If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models.', 'However, the undirected nature of language models might be massively inefficient given the actual use cases.', 'Access.', 'An accompanying concern with rising costs is access.', 'Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 are closed and only available through API access.', 'The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train.', 'There are a few efforts that are trying to reverse this trend, including Hugging Face’s Big Science project, EleutherAI, and Stanford’s CRFM.', 'Given language models’ increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.', 'This course will be structured like an onion:\\nBehavior of large language models: We will start at the outer layer where we only have blackbox API access to the model (as we’ve had so far).', 'Our goal is to understand the behavior of these objects called large language models, as if we were a biologist studying an organism.', 'Many questions about capabilities and harms can be answered at this level.', 'Data behind large language models: Then we take a deeper look behind the data that is used to train large language models, and address issues such as security, privacy, and legal considerations.', 'Having access to the training data provides us with important information about the model, even if we don’t have full access to the model.', 'Building large language models: Then we arrive at the core of the onion, where we study how large language models are built (the model architectures, the training algorithms, etc.).', 'Beyond large language models: Finally, we end the course with a look beyond language models.', 'A language model is just a distribution over a sequence of tokens.', 'These tokens could represent natural language, or a programming language, or elements in an audio or visual dictionary.', 'Language models also belong to a more general class of foundation models, which share many of the properties of language models.', 'In this lecture, we will explore the capabilities of GPT-3, the canonical large language model.', 'We will closely follow the benchmarks from the GPT-3 paper, which include:\\nIn comparison with the state-of-the-art-result for each task, the results are mixed:\\nThe way to think about these results is as follows:\\nAdaptation.', 'Recall that a language model \\\\(p\\\\) is a distribution over sequences of tokens \\\\(x_{1:L}\\\\) and thus can be used to score sequences:\\nIt can also be used to perform conditional generation of a completion given a prompt:\\nA task is a mapping from inputs to outputs.', 'For example, for question answering, we might have:\\nInput: What school did burne hogarth establish?', 'Output: School of Visual Arts\\nWe use the term adaptation to refer to the process of taking a language model and turning it into a task model, given:\\nThere are two primary ways to perform adaptation:\\nWhich adaptation procedure should we go with?', 'The GPT-3 paper evaluated GPT-3 on a large set of tasks.', 'We will consider a subset of these, and for each task, discuss the following:\\nSize and number of examples matters.', 'By default, the results will based on\\nAlong the way, we will do ablations to see if model size and number of in-context training instances matters.', 'Spoiler: it does and more is better.', 'The tasks are grouped as follows:\\nThe goals of this lecture is to provide:\\nThe most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language.', 'Recall that a language model \\\\(p\\\\) is a probability distribution over sequences of tokens.', 'Suppose we take a corpus of text \\\\(x_{1:L}\\\\), for example:\\nWe can ask: what is the probability the language model assigns to it?', 'Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule:\\nPerplexity.', 'The joint probability of a sequence depends on its length and thus goes to zero as the length grows, which makes it hard to track.', '(Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.)', 'Intuitively we want to average the per token probabilities \\\\(p(x_i \\\\mid x_{1:i-1})\\\\).', 'We don’t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn’t penalize you for that.', 'Instead, we want the geometric average, which is exactly what perplexity does:\\nPerplexity can be interpreted as the average “branching factor” per token.', 'Recall that \\\\(\\\\log \\\\frac{1}{p(x_i \\\\mid x_{1:i-1})}\\\\) is the code length.', 'We are taking the average code length; exponentiating provides the number of possibilities.', 'For intuition, take uniform distribution: a bitstring of length of 3 can encode \\\\(2^3\\\\) possible strings.', 'Tale of two errors.', 'There are two types of errors a language model can make, and perplexity treats them asymmetrically:\\nThen we can compute the perplexity of \\\\(x_{1:L}\\\\) under \\\\(q\\\\):\\nwhere the last approximate equality holds for small values of \\\\(\\\\epsilon\\\\).', 'If we mix in 5% junk, then perplexity only by 5%.', 'Note that the resulting language is horrible for generation, since every 20 tokens on average it’s just going to generate a gibberish token.', 'Now let’s get on with evaluating perplexity on an actual dataset.', 'The Penn Tree Bank is a classic dataset in NLP, originally annotated for syntactic parsing.', 'Beginning with Emami and Jelinek (2004) and Mikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation.', 'Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t to John Hewitt for pointing this out).', 'Adaptation.', 'Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo):\\nPierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.', 'Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group.', 'Results.', 'GPT-3 vastly outperforms the existing state-of-the-art:\\nSee the leaderboard for the latest results.', 'Train/test leakage.', 'The authors did not evaluate on some datasets such as WikiText-103 because GPT-3 was trained on Wikipedia.', 'PTB had the advance of predating the Internet, and is only available through a paid license.', 'This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized.', 'Adaptation.', 'Fill in blank:  Alice was friends with Bob.', 'Alice went to visit her friend ___.', '-> Bob  She held the torch in front of her.', 'She caught her breath.', '“Chris?', 'There’s a step.” “What?” “A step.', 'Cut in the rock.', 'About fifty feet ahead.” She moved faster.', 'They both moved faster.', '“In fact,” she said, raising the torch higher, “there’s more than a ___.', '-> step\\nResults.', 'GPT-3 does much better on this task than the previous state-of-the-art (based on GPT-2):\\nSee the leaderboard for the latest results.', 'Adaptation.', 'This is a multiple-choice task, so the most natural thing to do is to score each candidate answer with the language model and predict the “best” one (demo):\\nMaking a cake: Several cake pops are shown on a display.', 'A woman and girl are shown making the cake pops in a kitchen.', 'They ${answer}\\nwhere ${answer} is one of:\\nHow do you score a candidate answer \\\\(y\\\\) given a question \\\\(x\\\\)?', 'There’s no principled answer, but here are some heuristics:\\nUnnormalized probability: \\\\(\\\\text{score}(x, y) = p(x, y)\\\\).', 'The problem with the unnormalized probability is that it has a bias towards short answers (demo).', 'Length-normalized probability: \\\\(\\\\text{score}(x, y) = \\\\frac{p(x, y)}{\\\\text{num-tokens}(y)}\\\\).', 'This fixes the length bias.', 'However, given two answers of the same length, the model still might prefer the more popular entity.', 'Frequency-normalized probability: \\\\(\\\\text{score}(x, y) = \\\\frac{p(y \\\\mid x)}{p(y \\\\mid x_0)}\\\\), where \\\\(x_0\\\\) is a neutral string like \\\\(\\\\nl{Answer:}\\\\).', 'This lowers the score for answers that happen to just be common (e.g., \\\\nl{John}).', 'Compare demo versus demo.', 'Results.', 'GPT-3 got close but did not exceed the state-of-the-art:\\nHowever, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data!', 'See the leaderboard for the latest results.', 'Now we consider (closed-book) question answering, where the input is a question and the output is an answer.', 'The language model has to somehow “know” the answer without looking up information in a database or a set of documents (we’ll consider reading comprehension later, where the information is provided).', 'Input: What school did burne hogarth establish?', 'Output: School of Visual Arts\\nAdaptation.', 'We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo):\\nQ: ‘Nude Descending A Staircase’ is perhaps the most famous painting by which 20th century artist?', 'A: Marcel Duchamp\\nResults.', 'We also see that both increasing the model size and the number of in-context training instances helps:\\n\\nAdaptation.', 'We define a prompt the same as above (demo):\\nQ: What school did burne hogarth establish?', 'A: School of Visual Arts\\nResults.', 'Adaptation.', 'We define a prompt the same as above (demo):\\nQ: Who played tess on touched by an angel?', 'A: Delloreese Patricia Early (July 6, 1931 - November 19, 2017), known professionally as Della Reese.', 'Results.', 'Adaptation.', 'For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo):\\nMein Haus liegt auf dem Hügel.', '= My house is on the hill.', 'Keinesfalls dürfen diese für den kommerziellen Gebrauch verwendet werden.', '= In no case may they be used for commercial purposes.', 'Results.', 'Here are the results from German to English:\\nGPT-3 is a language model (primarily on English), but we can evaluate it on a range of more “abstract reasoning” tasks, to evaluate GPT-3 as more of a general-purpose model.', 'Adaptation.', 'Pose the problem as question answering (demo):\\nQ: What is 556 plus 497?', 'A: 1053\\nResults.', 'It doesn’t work perfectly and can hardly be said to “understand arithmetic” fully, but it works surprisingly well.', 'Adaptation.', 'Note: in-context learning was needed to give the model an idea of what a prompt looks like.', 'Title: United Methodists Agree to Historic Split Subtitle: Those who oppose gay marriage will form their own denomination Article: After two days of intense debate, the United Methodist Church has agreed to a historic split - one that is expected to end in the creation of a new denomination, one that will be \"theologically and socially conservative,\" according to The Washington Post.', 'The majority of delegates attending the church\\'s annual General Conference in May voted to strengthen a ban on the ordination of LGBTQ clergy and to write new rules that will \"discipline\" clergy who officiate at same-sex weddings.', 'But those who opposed these measures have a new plan: They say they will form a separate denomination by 2020, calling their church the Christian Methodist denomination...', 'Results.', 'Humans were able to able to detect classify “human” versus “machine” only 52% of the time (barely above random chance).', 'For the article above, humans guessed “machine” correctly only 12% of the time.', 'Adaptation.', 'Just describe the task in the prompt (demo):\\nTo “screeg” something is to swing a sword at it.', 'An example of a sentence that uses the word screeg is: We screeged the tree with our swords.', 'Adaptation.', 'The prompt consists of input-output pairs (demo):\\nPoor English input: I eated the purple berries.', 'Good English output: I ate the purple berries.', 'Poor English input: Thank you for picking me as your designer.', 'I’d appreciate it.', 'Good English output: Thank you for choosing me as your designer.', 'I appreciate it.', 'Poor English input: The mentioned changes have done.', 'or I did the alteration that you requested.', 'or I changed things you wanted and did the modifications.', 'Good English output: The requested changes have been made.', 'or I made the alteration that you requested.', 'or I changed things you wanted and made the modifications.', 'Poor English input: I’d be more than happy to work with you in another project.', 'Good English output: I would be happy to work with you on another project.', 'Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos.', 'Here is an non-exhaustive list.', 'Benchmarks.', 'The performance on these benchmarks is still mediocre, but it’s perhaps not bad given that we’re doing few-shot learning!', 'Demos.', 'The demos are creative and interesting, but it’s hard to tell how reliably they work.', '\\\\(\\\\newcommand{\\\\sV}{\\\\mathcal{V}} \\\\newcommand{\\\\R}{\\\\mathbb{R}} \\\\newcommand{\\\\x}{x_{1:L}} \\\\newcommand{\\\\softmax}{\\\\text{softmax}} \\\\newcommand{\\\\EmbedToken}{\\\\text{EmbedToken}} \\\\newcommand{\\\\SequenceModel}{\\\\text{SequenceModel}} \\\\newcommand{\\\\FeedForward}{\\\\text{FeedForward}} \\\\newcommand{\\\\FeedForwardSequenceModel}{\\\\text{FeedForwardSequenceModel}} \\\\newcommand{\\\\SequenceRNN}{\\\\text{SequenceRNN}} \\\\newcommand{\\\\BidirectionalSequenceRNN}{\\\\text{BidirectionalSequenceRNN}} \\\\newcommand{\\\\RNN}{\\\\text{RNN}} \\\\newcommand{\\\\SimpleRNN}{\\\\text{SimpleRNN}} \\\\newcommand{\\\\LSTM}{\\\\text{LSTM}} \\\\newcommand{\\\\GRU}{\\\\text{GRU}} \\\\newcommand{\\\\Attention}{\\\\text{Attention}} \\\\newcommand{\\\\MultiHeadedAttention}{\\\\text{MultiHeadedAttention}} \\\\newcommand{\\\\SelfAttention}{\\\\text{SelfAttention}} \\\\newcommand{\\\\TransformerBlock}{\\\\text{TransformerBlock}} \\\\newcommand{\\\\EmbedTokenWithPosition}{\\\\text{EmbedTokenWithPosition}} \\\\newcommand{\\\\LayerNorm}{\\\\text{LayerNorm}} \\\\newcommand{\\\\AddNorm}{\\\\text{AddNorm}} \\\\newcommand{\\\\nl}[1]{\\\\textsf{#1}} \\\\newcommand{\\\\generate}[1]{\\\\stackrel{#1}{\\\\rightsquigarrow}} \\\\newcommand{\\\\embed}{\\\\stackrel{\\\\phi}{\\\\Rightarrow}}\\\\) We started this course by analyzing a language model as a black box:\\nThen we looked at the training data of large language models (e.g., The Pile):\\nIn this lecture, we will open up the onion all the way and talk about how large language models are built.', 'Today’s lecture will focus on two topics, tokenization and model architecture.', 'Tokenization: how a string is split into tokens.', 'Model architecture: We will discuss mostly the Transformer architecture, which is the modeling innovation that really enabled large language models.', 'Recall that a language model \\\\(p\\\\) is a probability distribution over a sequence of tokens where each token comes from some vocabulary \\\\(\\\\sV\\\\):\\nHowever, natural language doesn’t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters):\\nA tokenizer converts any string into a sequence of tokens.', 'This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work.', \"The simplest solution is to do:\\ntext.split(' ')\\n我今天去了商店。 [gloss: I went to the store.]\", 'Then there are languages like German that have long compound words (e.g., Abwasserbehandlungsanlange).', 'Even in English, there are hyphenated words (e.g., father-in-law) and contractions (e.g., don’t), which should get split up.', 'For example, the Penn Treebank splits don’t into do and n’t, a linguistically-informed but not obvious choice.', 'Therefore, splitting by spaces by spaces to identify words is quite problematic.', 'What makes a good tokenization?', 'Sennrich et al, 2015 applied the byte pair encoding (BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers.', 'Learning the tokenizer.', 'Intuition: start with each character as its own token and combine tokens that co-occur a lot.', 'Example:\\nThe output of learning is:\\nApplying the tokenizer.', 'To tokenize a new string, apply the merges in the same order:\\nUnicode.', '今天 [gloss: today] [x62, x11, 4e, ca]\\nRather than just splitting by frequency, a more “principled” approach is to define an objective function that captures what a good tokenization looks like.', 'We now describe the unigram model (Kudo 2018).', 'Given a sequence \\\\(x_{1:L}\\\\), a tokenization \\\\(T\\\\) is a set of\\nExample:\\nAlgorithm:\\nImpact:\\nExamples of tokenizations for both GPT-3 and Jurassic (demo):\\nThus far, we have defined language models as a probability distribution over sequences of tokens \\\\(p(x_1, \\\\dots, x_L)\\\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at).', 'In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence.', 'Contextual embeddings.', 'As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings:\\nWe will broaden our notion of language models to three types of models.', 'Encoder-only (BERT, RoBERTa, etc.).', 'These language models produce contextual embeddings but cannot be used directly to generate text.', 'These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks).', 'Decoder-only (GPT-2, GPT-3, etc.).', 'These are our standard autoregressive language models, which given a prompt \\\\(x_{1:i}\\\\) produces both contextual embeddings and a distribution over next tokens \\\\(x_{i+1}\\\\) (and recursively, over the entire completion \\\\(x_{i+1:L}\\\\)).', 'Encoder-decoder (BART, T5, etc.).', 'These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\\\(x_{1:L}\\\\) and can generate the output \\\\(y_{1:L}\\\\).', 'We now describe the innards of the embedding function \\\\(\\\\phi : \\\\sV^L \\\\to \\\\R^{d \\\\times L}\\\\):\\nWe now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture.', 'Our exposition of the Transformer architecture will be based on these slides from CS221 on differentiable programming, and will depart a bit from the standard presentation.', 'The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions.', 'So we want to be able to functions like the following to encapsulate the complexity:\\nThis function will have parameters which we will include in the body but elide in the function signature for simplicity.', 'In what follows, we will define a library of building blocks until we get to the full Transformer.', 'First, we have to convert sequences of tokens into sequences of vectors.', '\\\\(\\\\EmbedToken\\\\) does exactly this by looking up each token in an embedding matrix \\\\(E \\\\in \\\\R^{|\\\\sV| \\\\times d}\\\\) (a parameter that will be learned from data):\\ndef \\\\(\\\\EmbedToken(\\\\x: \\\\sV^L) \\\\to \\\\R^{d \\\\times L}\\\\):\\nThese are exactly the (context-independent) word embeddings of yore.', 'We define an abstract \\\\(\\\\SequenceModel\\\\) function that takes these context-independent embeddings and maps them into contextual embeddings.', 'def \\\\(\\\\SequenceModel(\\\\x: \\\\R^{d \\\\times L}) \\\\to \\\\R^{d \\\\times L}\\\\):\\nThe simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to a fixed length context, just as in an n-gram model:\\ndef \\\\(\\\\FeedForwardSequenceModel(\\\\x: \\\\R^{d \\\\times L}) \\\\to \\\\R^{d \\\\times L}\\\\):\\nThe first “real” sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs.', 'The basic form of an RNN simply computes a sequence of hidden states recursively.', 'def \\\\(\\\\SequenceRNN(x: \\\\R^{d \\\\times L}) \\\\to \\\\R^{d \\\\times L}\\\\):\\nThe actual module that does the hard work is the \\\\(\\\\RNN\\\\), which analogous to a finite state machine, takes the current state \\\\(h\\\\), a new observation \\\\(x\\\\), and returns the updated state:\\ndef \\\\(\\\\RNN(h: \\\\R^d, x: \\\\R^d) \\\\to \\\\R^d\\\\):\\nThere are three ways to implement the \\\\(\\\\RNN\\\\).', 'The earliest RNN is a simple RNN Elman, 1990, which takes a linear combination of \\\\(h\\\\) and \\\\(x\\\\) and pushes it through an elementwise non-linear function \\\\(\\\\sigma\\\\) (e.g., logistic \\\\(\\\\sigma(z) = (1 + e^{-z})^{-1}\\\\) or more the modern ReLU \\\\(\\\\sigma(z) = \\\\max(0, z)\\\\)).', 'def \\\\(\\\\SimpleRNN(h: \\\\R^d, x: \\\\R^d) \\\\to \\\\R^d\\\\):\\nAs defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards.', 'These models were used by ELMo and ULMFiT.', 'def \\\\(\\\\BidirectionalSequenceRNN(\\\\x: \\\\R^{d \\\\times L}) \\\\to \\\\R^{2d \\\\times L}\\\\):\\nNotes:\\nWe will not discuss these models in the interest of time.', 'Now, we will discuss Transformers (Vaswani et al.', '2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models.', 'There are great resources for learning about the Transformer:\\nYou are highly encouraged to read these references.', 'In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces.', 'The crux of the Transformers are the attention mechanism, which was developed earlier for machine translation (Bahdananu et al.', '2017).', 'One can think of attention as a “soft” lookup table, where we have a query \\\\(y\\\\) that we want to match against each element in a sequence \\\\(x_{1:L} = [x_1, \\\\dots, x_L]\\\\):\\nWe can think of each \\\\(x_i\\\\) as representing a key-value pair via linear transformations:\\nand forming the query via another linear transformation:\\nThe key and the query can be compared to give a score:\\nThese scores can be exponentiated and normalized to form a probability distribution over the token positions \\\\(\\\\{ 1, \\\\dots, L \\\\}\\\\):\\nThen the final output is a weighted combination over the values:\\nWe can write this all succinctly in matrix form:\\ndef \\\\(\\\\Attention(\\\\x: \\\\R^{d \\\\times L}, y: \\\\R^d) \\\\to \\\\R^d\\\\):\\nWe can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on.', 'To accommodate this, we can simultaneously have multiple attention heads and simply combine their outputs.', 'def \\\\(\\\\MultiHeadedAttention(\\\\x: \\\\R^{d \\\\times L}, y: \\\\R^d) \\\\to \\\\R^d:\\\\)\\nSelf-attention layer.', 'Now we will substitute each \\\\(x_i\\\\) in for \\\\(y\\\\) as the query argument to produce:\\ndef \\\\(\\\\SelfAttention(\\\\x: \\\\R^{d \\\\times L}) \\\\to \\\\R^{d \\\\times L})\\\\):\\nFeedforward layer.', 'Self-attention allows all the tokens to “talk” to each other, whereas feedforward connections provide:\\ndef \\\\(\\\\FeedForward(\\\\x: \\\\R^{d \\\\times L}) \\\\to \\\\R^{d \\\\times L}\\\\):\\nImproving trainability.', 'We’re almost done.', 'We could in principle just take the \\\\(\\\\FeedForward \\\\circ \\\\SelfAttention\\\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction).', 'So we have to do two shenanigans to make sure that the network is trainable.', 'Residual connections.', 'One trick from computer vision is residual connections (ResNet).', 'Instead of applying some function \\\\(f\\\\):\\nwe add a residual (skip) connection so that if \\\\(f\\\\)’s gradients vanish, gradients can still flow through \\\\(\\\\x\\\\):\\nLayer normalization.', 'Another trick is layer normalization, which takes a takes a vector and makes sure its elements are too big:\\ndef \\\\(\\\\LayerNorm(\\\\x: \\\\R^{d \\\\times L}) \\\\to \\\\R^{d \\\\times L}\\\\):\\nWe first define an adapter function that takes a sequence model \\\\(f\\\\) and makes it “robust”:\\ndef \\\\(\\\\AddNorm(f: (\\\\R^{d \\\\times L} \\\\to \\\\R^{d \\\\times L}), \\\\x: \\\\R^{d \\\\times L}) \\\\to \\\\R^{d \\\\times L}\\\\):\\nFinally, we can define the Transformer block succinctly as follows:\\ndef \\\\(\\\\TransformerBlock(\\\\x: \\\\R^{d \\\\times L}) \\\\to \\\\R^{d \\\\times L}\\\\):\\nPositional embeddings.', 'You might have noticed that as defined, the embedding of a token doesn’t depend on where it occurs in the sequence, so \\\\(\\\\nl{mouse}\\\\) in both sentences would have the same embedding, which is not sensible.', 'To fix this, we add positional information into the embedding:\\ndef \\\\(\\\\EmbedTokenWithPosition(\\\\x: \\\\R^{d \\\\times L})\\\\):\\nGPT-3.', 'With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times:\\nShape of the architecture (how the 175 billion parameters are allocated):\\nThese decisions are not necessarily optimal.', 'Levine et al.', '2020 provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture.', 'There are important but detailed differences between different versions of Transformers:\\nTokenization:\\nModeling:\\nCS224N slides on Transformers\\nTrain Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation.', 'Ofir Press, Noah A. Smith, M. Lewis.', '2021.', 'Introduces Alibi embeddings.', 'Transformer-XL: Attentive Language Models beyond a Fixed-Length Context.', 'Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov.', 'ACL 2019.', 'Introduces recurrence on Transformers, relative position encoding scheme.', 'Decoder-only architectures:\\nEncoder-only architectures:\\nEncoder-decoder architectures:\\nSo far, we’ve talked about the behavior (capabilities and harms) of large language models.', 'Now, we peel open the first layer of the onion and start discussing how these models are constructed.', 'The starting point of any machine learning approach is training data, so this is where we’ll start.', 'Aside: Normally in machine learning, the training data and the test (evaluation) data are similar or at least of the same type.', 'But for large language models, the training data is just “raw text”.', 'In the rest of the lecture, we’ll talk about:\\nRecall that large language models are trained on “raw text”.', 'To be highly capable (e.g., have linguistic and world knowledge), this text should span a broad range of domains, genres, languages, etc.', 'A natural place (but not the only place) to look for such text is the web, so this will be a major focus of our attention.', 'The web is absolutely huge.', 'As a lower bound, the Google search index is 100 petabytes (reference).', 'The actual web is likely even larger, and the Deep Web is even larger than that.', 'It is worth noting that private datasets that reside in big companies are even larger than what’s available publicly.', 'For example, WalMart generates 2.5 petabytes of data each hour!', 'Common Crawl is a nonprofit organization that crawls the web and provides snapshots that are free to the public.', 'Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher.', 'The April 2021 snapshot of Common Crawl has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index.', 'Representation.', 'Despite the richness of web data, it has been noted in Bender et al, 2021 that:\\nTakeaway: it is crucial to understand and document the composition of the datasets used to train large language models.', 'WebText.', 'The WebText dataset was used to train GPT-2.', 'OpenWebText.', 'WebText was not released by OpenAI, but it was replicated (in spirit) by the OpenWebText dataset.', 'Toxicity analysis.', 'Gehman et al.', '2020, the RealToxicityPrompts paper, analyzed these two datasets and found:\\nThe Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model.', 'Analysis.', 'Dodge et al.', '2021 performed a thorough analysis of the C4 dataset.', 'Documentation levels:\\nNote: Raffel et al.', '2020 only provided scripts to recreate; cost thousands of dollars just to run these scripts.', 'Benchmark data contamination.', 'Example from the XSum summarization dataset:\\nInput: The 48-year-old former Arsenal goalkeeper played for the Royals for four years.', 'He was appointed youth academy director in 2000 and has been director of football since 2003.', 'A West Brom statement said: “He played a key role in the Championship club twice winning promotion to the Premier League in 2006 and 2012.', 'Output: West Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading.', 'There are two types of contamination:\\nNote that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage).', 'The dataset could also be responsible for various harms:\\n\\n\\nData composition.', 'Compare:\\n\\nTakeaway: The Pile contains a lot of information that’s not well covered by GPT-3’s dataset.', 'They also performed analysis of pejorative content, gender/religion biases.', 'The findings are qualitatively similar to previous work.', 'We now step back from the specifics of language modeling datasets and discuss general principles around data.', 'Two purposes:\\nDataset lifecycle (a sample of the questions from each category are provided below):\\nData statements.', 'The data statements work is specialized to NLP datasets, and covers other aspects:\\nAs an example, let’s look at the datasheet for The Pile.', 'So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation.', 'But data is a broad concept which can be studied from many other angles.', 'Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm.', 'In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant in industry.', 'Data dignity.', 'This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.', 'Documentation for datasets:\\nDatasets:\\nAnalysis of datasets:\\nFiltering datasets:\\nData ecosystems:\\n\\\\(\\\\newcommand{\\\\sV}{\\\\mathcal{V}} \\\\newcommand{\\\\sO}{\\\\mathcal{O}} \\\\newcommand{\\\\sD}{\\\\mathcal{D}} \\\\newcommand{\\\\sN}{\\\\mathcal{N}} \\\\newcommand{\\\\R}{\\\\mathbb{R}} \\\\newcommand{\\\\E}{\\\\mathbb{E}} \\\\newcommand{\\\\x}{x_{1:L}} \\\\newcommand{\\\\tx}{\\\\tilde x_{1:L}} \\\\newcommand{\\\\nl}[1]{\\\\textsf{#1}} \\\\newcommand{\\\\softmax}{\\\\text{softmax}} \\\\newcommand{\\\\TransformerBlock}{\\\\text{TransformerBlock}} \\\\newcommand{\\\\EmbedTokenWithPosition}{\\\\text{EmbedTokenWithPosition}} \\\\newcommand{\\\\SentenceEmbedding}{\\\\text{SentenceEmbedding}} \\\\newcommand{\\\\BERT}{\\\\text{BERT}} \\\\newcommand{\\\\MASK}{\\\\nl{[MASK]}} \\\\newcommand{\\\\SEP}{\\\\nl{[SEP]}} \\\\newcommand{\\\\CLS}{\\\\nl{[CLS]}} \\\\newcommand{\\\\generate}[1]{\\\\stackrel{#1}{\\\\rightsquigarrow}} \\\\newcommand{\\\\embed}{\\\\stackrel{\\\\phi}{\\\\Rightarrow}}\\\\) Last lecture, we talked about the model architecture for large language models (e.g., the Transformer).', 'In this lecture, we will discuss how to train large language models.', 'We will consider objective functions for the three types of language models:\\nWe can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers):\\nRecall that an autoregressive language model defines a conditional distribution:\\nWe define it as follows:\\nSuccinctly:\\nMaximum likelihood.', 'Let \\\\(\\\\theta\\\\) be all the parameters of large language models.', 'Let \\\\(\\\\sD\\\\) be the training data consisting of a set of sequences.', 'We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function:\\nThere’s more to say about how to efficiently optimize this function, but that’s all there is for the objective.', 'Unidirectional to bidirectional.', 'A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don’t need to generate.', 'BERT.', 'We will first present the BERT objective function, which contains two terms:\\nTake the example sequence for natural language inference (predict entailment, contradiction, or neutral):\\nThere are two special tokens:\\nUsing our notation from the previous lecture, the BERT model is defined as:\\nwhere \\\\(\\\\SentenceEmbedding(\\\\x)\\\\) returns one of 2 vectors depending on the sequence:\\n\\nBERT-large has \\\\(n_\\\\text{heads} = 16\\\\) attention heads, and a \\\\(d_\\\\text{model} = 1024\\\\) dimensional model, resulting in 355M parameters.', 'Masked language modeling.', 'The basic idea of the masked language model is to train on the prediction problem:\\nMore more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\\\(\\\\tx\\\\) and try to reconstruct the original \\\\(\\\\x\\\\).', 'Model.', 'We first define the model distribution that takes \\\\(\\\\tx\\\\) and predicts each token independently given the contextual embedding:\\nMasking function.', 'We define a (stochastic) noising function \\\\(A(\\\\tx \\\\mid \\\\x)\\\\) that:\\nHere’s how \\\\(A\\\\) is defined:\\nReducing distribution shift.', 'If we were to always replace chosen tokens in \\\\(I\\\\) with \\\\(\\\\MASK\\\\), then:\\nNext sentence prediction.', 'Recall that BERT is trained on pairs of sentences concatenated.', 'The goal of next sentence prediction is to predict whether the second sentence follows from the first or not.', '\\\\([\\\\CLS, \\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}, \\\\SEP, \\\\nl{it}, \\\\nl{was}, \\\\nl{full}] \\\\Rightarrow 1\\\\).', '\\\\([\\\\CLS, \\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}, \\\\SEP, \\\\nl{hello}, \\\\nl{world}] \\\\Rightarrow 0\\\\).', 'We will use the embedding of the \\\\(\\\\CLS\\\\) token to make this binary classification decision.', 'Dataset.', 'Let \\\\(\\\\sD\\\\) be a set of examples \\\\((\\\\x, c)\\\\) constructed as follows:\\nObjective.', 'Then the BERT objective is:\\nWe will talk about training later, but a few quick notes about BERT:\\nRoBERTa makes the following changes to BERT:\\nExample task (table-to-text generation):\\nRecall that encoder-decoder models (e.g., BART, T5):\\nBART (Bidirectional Auto-Regressive Transformers).', 'BART (Lewis et al.', '2019) is a Transformer-based encoder-decoder model.', 'BART considers the following transformations \\\\(A(\\\\tx \\\\mid \\\\x)\\\\):  Based on BERT-scaled experiments, they decided on the following transformations for the final model:\\nThey demonstrated strong results on both classification and generation tasks using fine-tuning.', 'T5 (Text-to-Text Transfer Transformer).', 'T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model.', 'Tasks:\\nThis paper experimented with many different unsupervised objectives:  and found that the “i.i.d.', 'noise, replace spans” worked well (though many objectives were similar).', 'They also cast all classical NLP tasks in a uniform framework as “text-to-text” tasks:  Note the difference in approach to classification tasks:\\nNotes:\\nNow we turn our attention to how to optimize the objective.', 'For simplicity, let’s take autogressive language modeling:\\nStochastic gradient descent (SGD).', 'A first cut is just to do stochastic gradient descent with mini-batches:\\nThe key concerns in optimization are:\\nThere are several levels that we can approach optimization:\\nADAM (adaptive moment estimation).', 'ADAM incorporates two ideas:\\nHave an adaptive (different) step size for each dimension of \\\\(\\\\theta\\\\) (inspiration from second-order methods).', 'Updating parameters.', 'Memory.', 'Using Adam increases the amount of storage from \\\\(2(\\\\text{num-params})\\\\) (from \\\\(\\\\theta_t,g_t\\\\)) to \\\\(4(\\\\text{num-params})\\\\) (from \\\\(\\\\theta_t,g_t,m_t,v_t\\\\)).', 'AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint.', 'Mixed-precision training is another method for reducing memory (Narang et al., 2018).', 'Learning rates.', 'Initialization.', 'For GPT-3:\\n\\n\\n\\n\\nAttention Is All You Need\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1 Introduction\\n2 Background\\n\\n3 Model Architecture\\n\\n\\n3.1 Encoder and Decoder Stacks\\n\\nEncoder:\\nDecoder:\\n\\n\\n\\n3.2 Attention\\n\\n3.2.1 Scaled Dot-Product Attention\\n3.2.2 Multi-Head Attention\\n3.2.3 Applications of Attention in our Model\\n\\n\\n3.3 Position-wise Feed-Forward Networks\\n3.4 Embeddings and Softmax\\n3.5 Positional Encoding\\n\\n\\n4 Why Self-Attention\\n\\n5 Training\\n\\n5.1 Training Data and Batching\\n5.2 Hardware and Schedule\\n5.3 Optimizer\\n\\n5.4 Regularization\\n\\nResidual Dropout\\nLabel Smoothing\\n\\n\\n\\n\\n\\n6 Results\\n\\n6.1 Machine Translation\\n6.2 Model Variations\\n6.3 English Constituency Parsing\\n\\n\\n\\n7 Conclusion\\n\\nAcknowledgements\\n\\n\\n\\n\\n\\n\\n\\n\\nProvided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.', 'Attention Is All You Need\\n\\n\\n\\n\\\\ANDAshish Vaswani\\nGoogle Brain\\navaswani@google.com\\n&Noam Shazeer11footnotemark: 1\\nGoogle Brain\\nnoam@google.com\\n&Niki Parmar11footnotemark: 1\\nGoogle Research\\nnikip@google.com\\n&Jakob Uszkoreit11footnotemark: 1\\nGoogle Research\\nusz@google.com\\n&Llion Jones11footnotemark: 1\\nGoogle Research\\nllion@google.com\\n&Aidan N. Gomez11footnotemark: 1 \\xa0\\xa0\\nUniversity of Toronto\\naidan@cs.toronto.edu\\n&Łukasz Kaiser11footnotemark: 1\\nGoogle Brain\\nlukaszkaiser@google.com\\n&Illia Polosukhin11footnotemark: 1\\xa0\\xa0 \\nillia.polosukhin@gmail.com\\n\\nEqual contribution.', 'Listing order is random.', 'Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.', 'Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work.', 'Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.', 'Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.', 'Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.', 'Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.', 'Work performed while at Google Brain.Work performed while at Google Research.', 'Abstract\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder.', 'The best performing models also connect the encoder and decoder through an attention mechanism.', 'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.', 'Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.', 'Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.', 'On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.', 'We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.', '1 Introduction\\n\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5].', 'Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].', 'Recurrent models typically factor computation along the symbol positions of the input and output sequences.', 'Aligning the positions to steps in computation time, they generate a sequence of hidden states htsubscriptℎ𝑡h_{t}italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, as a function of the previous hidden state ht−1subscriptℎ𝑡1h_{t-1}italic_h start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT and the input for position t𝑡titalic_t.', 'This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.', 'Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter.', 'The fundamental constraint of sequential computation, however, remains.', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19].', 'In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.', 'The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.', 'In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.', 'This makes it more difficult to learn dependencies between distant positions [12].', 'In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section\\xa03.2.', 'Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.', 'Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].', 'End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].', 'To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.', 'In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].', '3 Model Architecture\\n\\nFigure 1: The Transformer - model architecture.', 'Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].', 'Here, the encoder maps an input sequence of symbol representations (x1,…,xn)subscript𝑥1…subscript𝑥𝑛(x_{1},...,x_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) to a sequence of continuous representations 𝐳=(z1,…,zn)𝐳subscript𝑧1…subscript𝑧𝑛\\\\mathbf{z}=(z_{1},...,z_{n})bold_z = ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ).', 'Given 𝐳𝐳\\\\mathbf{z}bold_z, the decoder then generates an output sequence (y1,…,ym)subscript𝑦1…subscript𝑦𝑚(y_{1},...,y_{m})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) of symbols one element at a time.', 'At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.', 'The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure\\xa01, respectively.', '3.1 Encoder and Decoder Stacks\\n\\nEncoder:\\n\\nThe encoder is composed of a stack of N=6𝑁6N=6italic_N = 6 identical layers.', 'Each layer has two sub-layers.', 'The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.', 'We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1].', 'That is, the output of each sub-layer is LayerNorm\\u2062(x+Sublayer\\u2062(x))LayerNorm𝑥Sublayer𝑥\\\\mathrm{LayerNorm}(x+\\\\mathrm{Sublayer}(x))roman_LayerNorm ( italic_x + roman_Sublayer ( italic_x ) ), where Sublayer\\u2062(x)Sublayer𝑥\\\\mathrm{Sublayer}(x)roman_Sublayer ( italic_x ) is the function implemented by the sub-layer itself.', 'To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512subscript𝑑model512d_{\\\\text{model}}=512italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 512.', 'Decoder:\\n\\nThe decoder is also composed of a stack of N=6𝑁6N=6italic_N = 6 identical layers.', 'In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.', 'Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.', 'We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.', 'This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i𝑖iitalic_i can depend only on the known outputs at positions less than i𝑖iitalic_i.', '3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.', 'The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.', '3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure\\xa02).', 'The input consists of queries and keys of dimension dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, and values of dimension dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT.', 'We compute the dot products of the query with all keys, divide each by dksubscript𝑑𝑘\\\\sqrt{d_{k}}square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG, and apply a softmax function to obtain the weights on the values.', 'In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q𝑄Qitalic_Q.', 'The keys and values are also packed together into matrices K𝐾Kitalic_K and V𝑉Vitalic_V.', 'We compute the matrix of outputs as:\\n\\n\\n\\n\\n\\nAttention\\u2062(Q,K,V)=softmax\\u2062(Q\\u2062KTdk)\\u2062VAttention𝑄𝐾𝑉softmax𝑄superscript𝐾𝑇subscript𝑑𝑘𝑉\\\\mathrm{Attention}(Q,K,V)=\\\\mathrm{softmax}(\\\\frac{QK^{T}}{\\\\sqrt{d_{k}}})Vroman_Attention ( italic_Q , italic_K , italic_V ) = roman_softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ) italic_V\\n\\n(1)\\n\\n\\n\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention.', 'Dot-product attention is identical to our algorithm, except for the scaling factor of 1dk1subscript𝑑𝑘\\\\frac{1}{\\\\sqrt{d_{k}}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG.', 'Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.', 'While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.', 'While for small values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT [3].', 'We suspect that for large values of dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 111To illustrate why the dot products get large, assume that the components of q𝑞qitalic_q and k𝑘kitalic_k are independent random variables with mean 00 and variance 1111.', 'Then their dot product, q⋅k=∑i=1dkqi\\u2062ki⋅𝑞𝑘superscriptsubscript𝑖1subscript𝑑𝑘subscript𝑞𝑖subscript𝑘𝑖q\\\\cdot k=\\\\sum_{i=1}^{d_{k}}q_{i}k_{i}italic_q ⋅ italic_k = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, has mean 00 and variance dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT.. To counteract this effect, we scale the dot products by 1dk1subscript𝑑𝑘\\\\frac{1}{\\\\sqrt{d_{k}}}divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG.', '3.2.2 Multi-Head Attention\\n\\n\\n\\n\\nScaled Dot-Product Attention\\n\\n\\n\\n\\n\\nMulti-Head Attention\\n\\n\\n\\n\\nFigure 2: (left) Scaled Dot-Product Attention.', '(right) Multi-Head Attention consists of several attention layers running in parallel.', 'Instead of performing a single attention function with dmodelsubscript𝑑modeld_{\\\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values hℎhitalic_h times with different, learned linear projections to dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT dimensions, respectively.', 'On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT-dimensional output values.', 'These are concatenated and once again projected, resulting in the final values, as depicted in Figure\\xa02.', 'Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.', 'With a single attention head, averaging inhibits this.', 'MultiHead\\u2062(Q,K,V)MultiHead𝑄𝐾𝑉\\\\displaystyle\\\\mathrm{MultiHead}(Q,K,V)roman_MultiHead ( italic_Q , italic_K , italic_V )\\n=Concat\\u2062(head1,…,headh)\\u2062WOabsentConcatsubscripthead1…subscriptheadhsuperscript𝑊𝑂\\\\displaystyle=\\\\mathrm{Concat}(\\\\mathrm{head_{1}},...,\\\\mathrm{head_{h}})W^{O}= roman_Concat ( roman_head start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , roman_head start_POSTSUBSCRIPT roman_h end_POSTSUBSCRIPT ) italic_W start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT\\n\\n\\n\\n\\nwhere\\u2062headiwheresubscriptheadi\\\\displaystyle\\\\text{where}~{}\\\\mathrm{head_{i}}where roman_head start_POSTSUBSCRIPT roman_i end_POSTSUBSCRIPT\\n=Attention\\u2062(Q\\u2062WiQ,K\\u2062WiK,V\\u2062WiV)absentAttention𝑄subscriptsuperscript𝑊𝑄𝑖𝐾subscriptsuperscript𝑊𝐾𝑖𝑉subscriptsuperscript𝑊𝑉𝑖\\\\displaystyle=\\\\mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})= roman_Attention ( italic_Q italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_K italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_V italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )\\n\\n\\n\\n\\n\\nWhere the projections are parameter matrices WiQ∈ℝdmodel×dksubscriptsuperscript𝑊𝑄𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑘W^{Q}_{i}\\\\in\\\\mathbb{R}^{d_{\\\\text{model}}\\\\times d_{k}}italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WiK∈ℝdmodel×dksubscriptsuperscript𝑊𝐾𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑘W^{K}_{i}\\\\in\\\\mathbb{R}^{d_{\\\\text{model}}\\\\times d_{k}}italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, WiV∈ℝdmodel×dvsubscriptsuperscript𝑊𝑉𝑖superscriptℝsubscript𝑑modelsubscript𝑑𝑣W^{V}_{i}\\\\in\\\\mathbb{R}^{d_{\\\\text{model}}\\\\times d_{v}}italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and WO∈ℝh\\u2062dv×dmodelsuperscript𝑊𝑂superscriptℝℎsubscript𝑑𝑣subscript𝑑modelW^{O}\\\\in\\\\mathbb{R}^{hd_{v}\\\\times d_{\\\\text{model}}}italic_W start_POSTSUPERSCRIPT italic_O end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_h italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT.', 'In this work we employ h=8ℎ8h=8italic_h = 8 parallel attention layers, or heads.', 'For each of these we use dk=dv=dmodel/h=64subscript𝑑𝑘subscript𝑑𝑣subscript𝑑modelℎ64d_{k}=d_{v}=d_{\\\\text{model}}/h=64italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT / italic_h = 64.', 'Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.', '3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n\\n•\\n\\nIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.', 'This allows every position in the decoder to attend over all positions in the input sequence.', 'This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].', '•\\n\\nThe encoder contains self-attention layers.', 'In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.', 'Each position in the encoder can attend to all positions in the previous layer of the encoder.', '•\\n\\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.', 'We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.', 'We implement this inside of scaled dot-product attention by masking out (setting to −∞-\\\\infty- ∞) all values in the input of the softmax which correspond to illegal connections.', 'See Figure\\xa02.', '3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.', 'This consists of two linear transformations with a ReLU activation in between.', 'FFN\\u2062(x)=max\\u2061(0,x\\u2062W1+b1)\\u2062W2+b2FFN𝑥0𝑥subscript𝑊1subscript𝑏1subscript𝑊2subscript𝑏2\\\\mathrm{FFN}(x)=\\\\max(0,xW_{1}+b_{1})W_{2}+b_{2}roman_FFN ( italic_x ) = roman_max ( 0 , italic_x italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT\\n\\n(2)\\n\\n\\n\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer.', 'Another way of describing this is as two convolutions with kernel size 1.', 'The dimensionality of input and output is dmodel=512subscript𝑑model512d_{\\\\text{model}}=512italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT = 512, and the inner-layer has dimensionality df\\u2062f=2048subscript𝑑𝑓𝑓2048d_{ff}=2048italic_d start_POSTSUBSCRIPT italic_f italic_f end_POSTSUBSCRIPT = 2048.', '3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodelsubscript𝑑modeld_{\\\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT.', 'We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.', 'In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30].', 'In the embedding layers, we multiply those weights by dmodelsubscript𝑑model\\\\sqrt{d_{\\\\text{model}}}square-root start_ARG italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_ARG.', '3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.', 'To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks.', 'The positional encodings have the same dimension dmodelsubscript𝑑modeld_{\\\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT as the embeddings, so that the two can be summed.', 'There are many choices of positional encodings, learned and fixed [9].', 'In this work, we use sine and cosine functions of different frequencies:\\n\\n\\n\\n\\n\\nP\\u2062E(p\\u2062o\\u2062s,2\\u2062i)=s\\u2062i\\u2062n\\u2062(p\\u2062o\\u2062s/100002\\u2062i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖𝑠𝑖𝑛𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\\\displaystyle PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\\\\text{model}}})italic_P italic_E start_POSTSUBSCRIPT ( italic_p italic_o italic_s , 2 italic_i ) end_POSTSUBSCRIPT = italic_s italic_i italic_n ( italic_p italic_o italic_s / 10000 start_POSTSUPERSCRIPT 2 italic_i / italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT )\\n\\n\\n\\n\\nP\\u2062E(p\\u2062o\\u2062s,2\\u2062i+1)=c\\u2062o\\u2062s\\u2062(p\\u2062o\\u2062s/100002\\u2062i/dmodel)𝑃subscript𝐸𝑝𝑜𝑠2𝑖1𝑐𝑜𝑠𝑝𝑜𝑠superscript100002𝑖subscript𝑑model\\\\displaystyle PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\\\\text{model}}})italic_P italic_E start_POSTSUBSCRIPT ( italic_p italic_o italic_s , 2 italic_i + 1 ) end_POSTSUBSCRIPT = italic_c italic_o italic_s ( italic_p italic_o italic_s / 10000 start_POSTSUPERSCRIPT 2 italic_i / italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT )\\n\\n\\n\\n\\n\\nwhere p\\u2062o\\u2062s𝑝𝑜𝑠positalic_p italic_o italic_s is the position and i𝑖iitalic_i is the dimension.', 'That is, each dimension of the positional encoding corresponds to a sinusoid.', 'The wavelengths form a geometric progression from 2\\u2062π2𝜋2\\\\pi2 italic_π to 10000⋅2\\u2062π⋅100002𝜋10000\\\\cdot 2\\\\pi10000 ⋅ 2 italic_π.', 'We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k𝑘kitalic_k, P\\u2062Ep\\u2062o\\u2062s+k𝑃subscript𝐸𝑝𝑜𝑠𝑘PE_{pos+k}italic_P italic_E start_POSTSUBSCRIPT italic_p italic_o italic_s + italic_k end_POSTSUBSCRIPT can be represented as a linear function of P\\u2062Ep\\u2062o\\u2062s𝑃subscript𝐸𝑝𝑜𝑠PE_{pos}italic_P italic_E start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT.', 'We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table\\xa03 row (E)).', 'We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.', '4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1,…,xn)subscript𝑥1…subscript𝑥𝑛(x_{1},...,x_{n})( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) to another sequence of equal length (z1,…,zn)subscript𝑧1…subscript𝑧𝑛(z_{1},...,z_{n})( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ), with xi,zi∈ℝdsubscript𝑥𝑖subscript𝑧𝑖superscriptℝ𝑑x_{i},z_{i}\\\\in\\\\mathbb{R}^{d}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, such as a hidden layer in a typical sequence transduction encoder or decoder.', 'Motivating our use of self-attention we consider three desiderata.', 'One is the total computational complexity per layer.', 'Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.', 'The third is the path length between long-range dependencies in the network.', 'Learning long-range dependencies is a key challenge in many sequence transduction tasks.', 'One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network.', 'The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12].', 'Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.', 'Table 1: \\nMaximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types.', 'n𝑛nitalic_n is the sequence length, d𝑑ditalic_d is the representation dimension, k𝑘kitalic_k is the kernel size of convolutions and r𝑟ritalic_r the size of the neighborhood in restricted self-attention.', 'Layer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\n\\n\\n\\n\\n\\n\\nOperations\\n\\n\\n\\n\\nSelf-Attention\\nO\\u2062(n2⋅d)𝑂⋅superscript𝑛2𝑑O(n^{2}\\\\cdot d)italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ⋅ italic_d )\\nO\\u2062(1)𝑂1O(1)italic_O ( 1 )\\nO\\u2062(1)𝑂1O(1)italic_O ( 1 )\\n\\n\\nRecurrent\\nO\\u2062(n⋅d2)𝑂⋅𝑛superscript𝑑2O(n\\\\cdot d^{2})italic_O ( italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )\\nO\\u2062(n)𝑂𝑛O(n)italic_O ( italic_n )\\nO\\u2062(n)𝑂𝑛O(n)italic_O ( italic_n )\\n\\n\\nConvolutional\\nO\\u2062(k⋅n⋅d2)𝑂⋅𝑘𝑛superscript𝑑2O(k\\\\cdot n\\\\cdot d^{2})italic_O ( italic_k ⋅ italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )\\nO\\u2062(1)𝑂1O(1)italic_O ( 1 )\\nO\\u2062(l\\u2062o\\u2062gk\\u2062(n))𝑂𝑙𝑜subscript𝑔𝑘𝑛O(log_{k}(n))italic_O ( italic_l italic_o italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_n ) )\\n\\n\\nSelf-Attention (restricted)\\nO\\u2062(r⋅n⋅d)𝑂⋅𝑟𝑛𝑑O(r\\\\cdot n\\\\cdot d)italic_O ( italic_r ⋅ italic_n ⋅ italic_d )\\nO\\u2062(1)𝑂1O(1)italic_O ( 1 )\\nO\\u2062(n/r)𝑂𝑛𝑟O(n/r)italic_O ( italic_n / italic_r )\\n\\n\\n\\n\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O\\u2062(n)𝑂𝑛O(n)italic_O ( italic_n ) sequential operations.', 'In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n𝑛nitalic_n is smaller than the representation dimensionality d𝑑ditalic_d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations.', 'To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r𝑟ritalic_r in the input sequence centered around the respective output position.', 'This would increase the maximum path length to O\\u2062(n/r)𝑂𝑛𝑟O(n/r)italic_O ( italic_n / italic_r ).', 'We plan to investigate this approach further in future work.', 'A single convolutional layer with kernel width k<n𝑘𝑛k<nitalic_k < italic_n does not connect all pairs of input and output positions.', 'Doing so requires a stack of O\\u2062(n/k)𝑂𝑛𝑘O(n/k)italic_O ( italic_n / italic_k ) convolutional layers in the case of contiguous kernels, or O\\u2062(l\\u2062o\\u2062gk\\u2062(n))𝑂𝑙𝑜subscript𝑔𝑘𝑛O(log_{k}(n))italic_O ( italic_l italic_o italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_n ) ) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network.', 'Convolutional layers are generally more expensive than recurrent layers, by a factor of k𝑘kitalic_k.', 'Separable convolutions [6], however, decrease the complexity considerably, to O\\u2062(k⋅n⋅d+n⋅d2)𝑂⋅𝑘𝑛𝑑⋅𝑛superscript𝑑2O(k\\\\cdot n\\\\cdot d+n\\\\cdot d^{2})italic_O ( italic_k ⋅ italic_n ⋅ italic_d + italic_n ⋅ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ).', 'Even with k=n𝑘𝑛k=nitalic_k = italic_n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.', 'As side benefit, self-attention could yield more interpretable models.', 'We inspect attention distributions from our models and present and discuss examples in the appendix.', 'Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.', '5 Training\\n\\nThis section describes the training regime for our models.', '5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.', 'Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens.', 'For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38].', 'Sentence pairs were batched together by approximate sequence length.', 'Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.', '5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs.', 'For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds.', 'We trained the base models for a total of 100,000 steps or 12 hours.', 'For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.', 'The big models were trained for 300,000 steps (3.5 days).', '5.3 Optimizer\\n\\nWe used the Adam optimizer\\xa0[20] with β1=0.9subscript𝛽10.9\\\\beta_{1}=0.9italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9, β2=0.98subscript𝛽20.98\\\\beta_{2}=0.98italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.98 and ϵ=10−9italic-ϵsuperscript109\\\\epsilon=10^{-9}italic_ϵ = 10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT.', 'We varied the learning rate over the course of training, according to the formula:\\n\\n\\n\\n\\n\\nl\\u2062r\\u2062a\\u2062t\\u2062e=dmodel−0.5⋅min\\u2061(s\\u2062t\\u2062e\\u2062p\\u2062_\\u2062n\\u2062u\\u2062m−0.5,s\\u2062t\\u2062e\\u2062p\\u2062_\\u2062n\\u2062u\\u2062m⋅w\\u2062a\\u2062r\\u2062m\\u2062u\\u2062p\\u2062_\\u2062s\\u2062t\\u2062e\\u2062p\\u2062s−1.5)𝑙𝑟𝑎𝑡𝑒⋅superscriptsubscript𝑑model0.5𝑠𝑡𝑒𝑝_𝑛𝑢superscript𝑚0.5⋅𝑠𝑡𝑒𝑝_𝑛𝑢𝑚𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝superscript𝑠1.5lrate=d_{\\\\text{model}}^{-0.5}\\\\cdot\\\\min({step\\\\_num}^{-0.5},{step\\\\_num}\\\\cdot{%\\nwarmup\\\\_steps}^{-1.5})italic_l italic_r italic_a italic_t italic_e = italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT ⋅ roman_min ( italic_s italic_t italic_e italic_p _ italic_n italic_u italic_m start_POSTSUPERSCRIPT - 0.5 end_POSTSUPERSCRIPT , italic_s italic_t italic_e italic_p _ italic_n italic_u italic_m ⋅ italic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s start_POSTSUPERSCRIPT - 1.5 end_POSTSUPERSCRIPT )\\n\\n(3)\\n\\n\\n\\n\\nThis corresponds to increasing the learning rate linearly for the first w\\u2062a\\u2062r\\u2062m\\u2062u\\u2062p\\u2062_\\u2062s\\u2062t\\u2062e\\u2062p\\u2062s𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝𝑠warmup\\\\_stepsitalic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.', 'We used w\\u2062a\\u2062r\\u2062m\\u2062u\\u2062p\\u2062_\\u2062s\\u2062t\\u2062e\\u2062p\\u2062s=4000𝑤𝑎𝑟𝑚𝑢𝑝_𝑠𝑡𝑒𝑝𝑠4000warmup\\\\_steps=4000italic_w italic_a italic_r italic_m italic_u italic_p _ italic_s italic_t italic_e italic_p italic_s = 4000.', '5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\n\\nResidual Dropout\\n\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized.', 'In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.', 'For the base model, we use a rate of Pd\\u2062r\\u2062o\\u2062p=0.1subscript𝑃𝑑𝑟𝑜𝑝0.1P_{drop}=0.1italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT = 0.1.', 'Label Smoothing\\n\\nDuring training, we employed label smoothing of value ϵl\\u2062s=0.1subscriptitalic-ϵ𝑙𝑠0.1\\\\epsilon_{ls}=0.1italic_ϵ start_POSTSUBSCRIPT italic_l italic_s end_POSTSUBSCRIPT = 0.1 [36].', 'This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.', '6 Results\\n\\n\\n6.1 Machine Translation\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.', 'Model\\nBLEU\\n\\nTraining Cost (FLOPs)\\n\\n\\nEN-DE\\nEN-FR\\n\\nEN-DE\\nEN-FR\\n\\n\\nByteNet [18]\\n\\n23.75\\n\\n\\n\\n\\n\\n\\nDeep-Att + PosUnk [39]\\n\\n\\n39.2\\n\\n\\n1.0⋅1020⋅1.0superscript10201.0\\\\cdot 10^{20}1.0 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\\n\\n\\nGNMT + RL [38]\\n\\n24.6\\n39.92\\n\\n2.3⋅1019⋅2.3superscript10192.3\\\\cdot 10^{19}2.3 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\\n1.4⋅1020⋅1.4superscript10201.4\\\\cdot 10^{20}1.4 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\\n\\n\\nConvS2S [9]\\n\\n25.16\\n40.46\\n\\n9.6⋅1018⋅9.6superscript10189.6\\\\cdot 10^{18}9.6 ⋅ 10 start_POSTSUPERSCRIPT 18 end_POSTSUPERSCRIPT\\n1.5⋅1020⋅1.5superscript10201.5\\\\cdot 10^{20}1.5 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\\n\\n\\nMoE [32]\\n\\n26.03\\n40.56\\n\\n2.0⋅1019⋅2.0superscript10192.0\\\\cdot 10^{19}2.0 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\\n1.2⋅1020⋅1.2superscript10201.2\\\\cdot 10^{20}1.2 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\\n\\n\\n\\nDeep-Att + PosUnk Ensemble [39]\\n\\n\\n40.4\\n\\n\\n8.0⋅1020⋅8.0superscript10208.0\\\\cdot 10^{20}8.0 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\\n\\n\\nGNMT + RL Ensemble [38]\\n\\n26.30\\n41.16\\n\\n1.8⋅1020⋅1.8superscript10201.8\\\\cdot 10^{20}1.8 ⋅ 10 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT\\n1.1⋅1021⋅1.1superscript10211.1\\\\cdot 10^{21}1.1 ⋅ 10 start_POSTSUPERSCRIPT 21 end_POSTSUPERSCRIPT\\n\\n\\nConvS2S Ensemble [9]\\n\\n26.36\\n41.29\\n\\n7.7⋅1019⋅7.7superscript10197.7\\\\cdot 10^{19}7.7 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\\n1.2⋅1021⋅1.2superscript10211.2\\\\cdot 10^{21}1.2 ⋅ 10 start_POSTSUPERSCRIPT 21 end_POSTSUPERSCRIPT\\n\\n\\n\\nTransformer (base model)\\n27.3\\n38.1\\n\\n3.3⋅𝟏𝟎𝟏𝟖bold-⋅3.3superscript10183.3\\\\cdot 10^{18}bold_3.3 bold_⋅ bold_10 start_POSTSUPERSCRIPT bold_18 end_POSTSUPERSCRIPT\\n\\n\\nTransformer (big)\\n28.4\\n41.8\\n\\n2.3⋅1019⋅2.3superscript10192.3\\\\cdot 10^{19}2.3 ⋅ 10 start_POSTSUPERSCRIPT 19 end_POSTSUPERSCRIPT\\n\\n\\n\\n\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table\\xa02) outperforms the best previously reported models (including ensembles) by more than 2.02.02.02.0 BLEU, establishing a new state-of-the-art BLEU score of 28.428.428.428.4.', 'The configuration of this model is listed in the bottom line of Table\\xa03.', 'Training took 3.53.53.53.5 days on 8888 P100 GPUs.', 'Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.', 'On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.041.041.041.0, outperforming all of the previously published single models, at less than 1/4141/41 / 4 the training cost of the previous state-of-the-art model.', 'The Transformer (big) model trained for English-to-French used dropout rate Pd\\u2062r\\u2062o\\u2062p=0.1subscript𝑃𝑑𝑟𝑜𝑝0.1P_{drop}=0.1italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT = 0.1, instead of 0.30.30.30.3.', 'For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals.', 'For the big models, we averaged the last 20 checkpoints.', 'We used beam search with a beam size of 4444 and length penalty α=0.6𝛼0.6\\\\alpha=0.6italic_α = 0.6 [38].', 'These hyperparameters were chosen after experimentation on the development set.', 'We set the maximum output length during inference to input length + 50505050, but terminate early when possible [38].', 'Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature.', 'We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 222We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively..\\n\\n\\n\\n\\n6.2 Model Variations\\n\\nTable 3: Variations on the Transformer architecture.', 'Unlisted values are identical to those of the base model.', 'All metrics are on the English-to-German translation development set, newstest2013.', 'Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.', 'N𝑁Nitalic_N\\ndmodelsubscript𝑑modeld_{\\\\text{model}}italic_d start_POSTSUBSCRIPT model end_POSTSUBSCRIPT\\ndffsubscript𝑑ffd_{\\\\text{ff}}italic_d start_POSTSUBSCRIPT ff end_POSTSUBSCRIPT\\nhℎhitalic_h\\ndksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT\\ndvsubscript𝑑𝑣d_{v}italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT\\nPd\\u2062r\\u2062o\\u2062psubscript𝑃𝑑𝑟𝑜𝑝P_{drop}italic_P start_POSTSUBSCRIPT italic_d italic_r italic_o italic_p end_POSTSUBSCRIPT\\nϵl\\u2062ssubscriptitalic-ϵ𝑙𝑠\\\\epsilon_{ls}italic_ϵ start_POSTSUBSCRIPT italic_l italic_s end_POSTSUBSCRIPT\\ntrain\\nPPL\\nBLEU\\nparams\\n\\n\\n\\nsteps\\n(dev)\\n(dev)\\n×106absentsuperscript106\\\\times 10^{6}× 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT\\n\\n\\n\\n\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n\\n\\n\\n\\n(A)\\n\\n\\n\\n\\n1\\n512\\n512\\n\\n\\n\\n5.29\\n24.9\\n\\n\\n\\n\\n\\n\\n4\\n128\\n128\\n\\n\\n\\n5.00\\n25.5\\n\\n\\n\\n\\n\\n\\n16\\n32\\n32\\n\\n\\n\\n4.91\\n25.8\\n\\n\\n\\n\\n\\n\\n32\\n16\\n16\\n\\n\\n\\n5.01\\n25.4\\n\\n\\n\\n\\n\\n(B)\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\n\\n5.16\\n25.1\\n58\\n\\n\\n\\n\\n\\n\\n32\\n\\n\\n\\n\\n5.01\\n25.4\\n60\\n\\n\\n\\n\\n(C)\\n\\n2\\n\\n\\n\\n\\n\\n\\n\\n\\n6.11\\n23.7\\n36\\n\\n\\n4\\n\\n\\n\\n\\n\\n\\n\\n\\n5.19\\n25.3\\n50\\n\\n\\n8\\n\\n\\n\\n\\n\\n\\n\\n\\n4.88\\n25.5\\n80\\n\\n\\n\\n256\\n\\n\\n32\\n32\\n\\n\\n\\n5.75\\n24.5\\n28\\n\\n\\n\\n1024\\n\\n\\n128\\n128\\n\\n\\n\\n4.66\\n26.0\\n168\\n\\n\\n\\n\\n1024\\n\\n\\n\\n\\n\\n\\n5.12\\n25.4\\n53\\n\\n\\n\\n\\n4096\\n\\n\\n\\n\\n\\n\\n4.75\\n26.2\\n90\\n\\n\\n\\n\\n(D)\\n\\n\\n\\n\\n\\n\\n\\n0.0\\n\\n\\n5.77\\n24.6\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.2\\n\\n\\n4.95\\n25.5\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.0\\n\\n4.67\\n25.3\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.2\\n\\n5.47\\n25.7\\n\\n\\n\\n\\n\\n(E)\\n\\npositional embedding instead of sinusoids\\n\\n4.92\\n25.7\\n\\n\\n\\n\\n\\nbig\\n6\\n1024\\n4096\\n16\\n\\n\\n0.3\\n\\n300K\\n4.33\\n26.4\\n213\\n\\n\\n\\n\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.', 'We used beam search as described in the previous section, but no checkpoint averaging.', 'We present these results in Table\\xa03.', 'In Table\\xa03 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2.', 'While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.', 'In Table\\xa03 rows (B), we observe that reducing the attention key size dksubscript𝑑𝑘d_{k}italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT hurts model quality.', 'This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial.', 'We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting.', 'In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.', '6.3 English Constituency Parsing\\n\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)\\n\\n\\n\\nParser\\nTraining\\nWSJ 23 F1\\n\\n\\nVinyals & Kaiser el al.', '(2014) [37]\\n\\nWSJ only, discriminative\\n88.3\\n\\n\\nPetrov et al.', '(2006) [29]\\n\\nWSJ only, discriminative\\n90.4\\n\\n\\nZhu et al.', '(2013) [40]\\n\\nWSJ only, discriminative\\n90.4\\n\\n\\nDyer et al.', '(2016) [8]\\n\\nWSJ only, discriminative\\n91.7\\n\\n\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\n\\n\\nZhu et al.', '(2013) [40]\\n\\nsemi-supervised\\n91.3\\n\\n\\nHuang & Harper (2009) [14]\\n\\nsemi-supervised\\n91.3\\n\\n\\nMcClosky et al.', '(2006) [26]\\n\\nsemi-supervised\\n92.1\\n\\n\\nVinyals & Kaiser el al.', '(2014) [37]\\n\\nsemi-supervised\\n92.1\\n\\n\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\n\\n\\nLuong et al.', '(2015) [23]\\n\\nmulti-task\\n93.0\\n\\n\\nDyer et al.', '(2016) [8]\\n\\ngenerative\\n93.3\\n\\n\\n\\n\\n\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing.', 'This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input.', 'Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37].', 'We trained a 4-layer transformer with dm\\u2062o\\u2062d\\u2062e\\u2062l=1024subscript𝑑𝑚𝑜𝑑𝑒𝑙1024d_{model}=1024italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences.', 'We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37].', 'We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.', 'We performed only a small number of experiments to select the dropout, both attention and residual (section\\xa05.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model.', 'During inference, we increased the maximum output length to input length + 300300300300.', 'We used a beam size of 21212121 and α=0.3𝛼0.3\\\\alpha=0.3italic_α = 0.3 for both WSJ only and the semi-supervised setting.', 'Our results in Table\\xa04 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8].', 'In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.', '7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.', 'For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers.', 'On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.', 'In the former task our best model outperforms even all previously reported ensembles.', 'We are excited about the future of attention-based models and plan to apply them to other tasks.', 'We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.', 'Making generation less sequential is another research goals of ours.', 'The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor.', 'Acknowledgements\\n\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for\\ntheir fruitful comments, corrections and inspiration.', 'References\\n\\n\\n[1]\\n\\nJimmy\\xa0Lei Ba, Jamie\\xa0Ryan Kiros, and Geoffrey\\xa0E Hinton.', 'Layer normalization.', 'arXiv preprint arXiv:1607.06450, 2016.', '[2]\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.', 'Neural machine translation by jointly learning to align and\\ntranslate.', 'CoRR, abs/1409.0473, 2014.', '[3]\\n\\nDenny Britz, Anna Goldie, Minh-Thang Luong, and Quoc\\xa0V. Le.', 'Massive exploration of neural machine translation architectures.', 'CoRR, abs/1703.03906, 2017.', '[4]\\n\\nJianpeng Cheng, Li\\xa0Dong, and Mirella Lapata.', 'Long short-term memory-networks for machine reading.', 'arXiv preprint arXiv:1601.06733, 2016.', '[5]\\n\\nKyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger\\nSchwenk, and Yoshua Bengio.', 'Learning phrase representations using rnn encoder-decoder for\\nstatistical machine translation.', 'CoRR, abs/1406.1078, 2014.', '[6]\\n\\nFrancois Chollet.', 'Xception: Deep learning with depthwise separable convolutions.', 'arXiv preprint arXiv:1610.02357, 2016.', '[7]\\n\\nJunyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua\\nBengio.', 'Empirical evaluation of gated recurrent neural networks on sequence\\nmodeling.', 'CoRR, abs/1412.3555, 2014.', '[8]\\n\\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah\\xa0A. Smith.', 'Recurrent neural network grammars.', 'In Proc.', 'of NAACL, 2016.', '[9]\\n\\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann\\xa0N. Dauphin.', 'Convolutional sequence to sequence learning.', 'arXiv preprint arXiv:1705.03122v2, 2017.', '[10]\\n\\nAlex Graves.', 'Generating sequences with recurrent neural networks.', 'arXiv preprint arXiv:1308.0850, 2013.', '[11]\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.', 'Deep residual learning for image recognition.', 'In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition, pages 770–778, 2016.', '[12]\\n\\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber.', 'Gradient flow in recurrent nets: the difficulty of learning long-term\\ndependencies, 2001.', '[13]\\n\\nSepp Hochreiter and Jürgen Schmidhuber.', 'Long short-term memory.', 'Neural computation, 9(8):1735–1780, 1997.', '[14]\\n\\nZhongqiang Huang and Mary Harper.', 'Self-training PCFG grammars with latent annotations across\\nlanguages.', 'In Proceedings of the 2009 Conference on Empirical Methods in\\nNatural Language Processing, pages 832–841.', 'ACL, August 2009.', '[15]\\n\\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.', 'Exploring the limits of language modeling.', 'arXiv preprint arXiv:1602.02410, 2016.', '[16]\\n\\nŁukasz Kaiser and Samy Bengio.', 'Can active memory replace attention?', 'In Advances in Neural Information Processing Systems, (NIPS),\\n2016.', '[17]\\n\\nŁukasz Kaiser and Ilya Sutskever.', 'Neural GPUs learn algorithms.', 'In International Conference on Learning Representations\\n(ICLR), 2016.', '[18]\\n\\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van\\xa0den Oord, Alex\\nGraves, and Koray Kavukcuoglu.', 'Neural machine translation in linear time.', 'arXiv preprint arXiv:1610.10099v2, 2017.', '[19]\\n\\nYoon Kim, Carl Denton, Luong Hoang, and Alexander\\xa0M. Rush.', 'Structured attention networks.', 'In International Conference on Learning Representations, 2017.', '[20]\\n\\nDiederik Kingma and Jimmy Ba.', 'Adam: A method for stochastic optimization.', 'In ICLR, 2015.', '[21]\\n\\nOleksii Kuchaiev and Boris Ginsburg.', 'Factorization tricks for LSTM networks.', 'arXiv preprint arXiv:1703.10722, 2017.', '[22]\\n\\nZhouhan Lin, Minwei Feng, Cicero Nogueira\\xa0dos Santos, Mo\\xa0Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio.', 'A structured self-attentive sentence embedding.', 'arXiv preprint arXiv:1703.03130, 2017.', '[23]\\n\\nMinh-Thang Luong, Quoc\\xa0V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.', 'Multi-task sequence to sequence learning.', 'arXiv preprint arXiv:1511.06114, 2015.', '[24]\\n\\nMinh-Thang Luong, Hieu Pham, and Christopher\\xa0D Manning.', 'Effective approaches to attention-based neural machine translation.', 'arXiv preprint arXiv:1508.04025, 2015.', '[25]\\n\\nMitchell\\xa0P Marcus, Mary\\xa0Ann Marcinkiewicz, and Beatrice Santorini.', 'Building a large annotated corpus of english: The penn treebank.', 'Computational linguistics, 19(2):313–330, 1993.', '[26]\\n\\nDavid McClosky, Eugene Charniak, and Mark Johnson.', 'Effective self-training for parsing.', 'In Proceedings of the Human Language Technology Conference of\\nthe NAACL, Main Conference, pages 152–159.', 'ACL, June 2006.', '[27]\\n\\nAnkur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit.', 'A decomposable attention model.', 'In Empirical Methods in Natural Language Processing, 2016.', '[28]\\n\\nRomain Paulus, Caiming Xiong, and Richard Socher.', 'A deep reinforced model for abstractive summarization.', 'arXiv preprint arXiv:1705.04304, 2017.', '[29]\\n\\nSlav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.', 'Learning accurate, compact, and interpretable tree annotation.', 'In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages\\n433–440.', 'ACL, July 2006.', '[30]\\n\\nOfir Press and Lior Wolf.', 'Using the output embedding to improve language models.', 'arXiv preprint arXiv:1608.05859, 2016.', '[31]\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch.', 'Neural machine translation of rare words with subword units.', 'arXiv preprint arXiv:1508.07909, 2015.', '[32]\\n\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,\\nGeoffrey Hinton, and Jeff Dean.', 'Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer.', 'arXiv preprint arXiv:1701.06538, 2017.', '[33]\\n\\nNitish Srivastava, Geoffrey\\xa0E Hinton, Alex Krizhevsky, Ilya Sutskever, and\\nRuslan Salakhutdinov.', 'Dropout: a simple way to prevent neural networks from overfitting.', 'Journal of Machine Learning Research, 15(1):1929–1958, 2014.', '[34]\\n\\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus.', 'End-to-end memory networks.', 'In C.\\xa0Cortes, N.\\xa0D. Lawrence, D.\\xa0D. Lee, M.\\xa0Sugiyama, and R.\\xa0Garnett,\\neditors, Advances in Neural Information Processing Systems 28, pages\\n2440–2448.', 'Curran Associates, Inc., 2015.', '[35]\\n\\nIlya Sutskever, Oriol Vinyals, and Quoc\\xa0VV Le.', 'Sequence to sequence learning with neural networks.', 'In Advances in Neural Information Processing Systems, pages\\n3104–3112, 2014.', '[36]\\n\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and\\nZbigniew Wojna.', 'Rethinking the inception architecture for computer vision.', 'CoRR, abs/1512.00567, 2015.', '[37]\\n\\nVinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton.', 'Grammar as a foreign language.', 'In Advances in Neural Information Processing Systems, 2015.', '[38]\\n\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc\\xa0V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et\\xa0al.', 'Google’s neural machine translation system: Bridging the gap between\\nhuman and machine translation.', 'arXiv preprint arXiv:1609.08144, 2016.', '[39]\\n\\nJie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu.', 'Deep recurrent models with fast-forward connections for neural\\nmachine translation.', 'CoRR, abs/1606.04199, 2016.', '[40]\\n\\nMuhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu.', 'Fast and accurate shift-reduce constituent parsing.', 'In Proceedings of the 51st Annual Meeting of the ACL (Volume 1:\\nLong Papers), pages 434–443.', 'ACL, August 2013.', 'Attention Visualizations\\n\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6.', 'Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making…more difficult’.', 'Attentions here shown only for the word ‘making’.', 'Different colors represent different heads.', 'Best viewed in color.', 'Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution.', 'Top: Full attentions for head 5.', 'Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6.', 'Note that the attentions are very sharp for this word.', 'Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence.', 'We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6.', 'The heads clearly learned to perform different tasks.', 'Generated  on Tue Apr 30 16:03:52 2024 by LaTeXML\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nshapes\\n\\n\\nB-.05emi-.025em b-.08em\\n    T-.1667em.7exE-.125emX\\n\\n\\n\\n\\n\\n\\n\\nGPT (Generative Pre-trained Transformer) – A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions\\n    Gokul\\xa0Yenduri, Ramalingam\\xa0M, Chemmalar\\xa0Selvi\\xa0G, Supriya\\xa0Y, Gautam\\xa0Srivastava, Praveen\\xa0Kumar\\xa0Reddy\\xa0Maddikunta, Deepti\\xa0Raj\\xa0G, Rutvij\\xa0H\\xa0Jhaveri, Prabadevi\\xa0B, Weizheng\\xa0Wang, Athanasios\\xa0V. Vasilakos, and Thippa\\xa0Reddy\\xa0Gadekallu\\n\\nGokul\\xa0Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y, Praveen Kumar Reddy Maddikunta, Deepti Raj G, Prabadevi B are with the School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu- 632014, India (Emails: { gokul.yenduri, ramalingam.m, chemmalarselvi.g, supriya.d, praveenkumarreddy, deeptiraj.g2020, prabadevi.b }@vit.ac.in)\\n\\nGautam Srivastava is with the Dept.', 'of Math and Computer Science, Brandon University, Canada, and the Research Centre for Interneural Computing, China Medical University, Taichung, Taiwan as well as Dept.', 'of Computer Science and Math, Lebanese American University, Beirut, Lebanon (email: srivastavag@brandonu.ca)\\n\\nRutvij H Jhaveri is with the Department of Computer Science and Engineering, School of Technology, Pandit Deendayal Energy University, India, (Email: rutvij.jhaveri@sot.pdpu.ac.in).', 'Weizheng Wang is with the Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China, (E-mail: weizheng.wang@ieee.org).', 'Athanasios V. Vasilakos is with the Center for AI Research (CAIR),University of Agder(UiA), Grimstad, Norway, (Email: thanos.vasilakos@uia.no).', 'Thippa Reddy Gadekallu is with the School of Information Technology and Engineering, Vellore Institute of Technology, Vellore 632014, India, Lovely Professional University, Phagwara, India, Department of Electrical and Computer Engineering, Lebanese American University, Byblos, Lebanon, Jiaxing University , Jiaxing 314001, China, Zhongda Group, China, 314312 (E-mail: thippareddy@ieee.org).', 'June 14, 2024\\n==========================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Generative Pre-trained Transformer (GPT) represent a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans.', 'GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks.', 'Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review.', 'This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications.', 'In this review, we also explored the potential challenges and limitations of a GPT.', 'Furthermore, we discuss potential solutions and future directions.', 'Overall, this paper aims to provide a comprehensive understanding of GPT, enabling technologies, their impact on various applications, emerging challenges, and potential solutions.', 'Generative Pre-trained Transformer, Natural language processing, Artificial Intelligence\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n    List of key acronyms only if it is repeated \\n    \\n    \\n        Acronyms     Description\\n\\n        \\n        AI     Artificial Intelligence \\n \\n        AR     Augmented Reality \\n        \\n \\n        BERT     Bidirectional Encoder Representations from Transformers \\n        \\n\\n        BGN     Boneh–Goh–Nissim\\n        \\n \\n        CNN     ConvolutionalNeural Network \\n        \\n \\n        DAP     Data Access Point \\n \\n        DLT     Decentralized Ledger Technology \\n \\n        DL     Deep Learning \\n        \\n \\n        DRL     Deep Reinforcement Learning \\n        \\n \\n        DR     Demand response \\n        \\n \\n        EC     Edge Computing \\n        \\n \\n        EU     End User \\n        \\n \\n        EAPs     Energy Access Points \\n        \\n \\n        5G     Fifth-Generation \\n \\n        \\n        4G     Fourth-Generation \\n \\n        GPT     Generative Pre-trained Transformer \\n \\n        GPU     Graphics Processing Unit \\n HPC     High Performance Computing \\n        \\n \\n        HCI     Human Computer Interaction \\n \\n        IoT     Internet of Things \\n        \\n \\n        ML     Machine Learning \\n        \\n \\n        NLP     Natural Language Processing \\n \\n        NPC     Non Playable Character \\n        \\n \\n        PLM     Pre-trained Language Models \\n        \\n \\n        PTM     Pre-Trained Models \\n        \\n \\n        RNN     Recurrent Neural Network \\n        \\n \\n        6G     Sixth-Generation \\n \\n        TL     Transfer Learning \\n        \\n \\n        VU     Virtual Reality \\n        \\n \\n        \\n    \\n \\n\\n\\n§ INTRODUCTION\\n \\nLanguage is the cornerstone of human communication and plays a vital role in shaping our interactions with the world.', 'With the advent of NLP, it has revolutionized the way we interact with machines.', 'NLP has become a game-changer in the world of communication, enabling humans to interact with machines in a more natural way.', 'The evolution of NLP has been fueled by the exponential growth of textual data in the internet.', 'Over the years, NLP has witnessed a significant transformation from simple rule-based systems to complex deep learning-based models.', 'Despite the advances, natural language understanding and generation have long been a challenging problem in the field of NLP, largely due to the complex nature of human language.', 'However, recent advancements have paved the way for the new approaches to tackle these challenges.', 'One such breakthrough in NLP, is the development of the GPT <cit.>.', 'GPT became famous after the launch of ChatGPT by OpenAI, a research company <cit.> that focuses on developing AI technologies.', 'GPT is a deep learning model that is pre-trained on large corpora of text data and can be fine-tuned for specific tasks like language generation, sentiment analysis, language modelling, machine translation, and text classification.', 'The transformer architecture used in GPT is a significant advancement over previous approaches to NLP, such as RNN and CNN.', \"It uses a self-attention mechanism to allow the model to consider the context of the entire sentence when generating the next word, which improved the model's ability to understand and generate language.\", 'The decoder is responsible for generating the output text based on the input representation <cit.>.', 'GPT can perform a wide range of tasks in NLP.', 'One of its key strengths is in natural language understanding (NLU), where it can analyze and comprehend the meaning of text, including identifying entities and relationships in sentences.', \"It's also proficient in natural language generation (NLG), which means it can create text output, such as writing creative content or answering questions in a comprehensive and informative way.\", 'Alternatively, GPT is also code generator, where it can write programming code in various languages, such as Python or JavaScript.', 'GPT can also be utilized for question answering, which means it can provide summaries of factual topics or create stories based on the input text.', 'Additionally, GPT can summarize a piece of text, such as providing a brief overview of a news article or research paper, and it can be used for translation, which makes it possible to translate text from one language to another.', \"Overall, GPT's ability to perform a wide range of NLP tasks with high accuracy and precision, makes it an invaluable tool for various industries, including finance, healthcare, marketing, and more.\", 'As NLP technology continues to advance, we can expect GPT and other language models to become even more sophisticated and powerful, enabling us to communicate with machines more naturally and effectively.', '§.§ Motivation\\n\\n\\nGPT has become a transformative technology in the field of NLP, enabling the rapid development and growth of a wide range of industries and applications.', \"Despite its wide adoption and numerous potential applications, there is still much to be explored and understood about GPT's capabilities.\", \"Although there are studies on GPT in the literature related to academia and libraries <cit.>, education <cit.>,\\nGPT models<cit.>, banking and corporate communication <cit.>, advancements in chatGPT and its version <cit.>, and on generative AI's <cit.>, no existing reviews are dedicated to providing a comprehensive survey on GPT.\", \"Therefore, there is a need for a comprehensive review that focuses on GPT's architecture, enabling technologies, potential applications, emerging challenges, interesting projects and future directions.\", 'These limitations motivated us to conduct this review.', 'Hence, this review will not only help researchers and practitioners in this field to gain a better understanding of GPT but also provide valuable insights into its potential applications and major limitations when conducting the research.', '§.§ Related Surveys and Contributions\\n\\n\\n\\nThe GPT model is a type of DL model that uses self-supervised learning to pre-train massive amounts of text data, enabling it to generate high-quality language output.', 'The recent advancements in GPT model research can be attributed to the continual improvement of its architecture, increased availability of computing power, and the development of novel techniques to fine-tune the model for specific tasks.', 'These advancements have led to the creation of larger and more powerful GPT models, enabling them to perform a wider range of NLP tasks with unprecedented accuracy and fluency.', 'These GPT models have demonstrated great potential in transforming various industries like healthcare <cit.>, customer service <cit.>, financial industry <cit.> and so on.', 'These applications are enabled by the generation of high-quality and diverse data like large-scale corpora of text data with different fast-growing enabling technologies<cit.>.', 'There are numerous survey papers published to provide a comprehensive overview of the latest developments in GPT models, insights into the different architectures, training methods, evaluation metrics, and highlight the challenges and future directions of this field.', 'This literature survey aims to review and analyze the key findings and contributions of the most recent survey papers published on GPT models, to provide a comprehensive and up-to-date understanding of the state-of-the-art in this exciting and rapidly evolving field.', 'Lund et al.', '<cit.> presents the potential effects of AI and GPT models, specifically ChatGPT, on academia and libraries.', 'They discussed the capabilities of ChatGPT in generating human-like responses and its potential applications.', 'They examine how AI-powered chatbots and virtual assistants based on GPT models can enhance student learning experiences, assist with research tasks, and improve library services.', 'They also address concerns regarding data privacy, biases, and the need for ethical guidelines.', 'Overall, this survey paper highlighted the transformative potential of AI and GPT models while emphasizing the importance of responsible deployment and human oversight.', 'Kasneci et al.', '<cit.> have reviewed the potential opportunities and challenges of using large language models, specifically ChatGPT, for educational purposes.', 'They highlighted the benefits and limitations of using such models by discussing their implications for teaching and learning.', 'In addition, a defined strategy and pedagogical approach with a heavy focus on critical thinking and fact-checking are required while using such large language models in educational institution.', 'Thus, they concluded the paper by highlighting the key technical challenges like copyright issues, biased content creation, user dependency, privacy and security, and high-cost language models when such language models are used in the educational sector.', 'Qiu et al.', '<cit.> presented an exhaustive survey of various types of GPT models by detailing their working architecture.', 'They discussed the evolution of pre-training methods for NLP, from language modelling to TL and pre-training on large-scale corpora.', 'It also reviews the different types of GPT models, including word embeddings, contextual embeddings, and transformer-based models, and discusses their applications in various NLP tasks such as text classification, Named Entity Recognition, and machine translation.', \"They highlighted the benefits of GPT's models for the NLP domain, such as its ability to improve model performance with limited annotated data, reduce the need for task-specific feature engineering, and enable TL across multiple tasks.\", 'They discussed the major challenges and limitations of PTMs, such as the risk of bias and the lack of interpretability.', 'George et al.', '<cit.> studied the potential impact of GPT-4, the next iteration of GPT models, on communication within corporate environments.', 'They discussed how GPT-4 can revolutionize business communication by enabling more efficient and effective interactions.', 'They explore various applications of GPT-4 in corporate settings, such as automating customer support through AI chatbots that can provide personalized responses and resolve queries in real-time.', 'They also addressed potential challenges and considerations associated with implementing GPT-4 in corporate settings.', 'These include concerns about data security, privacy, and the need for human oversight to ensure accurate and ethical communication.', 'Thus, they concluded by emphasizing the transformative potential of GPT-4 in revolutionizing business communication to fully harness the benefits of GPT-4 while addressing any potential risks or limitations.', 'Zhang et al.', '<cit.> presents an extensive survey of generative AI and evaluates the capabilities of the ChatGPT models, particularly from GPT-4 to GPT-5.', 'They provided an overview of generative AI, highlighting its significance in generating realistic and creative outputs across various domains and evaluate their advancements over previous iterations.', 'They analyze the architectural improvements, model size, training techniques, and dataset considerations employed in GPT-4 and GPT-5.', \"In addition to it, they presented a comprehensive comparison of ChatGPT with other state-of-the-art generative AI models, such as OpenAI's DALL-E and CLIP.\", 'Finally, they concluded with valuable insights into the capabilities and limitations of these models and highlights the broader landscape of generative AI.', 'Zaib et al.', '<cit.> provides a survey on the latest advancements in GPTS and PTMs for conversational AI applications.', 'They focused on PLMs and their approaches while building dialogue-based systems.', 'They also highlighted the potential use of transformer-based models such as BERT and GPT, which have demonstrated good performance in understanding NLP generation, and dialogue management.', 'Thus, they concluded with the significant challenges in the field of developing conversational AI systems using PLMs and GPTs.', '< g r a p h i c s >\\n\\n\\tOrganization Chart of the survey.', 'Thus, the comparison of existing surveys on GPT models highlighting the growing importance of these models in key areas of NLP and other related fields are discussed here.', 'Hence, this is the first-of-its-kind survey that presents the extensive information, by comparing existing surveys with our survey and summarized in Table <ref>.', 'Comparison of this survey with the existing surveys\\n\\n!', '2*Ref.', '11l|Applications    \\n  6l|Enabling Technologies    \\n  2*Remarks \\n 2-18\\n    \\n  1l|[origin=c]90\\xa0Education\\xa0    \\n  1l|[origin=c]90\\xa0Industry\\xa0    \\n  1l|[origin=c]90\\xa0Agriculture\\xa0    \\n  1l|[origin=c]90\\xa0Healthcare\\xa0    \\n  1l|[origin=c]90\\xa0Transport\\xa0    \\n  1l|[origin=c]90\\xa0E-Commerce \\xa0    \\n  1l|[origin=c]90\\xa0Entertainment\\xa0    \\n  1l|[origin=c]90\\xa0Lifestyle\\xa0    \\n  1l|[origin=c]90\\xa0Gaming\\xa0    \\n  1l|[origin=c]90\\xa0Marketing\\xa0    \\n  [origin=c]90\\xa0Finance\\xa0    \\n  1l|[origin=c]90\\xa0Big Data \\xa0    \\n  1l|[origin=c]90\\xa0AI\\xa0    \\n  1l|[origin=c]90\\xa0Cloud Computing \\xa0    \\n  1l|[origin=c]90\\xa0Edge Computing\\xa0    \\n  1l|[origin=c]90\\xa05G and Beyond\\xa0    \\n  [origin=c]90\\xa0HCI\\xa0    \\n   \\n <cit.>    \\n  1l|    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  X    \\n  1l|X    \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|X    \\n  X     They conducted a survey discussing capabilities of ChatGPT on academia and\\nlibraries.', 'Although, Key challenges of Chatgpt were highlighted, practical implementation challenges and research directions were missing.', '<cit.>    \\n  1l|    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  X    \\n  1l|X    \\n  1l|    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  X    \\n  Reviewed the potential opportunities\\nand challenges of using large language models, specifically\\nChatGPT, for educational purposes.', 'Thus, evolution of GPT and their preliminaries were not discussed in this survey paper.', '<cit.>    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  X    \\n  1l|X    \\n  1l|    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  X    \\n  They presented an exhaustive survey of various\\ntypes of GPT models by detailing their working architecture with benefits and limitations of GPTs.', 'However,  \\n \\n\\n<cit.>    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  X    \\n  1l|X    \\n  1l|    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  X    \\n  Studied the potential impact of GPT4 in business communication and explore various applications of GPT-4 in corporate settings by highlighting any potential\\nrisks or limitations.', 'But, how GPT architecture can be used in corporate is not found with key enabling technologies.', '<cit.>    \\n  1l|X    \\n  1l|    \\n  1l|X    \\n  1l|X    \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|X    \\n  1l|X    \\n  1l|    \\n      \\n  1l|X    \\n  1l|    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  X    \\n  Analyzed the architectural\\nimprovements, model size, training techniques, and dataset\\nconsiderations employed in GPT-4 and GPT-5.', 'However, preliminary details are unedr explored.', '<cit.>    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  1l|X    \\n  X    \\n  Recent trends in   language models, applications of dialogue management, question answering NLP   tasks were discussed along with challenges and future scope of GPT.', 'Although it covered most of the technical aspects, the integration challenges to overcomeare not presented.', 'Our Survey   Paper    \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|    \\n      \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|    \\n  1l|    \\n      \\n  Presents the evolution of GPT models, GPT architecture and its detailed working, key enabling technologies, significant advancements of GPT models and their potential benefits in real-life applications, GPT projects, lessons learnt, open challenges and future research directions.', '§.§ Systematic Literature Survey\\n\\n\\nIn this review of GPT, we conducted a thorough literature review using various reputable sources.', 'Our search was primarily focused on peer-reviewed journals, and high-quality articles from reputed national and international conferences, seminars, books, symposiums, and journals.', 'To ensure the credibility of our sources, we referred to well-known archives such as Google Scholar and arXiv, and publications from top databases like IEEE, Springer, Elsevier, Taylor & Francis, and Wiley.', 'To identify relevant GPT references and publications, we used keywords such as NLP\\xadGPT, GPT architecture, DL for GPT, Pre\\xadtraining GPT, Fine-tuning AI GPT and GPT vertical applications.', 'We then screened all the retrieved articles based on their titles, excluding any papers with poor-quality material.', 'Next, we reviewed the abstracts of the remaining articles to determine their contributions.', 'In the final step of our literature review, we extracted the necessary data for our analysis.', 'By following these phases, we ensured that our study was based on high-quality and credible sources.', \"§.§ Paper Organization\\n\\n\\nThe structure of this paper's organization is illustrated in Fig.\", '<ref>.', 'Section 2 presents the preliminaries of GPT models such as the definition of GPT, its evolution and architecture, how it works and presents the comparison of various GPT models.', 'Section 3 discusses the key enabling technologies for GPT models.', 'The impact of GPT models in various applications are presented in Section 4.', 'In Section 5, we highlighted some of the exciting GPT projects that are currently developed.', 'Section 6 includes open issues, other technical challenges and future research directions in the field of GPT.', 'Finally, we conclude the paper in Section 7, by summarizing the key findings and contributions of this study.', 'The list of key acronyms are listed in Table <ref>.', '§ PRELIMINARIES\\n\\n\\nIn this section, the evolution of GPT models, the architecture of GPT, working process of GPT models are discussed and finally, different versions of GPT models are compared.', '§.§ Generative Pre-trained Transformer\\n\\nThe GPT model produces enormous quantities of pertinent and complicated machine-generated text from a small amount of text as input.', 'GPT models can be identified as a language model that mimics human text using a DL techniques and it acts as an autoregressive model in which the present value is based on the previous value <cit.>.', '§.§.§ Definition 1\\n\\nGPTs are language models pre-trained on vast quantities of textual data and can perform a wide range of language-related tasks <cit.>.', '§.§.§ Definition 2\\n\\nA GPT is a language model relying on DL that can generate human-like texts based on a given text-based input.', '<cit.>.', '§.§.§ Definition 3\\n\\nGPT is a language model developed by OpenAI to help give systems intelligence and is used in such projects as ChatGPT <cit.>.', '§.§ Evolution of GPT\\n\\nGPT models have evolved through multiple changes and breakthroughs in NLP technology.', 'These are some significant turning points in the growth of the GPT model:\\n\\n\\t\\n    < g r a p h i c s >\\n\\n\\tGPT Road Map.', 'Before GPT, NLP models have been trained on large amounts of annotated data that is related to a specific task.', 'This had a significant drawback because it was difficult to access the quantity of labelled data required to train the model precisely.', 'The NLP models were unable to complete tasks outside of their training set since they were restricted to a particluar set of data.', 'To get around these restrictions, OpenAI offered a Generative Language Model called GPT-1 that was created using unlabeled data and then given to users to fine-tune to carry out subsequent tasks like sentiment analysis, categorization, and question-answering <cit.>.', 'This indicates that the model attempts to produce an appropriate response based on input and that the data used to train the model is not labelled <cit.>.', 'Fig.', '<ref> shows the timeline of the evolution of several pre-trained models from Eliza, which was created in 1960, to the more current 2022-ChatGPT.', 'GPT-1 was the first ever model that could read the text and respond to queries <cit.>.', 'OpenAI released GPT-1 in 2018.', 'GPT-1 was a major move forward in AI development because it enabled computers to comprehend textual material in a more natural manner than before.', 'This generative language model was able to learn a wide variety of connections and gain immense knowledge on a varied corpus of contiguous text and lengthy stretches <cit.>.', 'This happened after being trained on a huge BooksCorpus dataset.', 'In terms of design, GPT-1 employs a 12-layer decoder architecture transformer with a self-attention system for training.', \"GPT-1's capacity to execute zero-shot performance on different tasks was one of its major success as a result of its pre-training.\", 'This ability demonstrated that generative language modelling can be used to generalize the model when combined with a successful pretraining idea.', 'With TL as its foundation, GPT models evolved into a potent tool for performing NLP tasks with minimal fine-tuning <cit.>.', 'It paved the way for other models to progress even more in generative pre-training using larger datasets and parameters.', '<cit.>.', 'To create a better language model later in 2019, OpenAI created a GPT-2 using a bigger dataset and more parameters.', 'The model design and execution of GPT-2 are some of the key advancements <cit.>.', 'With 1.5 billion parameters, it has 10 times the size of GPT-1 (117 million parameters), and it has 10 times as many parameters and data <cit.>.', 'By using only the raw text as input and utilizing little to no training examples, it is effective in terms of resolving various language tasks related to translation, summarization, etc.', 'Evaluation of GPT-2 on various downstream task datasets revealed that it excelled by substantially increasing accuracy in recognizing long-distance relationships and predicting sentences <cit.>.', 'The most recent iteration of the GPT model is GPT-3.', 'It is a sizable language prediction and production model created by OpenAI that can produce lengthy passages of the source text.', \"GPT-3 eventually emerged as OpenAI's ground-breaking AI language software.\", 'Simply put, it is a piece of software that can create lines on its own that are so distinctive they almost sound like they were written by a human <cit.>.', 'The GPT-3 program is presently accessible with limited access via a cloud-based API, and access is required to investigate the utility.', 'Since its debut, it has produced several interesting apps.', 'Its capacity, which is about 175 billion parameters big and 100 times larger than GPT-2, is a key advantage.', 'It is taught using a corpus of 500 billion words called \"Common Crawl\" that was gathered from a sizable content archive and the internet <cit.>.', 'Its other noteworthy and unexpected capability is its ability to carry out basic mathematical operations, write bits of code, and carry out clever tasks.', 'As a result, NLP models can help businesses by responding more quickly to requests and accurately keeping best practices while minimizing human mistakes <cit.>.', 'Due to its intricacy and size, many academics and writers have referred to it as the ultimate black-box AI method.', 'Due to the high cost and inconvenience of performing inference, as well as the billion-parameter size that makes it resource-intensive, it is difficult to put into practice in jobs <cit.>.', 'GPT-4 was named as the successor of GPT-3.', 'In the meantime, several AI models built on GPT-3.5, an updated version of GPT-3, have been surreptitiously released by OpenAI <cit.>.', 'GPT-3.5 was trained on a mixture of text and code.', 'From the vast amounts of data collected from the web, which includes tens and thousand of Wikipedia entries, social media posts, and news items, GPT 3.5 learned the relations between words, sentences, and various components.', 'It was utilized by OpenAI to develop several systems that have been tailored to complete particular jobs <cit.>.', 'It collected vast amounts of data from the web, including tens of thousands of Wikipedia entries, posts on social media, and news items, and used that information to learn the relationships between sentences, words, and word components <cit.>.', 'The latest version of the GPT model by OpenAI is GPT-4 which is a multimodal big language model.', 'It was launched on March 14, 2023, and is now accessible to the general public through ChatGPT Plus in a constrained capacity.', 'A waitlist is required to gain access to the business API <cit.>.', 'Using both public data and \"data licensed from third-party providers,\" GPT-4 was pre-trained to anticipate the next coin as a transformer.', 'It was then adjusted with reinforcement learning based on input from humans and AI for human alignment and policy conformance.', 'In comparison to GPT-3, which had context windows of only 4096 and 2049 tokens, respectively, the group created two variants of GPT-4 with context windows of 8192 and 32768 tokens.', \"§.§ GPT model's architecture\\n\\n\\n\\nGPT models are based on neural networks that are used for NLP tasks, such as language modelling, text classification, and text generation.\", \"The GPT model's architecture is based on the transformer model <cit.>.\", 'The Transformer model uses self-attention mechanisms to process input sequences of variable length, making it well-suited for NLP tasks.', 'GPT simplifies the architecture by substituting encoder-decoder blocks with decoder blocks.', 'GPT model takes the transformer model and pre-trains it on large amounts of text data using unsupervised learning techniques.', 'The pre-training process involves predicting the next word in a sequence given the previous words, a task known as language modelling.', 'This pre-training process enables the model to learn representations of natural language that can be fine-tuned for specific downstream tasks <cit.>.', 'The following are the components of the GPT architecture.', '* Input Embedding layer: The embedding layer maps the input tokens (e.g., words or subwords) to continuous vector representations, which can be processed by the transformer blocks<cit.>.', '* Positional encoding: Since the transformer blocks do not have any notion of order or position, positional encoding is added to the input embeddings to provide information about the relative position of tokens.', 'Masking: In some cases, masking may be necessary to mask certain input tokens (e.g., in language modelling tasks, the model should only use tokens that come before the target token).', 'Transformer blocks: GPT models are based on the transformer architecture.', 'It is designed for NLP tasks and has been widely used in applications such as machine translation, text classification, and text generation.', 'Transformers allow the model to focus on different areas of the input while processing <cit.>.', '* Linear and Softmax Functions:\\nIn the GPT architecture, the softmax function is commonly used for classification tasks.', 'The softmax function is applied to the output of the final layer of the model.', 'It generates a probability distribution over a set of output classes.', 'The output of the final layer is specifically converted into a set of logits before being normalized with the softmax function.', 'The normalized values obtained from the model can be interpreted as the likelihood or probability that a particular input belongs to each of the output classes.', 'The query, key, and value vectors for each token in the input sequence are frequently calculated using linear functions in the attention mechanism.', 'The output of the multi-head attention layer is transformed using them in the feedforward layers as well.', 'The output layer also employs linear functions to forecast the following token in the sequence <cit.>.', '* Pre-training: Pre-training is a key component of the GPT architecture.', 'In pre-training, the model is trained on a large amount of data in an unsupervised manner even before fine-tuning the model for specific tasks like classification and text generation.', '* Fine-tuning:\\nFine-tuning is the process of adapting a pre-trained neural network model, such as GPT, to a new task or dataset by further training the model on that task or dataset.', 'Fine-tuning in GPT involves adjusting the parameters of the pre-trained model to optimize performance on a specific downstream task, such as text classification or text generation <cit.>.', '* Language modeling:\\nLanguage modelling is a key task in the GPT architecture.', 'In the case of GPT, the language modelling task is performed during the pre-training phase of the model.', 'In pre-training, the model is trained based on a large amount of data using a language model objective.', 'It is the task of predicting the next word in sequence based on the previous words.', 'It allows the model to learn relationships between the words and their meaning in the training data <cit.>.', '* Unsupervised learning:\\nUnsupervised learning is an ML algorithm which enables the model to learn form unlabelled data without any human intervention.', 'GPT models use unsupervised learning in the pre-training phase to understand the relationships between the words and their context in the training data <cit.>.', 'Comparsion of different versions of GPT model\\n\\n!', 'Model    \\n  Tokens    \\n  Size    \\n  Parameters    \\n  Dataset    \\n  Year    \\n  Features    \\n  Input Type    \\n  Drawbacks \\n GPT-1    \\n  -    \\n  12-layer decoder    \\n  117M parameters    \\n  Books corpus    \\n  2018    \\n  Used   mostly for language modelling tasks and it is transformer based    \\n  A sequence of tokens and words    \\n  Limited   Capacity, Limited Data, Cannot perform complex tasks, Limited applications \\n GPT-2    \\n  -    \\n  10 times the size of   GPT-1    \\n  1.5B parameters    \\n  Downstream task   datasets    \\n  2019    \\n  Text   generation capabilities are improved and a chance for misuse    \\n  A sequence of tokens and words    \\n  Limited   Control, Limited Data Diversity, Expensive computational requirements, Risk of   improper information \\n GPT-3    \\n  4096 and 2049 tokens    \\n  100 times larger than   GPT-2    \\n  175B parameters    \\n  Common Crawl    \\n  2020    \\n  Good   NLP capabilities, language translation, summarization and generation of text    \\n  A sequence of tokens and words and images and tables    \\n  Limited   Control, Limited Data Diversity, Lack of explanation, Ethical concerns \\n GPT-3.5    \\n   maximum token limit of 4096 tokens    \\n  96 layers    \\n  similar or larger number of parameters like GPT-3    \\n  -    \\n  2022    \\n  Improves user experience by delivering more precise and contextually relevant information    \\n  The input type typically consists of text data    \\n  Limited resources to train,Data Bias,Lack of Explainability,Limited Contextual Understanding,High Inference Latency \\n GPT-4    \\n  8192 and 32768 tokens    \\n  -    \\n  100T parameters    \\n  -    \\n  2023    \\n  Creative   and technical writing tasks    \\n  A sequence of tokens and words and images and tables    \\n  - \\n \\n\\n\\n\\n\\n §.§ How do GPT models work?', '< g r a p h i c s >\\n\\n\\tTransformer Architecture.', '< g r a p h i c s >\\n\\n\\tTransformer Architecture and Input Transformations for Fine-Tuning on Different Tasks.', '< g r a p h i c s >\\n\\n\\tHow does GPT Work.', 'GPT models work by using a transformer which is a neural network architecture that processes the input sequences of natural language text <cit.>.', 'The GPT model uses unsupervised learning techniques to pre-train this transformer architecture on a significant amount of text input <cit.>.', 'The model gains the ability to anticipate the subsequent word in a sequence based on the preceding words during pre-training.', 'Language modelling is the process that enables a model to discover the statistical connections between words and their context in training data.', 'Fig.', '<ref> shows the various stages of GPT operation.', 'The first step entails supervised fine-tuning, the second step involves producing optimal responses to input, and the third step involves proximal policy optimization and reinforcement learning.', 'The model can be fine-tuned for particular tasks, like text classification or text production, after pre-training.', \"The model is trained on a smaller dataset that is unique to the work at hand during fine-tuning, and the model's parameters are changed to maximize performance on that task <cit.>.\", 'Fig.', '<ref> shows the general transformer architecture of GPT.', 'When used for text creation, GPT models create text by anticipating the following word in a series based on the previously created words.', 'Depending on how it has been modified, the model can produce text that is comparable to the input text or that adheres to a certain theme or style.', 'Fig.', \"<ref> projects the GPT model's transformer architecture and input transformations for fine-tuning different tasks.\", '§.§  Comparisons of GPT Versions\\n\\n\\n\\n\\t\\n    < g r a p h i c s >\\n\\n\\tA comparison between unimodal, cross-modal, and multimodal modal GPTs.', 'There are several versions of GPT models each having their own features and capabilities.', 'Table <ref> presents a comparison of various versions of the GPT models.', 'The table presents the following details like year of release of the GPT model, parameters, tokens generated, input type, features of each model, drawbacks of each model, and the size of each model.', 'Generative AI (GAI) models are of different types like unimodal, cross-modal, and multimodal.', 'The first type is unimodal which rely on a single type of input, such as text or images.', 'The cross-modal, on the other hand, can process multiple types of inputs and relate them to each other.', 'The Multimodal is the most complex type of AI as it can process and integrate information from multiple modalities, such as speech, text, images, and even physical interactions with the environment.', 'GPT adopts only unimodal and multimodal types where ChatGPT is said to be unimodal, while GPT-4 is multimodal.', 'Fig.', '<ref> is an illustration that distinguishes between unimodal, cross-modal, and multimodal Generative AI models.', \"Overall, GPT models have demonstrated outstanding performance with NLP, by enhancing each iteration  and its predecessor' capabilities.\", 'Each model, however, also has its own restrictions and drawbacks, such as restricted output control, lack of diverse data, and ethical concerns.', 'While selecting a GPT model for a particular task, researchers and developers should carefully take these factors into account <cit.>.', 'In detail, this section describes the evolution, and architecture of GPT and compares the different versions and types of GPT.', '§ ENABLING TECHNOLOGIES\\n\\nGPT is a convergence of several technologies.', 'It is enabled by the latest technologies like Big data, AI, Cloud Computing, EC, 5G and beyond networks, and HCI.', 'In this section, we provide an overview of enabling technologies related to GPT.', 'The major technologies that constitute the GPT models are depicted in Fig.', '<ref>.', '§.§ Big Data\\n\\n\\nBig data refers to the vast amounts of structured and unstructured data generated by businesses, individuals, and machines.', 'The proliferation of new technologies, such as the IoT, has led to an explosion of data production from sources like social media, sensors, and transaction-based systems <cit.>.', 'The emergence of big data has revolutionized the way organizations approach data analysis and decision-making.', 'The training provided by this massive amount of data has yielded valuable insights for the use of advanced models like GPT in the field of NLP <cit.>.', 'The GPT models utilize DL and big data for natural language generation, with GPT-4 being the most advanced model to date <cit.>.', 'The training data for GPT models typically include millions or even trillions of data from a diverse range of sources, such as books, articles, websites, and social media platforms.', 'This large and diverse training data helps GPT models capture the variations in language usage, making them more accurate and effective at NLP tasks.', 'As a result, GPT models may be used for a variety of tasks, including question-answering, text summarization, and language translation <cit.>.', 'Moreover, since GPT models can learn from a variety of data sources, they can be tuned for certain tasks and domains, making them very adaptive and versatile.', 'GPT model has the potential to be utilized for a variety of activities, including the creation of images and videos in addition to its excellent language processing capabilities <cit.>.', 'While big data presents numerous benefits to GPT, by enabling the models to get trained with large amounts of data, it also presents several challenges <cit.>.', 'GPT is trained on a variety of data, large amounts of data, and also sensitive data.', 'Thus, ensuring data accuracy, privacy concerns, and ethical use of data are some of the challenges that must be considered.', 'However, with the continuous growth of available data, GPT models will become even more advanced and capable of performing increasingly complex tasks <cit.>.', 'The future of big data as an enabling technology for GPT models is promising, with the potential to revolutionize the field of NLP.', 'As technology continues to advance, organizations must prioritize ethical considerations and data accuracy to fully harness the benefits of big data and GPT models.', '§.§ Artificial Intelligence\\n\\n\\nAI refers to the simulation of intelligent behaviour in machines that are programmed to learn from their experience to reason, understand natural language, and perceive their environment <cit.>.', 'AI gives machines the ability to sense their surroundings, deal with what they see, handle issues, and take action to reach a particular objective.', 'The importance and capability of AI is growing all the time.', 'AI enables GPT models to allow machines to comprehend and react to human language.', 'There are several ways in which AI can continue to help improve GPT and make it more powerful and effective in its language generation capabilities <cit.>.', \"The following are the several ways through which AI can make GPT models more powerful:\\n\\n\\n  * Fine tuning\\n\\n  * Dialogue generation\\n\\n  * Natural language understanding\\n\\nGPT's model performance on particular tasks can be enhanced by utilizing AI approaches.\", 'For instance, it can be trained on a large corpus of text from a particular field such as legal documents or medical literature to better grasp and produce language in that field <cit.>.', 'Considering dialogue generation, AI techniques such as reinforcement learning and sequence-to-sequence models can be used to enable GPT generate more natural and engaging dialogue in conversational contexts.', 'Similarly, AI techniques such as semantic parsing and named entity recognition can be used to help GPT better understand the meaning of language and the relationships between words and phrases.', 'This can enable it generate more accurate and coherent language <cit.>.', \"The development and enhancement of GPT model language production capabilities depend heavily on AI, and GPT's capabilities will continue to be growing by continuous research and development in AI.\", 'As GPT models become more advanced, there are growing concerns about the potential for them in reinforcing biases and propagate harmful or offensive content <cit.>.', 'Some of these concerns also include bias which can lead to unintended discrimination and unfairness, lack of understanding of the context that can lead to misunderstandings or incorrect responses, poor data quality can lead to inaccurate or biased models, ethical concerns like privacy and autonomy <cit.>.', 'AI models like GPT require significant amounts of computational power to train and run, which can have a significant environmental impact due to their high energy consumption <cit.>.', \"Though AI has a great deal of promise, it's critical to be aware of the underlying issues and make efforts to fix them to ensure that it is utilized responsibly and morally for GPT.\", '< g r a p h i c s >\\n\\n\\tEnabling technologies of GPT models.', '§.§ Cloud Computing\\n\\nCloud computing refers to the on-demand availability of computer resources, such as storage, processing power, and applications, delivered over the internet <cit.>.', \"The GPT model's successes are possible not only because of algorithmic evolution but also increased computational capabilities i.e.\", 'exponential growth in hardware (computational power, storage capacity), cloud computing, and related operational software <cit.>.', 'The applications for cloud and EC working together such as natural language generation, image completion, or virtual simulations from wearable sensors see that the work is made more compute-intensive <cit.>.', 'GPT models need a lot of computational power to analyze a lot of data, and cloud computing offers the scalability required to cope with demand spikes.', 'Without worrying about the constraints of on-premises hardware, GPT models can rapidly and easily scale up or down as needed with cloud computing <cit.>.', 'Cloud-based platforms like Amazon Web Services (AWS) or Google Cloud Platform (GCP) provide access to distributed computing resources that can be used to train GPT.', 'Since cloud computing provides web-based solutions and thereby does not require the purchase and maintenance of costly hardware, it can be a cost-effective choice for a GPT model.', 'By utilizing cloud computing, the GPT model can only pay for the computing resources it uses <cit.>.', 'The other added advantage of cloud computing in GPT is, it gives GPT models the freedom to access computing resources whenever it wants, from any location in the world.', 'This makes GPT models more accessible to users by enabling smooth operation across a variety of gadgets and platforms <cit.>.', 'Cloud computing providers offer high security and compliance standards, which can protect the GPT model and its data from online dangers.', 'Cloud service providers also possess the knowledge and tools necessary to effectively address security problems and stop data leaks.', \"Cloud-based storage services, such as Amazon S3 or Google Cloud Storage, provide scalable and reliable storage for GPT's data.\", 'Despite the advantages of cloud computing where it can help GPT models to operate more efficiently, effectively, and securely, there are also a few technological aspects where it creates a drawback for GPT <cit.>.', 'To function properly, the GPT model needs a sizable amount of computing power and data storage.', 'These resources can be accessible online with cloud computing.', 'As a result, continued operation of the GPT model requires a robust and dependable internet connection, and any breakdown in connectivity may result in delays or even data loss.', 'There are some security concerns when storing sensitive data, such as personal information or trade secrets, in the cloud which can be risky if proper security measures are not in place <cit.>.', 'While cloud computing can be more cost-effective than building and maintaining an in-house computing infrastructure, it can still be expensive for long-term use.', 'It also suffers issues like performance variability, limited availability etc., \\n\\n\\n §.§ Edge Computing\\n\\nThe rapid growth of IoT, a large amount of data from several IoT devices, and also cloud services have necessitated the emergence of a concept called EC.', 'EC is an open AI and distributed design with decentralized computational power.', 'In EC, there is a lesser need for clients and servers to communicate over long distances, which lowers latency and bandwidth utilization.', 'Instead of depending on centralized data centers, EC entails bringing computing capacity and data storage closer to the consumer <cit.>.', 'In GPT, where there is a need for real-time data analysis, EC plays a major role in faster processing and better efficiency in producing good results<cit.>.', 'GPT models are typically large and complex, requiring significant processing power to run.', 'By deploying GPT models on the edge devices, closer to the source of data, latency can be reduced in replying to users who seek information through the GPT models by eliminating the need to move data back and forth from end devices to the cloud.', 'Since EC maintains data near the periphery and away from centralized servers, it can offer improved security and more privacy protections in the case of the requests made by users through GPT <cit.>.', 'GPT models utilize a lot of data for learning and thereby the cost of data transfer also increases with data volume.', 'EC can aid in controlling data transfer expenses.', 'EC can also help in lowering the amount of bandwidth by pre-processing the data even before transferring it to the cloud.', 'Particularly when analyzing photos or videos, GPT models can produce a lot of data <cit.>.', 'EC accelerators, such as graphics processing units (GPUs) and field-programmable gate arrays (FPGAs), can be used to speed up GPT model inference and training.', 'These accelerators can be integrated into edge devices or edge servers, providing more efficient processing of GPT models.', 'EC and GPT models make a great combination.', 'Comparative to cloud data centres, edge devices may have constrained computation and storage capabilities <cit.>.', 'This might limit the scope of GPT models that can be installed on edge devices in terms of size and complexity.', 'Since GPT models handle large and varied data, EC can also increase security risks and data privacy concerns.', 'Implementing EC in existing infrastructure can be difficult and require significant investment in hardware, software, and networking components.', 'This can be a barrier for many organizations which are using the GPT model and EC <cit.>.', '§.§ 5G and beyond networks\\n\\n\\n5G networks represent the latest generation of cellular networks that promise faster data speeds, lower latency, and the ability to connect a vast number of devices simultaneously <cit.>.', '5G and beyond networks enable faster data transmission speeds than previous generations of cellular networks, which can help in training and deploying larger and more complex language models.', 'This can result in faster training times and better performance.', '5G and beyond networks can provide lower latency than previous generations of cellular networks, which can reduce the time required for communication between GPT and other devices, such as servers or other language models <cit.>.', 'This can improve the real-time response of the GPT model for applications that require quick and accurate language processing.', '5G and beyond networks offer improved connectivity options, such as increased capacity and more reliable connections, which can help in scaling up the deployment of the GPT model for large-scale language processing tasks.', 'With the deployment of 5G and beyond networks, EC is becoming more prevalent.', 'This means that a GPT model can potentially be deployed closer to the end-user, reducing the latency and improving the response time for applications that require real-time language processing <cit.>.', 'Ultra-Reliable Low-Latency Communication (URLLC) is a key feature of 5G networks.', 'In the context of GPT language models, URLLC can enable real-time and reliable communication between multiple devices, such as edge devices, cloud servers, and end-users <cit.>.', \"Though 5G and beyond technology offers potential advantages to GPT models, it is also important to note that the actual impacts of this technology may change depending on how it's implemented and used.\", '5G enables the access to uncontrolled access to the Internet,it may attract cybersecurity risks and privacy concerns <cit.>.', 'Also, as GPT uses a large amount of data for analysis it could also cause privacy concerns.', '5G and beyond networks in GPT models need high infrastructural requirements which is a costly process.', '§.§ Human Computer Interaction\\n\\n\\nHCI, which is multi-faceted, concentrates on the design of computer technology and, in particular, on how people and computers communicate with each other <cit.>.', 'HCI has a greater influence over GPT models.', 'As a language model, GPT is designed to interact with humans by generating natural language responses to input text.', 'HCI research can help designers create more effective input mechanisms for the GPT model, such as natural language interfaces, that allow users to communicate more easily and accurately with the model <cit.>.', \"HCI also helps in enhancing the GPT model's user experience by creating interfaces that are more intuitive and user-friendly.\", 'This makes it easy for the users to interact with GPT models and understand their responses <cit.>.', 'HCI also estimates the performance of GPT models by evaluating their responses with real-time users and identifies the areas where the model needs improvement, thereby improving its reliability and accuracy.', 'HCI enhances the usability of GPT models by reducing the time and effort required for the users to interact with <cit.>.', 'While HCI can be incredibly helpful in improving the design and usability of GPT models, there are also some potential drawbacks to consider.', 'If the research is not conducted with a diverse group of users, HCI can introduce biases into the design of the GPT model.', 'HCI techniques can be expensive and time-consuming.', 'As GPT models become more complex, it may become more difficult to design interfaces and input mechanisms that are both effective and user-friendly <cit.>.', 'HCI may not always be able to provide the necessary insights or feedback to drive improvements in GPT models.', 'There are also ethical concerns around the use of GPT models, including issues related to privacy, bias, and the potential misuse of the technology <cit.>.', 'As GPT models become more complex, it may become more difficult to design interfaces and input mechanisms that are both effective and user-friendly.', '§ IMPACT OF GPT MODELS ON VARIOUS APPLICATIONS\\n\\nGPTs have made significant progress, and its impact is being felt across various industries \\n like education, healthcare, industry, agriculture, travel and transport, e-commerce, entertainment, lifestyle, gaming, marketing, and finance.', 'This section provides valuable insights on the impact of the GPT models in the aforementioned applications as depicted in Fig.', '<ref>.', '< g r a p h i c s >\\n\\n\\tThe impact of GPT models on various applications.', '§.§ Education\\n\\n\\n\\n\\n  §.§.§ Introduction\\n\\n\\nEducation has been around for centuries, with traditional education being the most common form.', 'Traditional education involves a teacher imparting knowledge to a group of students in a physical classroom.', \"While successful, traditional education can be restrictive and inflexible, limiting students' ability to learn at their own pace and in their preferred style.\", 'It can also be limited by geography, as students need to be physically present in a classroom to learn.', 'Technology has emerged as a solution to some of these issues, allowing for personalized learning experiences and more engaging, accessible resources.', 'Online learning platforms, digital textbooks, and multimedia tools offer students access to a vast array of resources from anywhere in the world.', 'Technology can also facilitate collaboration and communication among students and teachers, leading to a more dynamic and interactive learning experience.', 'Distance learning, hybrid learning models, and online classes are examples of how technology can help break down the barriers of traditional education, making learning more flexible, efficient, and effective.', 'By integrating technology into traditional education, we can create a more personalized and effective learning experience, benefiting students worldwide.', '§.§.§ Impact of GPT in Education\\n\\nThe field of education is constantly evolving, with advancements in technology playing a significant role in shaping the way we learn and teach.', 'One such technology that has the potential to transform the education industry is GPT.', 'As a large language model trained on a vast amount of data, GPT can generate human-like text that is coherent and informative, making it a valuable tool in developing educational content such as textbooks, study guides, and course materials.', 'Furthermore, GPT can be used to analyze and summarize complex text, which can help educators and students save time and increase comprehension.', 'With its ability to support NLP applications and create intelligent tutoring systems, GPT has the potential to revolutionize the way we learn and teach.', 'In this context, following section will explore the different ways in which GPT can contribute to the education industry and transform the future of learning.', '* Intelligent Tutoring: Intelligent tutoring is a teaching approach that uses AI and ML to provide personalized and adaptive instruction.', 'It analyzes student performance data, understands their strengths and weaknesses, and generates customized learning paths.', 'It provides immediate feedback, personalized guidance, and remedial support.', 'It is effective in improving learning outcomes, increasing student engagement, and reducing learning time.', 'With advanced natural language processing capabilities, GPT can enhance the personalized and adaptive instruction provided by intelligent tutoring systems.', 'It can analyze natural language input from students, enabling intelligent tutoring systems to better understand and respond to their queries, needs, and preferences.', 'It can also generate personalized feedback and assessment based on the individual learning progress of each student, helping them to identify and address their knowledge gaps and improve their performance.', 'GPT can also analyze student performance data and generate adaptive learning paths that provide customized instruction and remediation, ensuring that each student learns at their own pace and achieves their learning objectives.', 'Additionally, it can create interactive dialogue systems that simulate natural conversations between students and virtual tutors, making learning more engaging, interactive, and personalized <cit.>.', 'The authors in <cit.> have identified that GPT-4 model outperforms general-purpose GPT-3.5 model as well as GPTs (Med-PaLM, a prompt-tuned version of Flan-PaLM 540B) specailly trained on medical data.', \"The authors have tested GPT-4 models' ability to explain medical reasoning, personalize explanations to students, and interactively craft new counterfactual scenarios around a medical case.\", '* Learning assistance and material development: Learning materials are critical in education as they provide a structured way for students to acquire knowledge and skills.', 'They can be tailored to meet the needs of diverse learners and make learning more engaging and effective, supporting teachers to create a more dynamic and interactive learning environment.', 'GPT can contribute to creating learning materials by automating content generation, providing multilingual content creation, language correction, personalized content creation, conducting topic research, and generating assessments.', 'It saves time and effort for educators and publishers, improves the accuracy and readability of material, and makes learning more engaging and effective.', 'GPT can generate high-quality content such as summaries, quizzes, and lesson plans based on specific learning objectives, making learning accessible to a wider audience.', 'It can analyze written content and provide suggestions to improve grammar, punctuation, and readability.', 'GPT can also assist in research writing by suggesting ideas for structure, rephrasing and organizing content, and identifying gaps in research <cit.>.', 'Moreover, GPT can also provide personalized feedback based on individual learning progress, enhancing the development of more comprehensive and informative learning materials.', \"* Automated Assessments: \\n    Automated assessment in education uses technology to evaluate students' learning outcomes, providing immediate feedback and reducing potential bias in grading.\", 'It can also help teachers identify areas where students may need additional support, enabling them to tailor their teaching methods to better meet individual needs.', 'GPT with its advanced natural language processing skills, can help in automated assessment by analyzing and grading student responses to various types of assessment questions, including essays and short answer questions.', 'It can also provide feedback to students <cit.>, such as highlighting areas for improvement and suggesting further reading or resources.', \"GPT's natural language processing capabilities can help to identify the meaning and context of students' responses, making automated assessment more accurate and effective.\", \"Additionally, GPT can generate personalized learning materials and exercises based on students' assessment results, supporting educators to create more tailored and effective learning experiences.\", \"The authors in <cit.> have used Chat GPT in evaluating the students' assignments such as quiz style questions, and also in generating relevant practice problems to improve content retention and understanding.\", 'The results were promising in the classroom.', \"The authors believe that Chat GPT has the significant ability in reducing the load of instructor without compromising students' learning outcomes.\", '* Fostering Creativity: Creativity thinking plays a vital role in education by encouraging students to think beyond traditional boundaries and develop innovative solutions to complex problems.', 'It helps students to approach learning with an open mind and a willingness to explore new ideas, leading to greater engagement and motivation.', \"GPT's ability to generate human-like responses and creative writing can aid in improving creativity.\", 'It can help improve creativity by generating new and innovative ideas based on vast amounts of data and information.', 'By analyzing patterns in language and identifying connections between different concepts, GPT can suggest novel approaches to teaching and learning.', 'Additionally, GPT can also generate creative prompts or challenges for students, encouraging them to think outside the box and approach problems in unique ways <cit.>.', \"GPT can also analyze and evaluate students' creative work, providing feedback and suggestions for improvement.\", 'So, GPT can be a valuable tool for promoting and enhancing creativity among students and faculty members.', '§.§.§ Challenges\\n\\n\\n\\nThere are several advantages to incorporating GPTs in education, but it is essential to acknowledge the potential limitations.', \"While GPTs can quickly generate information, they may impede students' critical thinking and problem-solving skills.\", 'Furthermore, learners who benefit from personal interaction with instructors may find the lack of human involvement disadvantageous.', 'GPTs rely on statistical patterns, so they cannot provide a comprehensive understanding of the material being taught <cit.>.', 'Privacy concerns arise when using sensitive student data in GPTs for educational purposes.', 'Additionally, since GPTs cannot provide citations, it is challenging to identify the source of information generated.', 'The cost of maintaining GPT may be prohibitive for schools and educational institutions with limited resources.', 'Finally, distinguishing between reliable and unreliable information generated by GPTs can be difficult, so it is necessary to have human oversight to ensure data accuracy and regulate access.', '§.§.§ Summary\\n\\nGPT offers numerous advantages in the education sector, including personalized and adaptive instruction, automated assessment, creative writing support, and research writing assistance.', 'They have the potential to revolutionize teaching by creating lesson plans and activities, responding to natural language queries, and integrating multiple digital applications.', 'However, there are also challenges to consider, such as the potential negative impact on critical thinking and problem-solving skills, lack of human interaction, data security and privacy concerns, inability to provide full comprehension, lack of citations or sources, high cost of maintenance, and potential for producing unreliable information.', 'Further research is needed to explore human-computer interaction and user interface design to integrate GPT into educational workflows while ensuring that the information they provide is accurate and reliable.', '§.§ Healthcare\\n\\n\\n\\n  §.§.§ Introduction\\n\\nBefore technology became widespread in healthcare, healthcare services were primarily delivered through face-to-face interactions between healthcare professionals and patients.', 'Traditional healthcare faced several challenges, including limited medical instruments, paper-based health records, patients receiving care mostly in hospitals or clinics, physical travel requirements to receive medical attention, and limited medical research.', 'Despite these challenges, traditional healthcare still provided valuable medical services to patients.', 'However, with the introduction of technology, healthcare has become more efficient, accessible, and personalized, resulting in improved patient outcomes and better overall healthcare services.', 'Technology has become an essential aspect of society, as reflected in the significant investments made in this sector.', 'Despite the advancements in technology, the healthcare industry still faces various new challenges, including access to healthcare, high costs, personalized medicine, data privacy and security concerns, and an aging population.', 'However, technology has the potential to address these challenges and improve the efficiency of healthcare services.', '§.§.§ Impact of GPT in healthcare\\n\\nRecent years have seen significant advancements in technology, including in the healthcare industry.', 'Biotechnology, medical devices, and pharmaceuticals have undergone transformations through the use of cutting-edge technologies like DL <cit.> and ML <cit.>.', 'Currently, the healthcare sector is utilizing various forms of AI techniques for medical research and providing medical services.', 'One such technique is the GPT features of NLP, which hold immense potential for the healthcare industry.', 'GPT can help to overcome several challenges in healthcare in multiple ways.', 'For instance, it can be used to develop intelligent systems that assist doctors in making accurate diagnoses and providing clinical assistance <cit.> <cit.>.', 'GPT can also analyze large volumes of medical data and generate reports.', 'Furthermore, it has potential applications in drug discovery <cit.> <cit.>, personalized medicine, patient diagnosis, medical image analysis, analyzing electronic health records, clinical decision support systems, and disease prediction.', '* Drug Discovery: Recent AI and machine learning techniques <cit.> <cit.> are having the potential to contribute to the growth and development of drug discovery.', 'GPTs are capable of learning new patterns and relationships <cit.> in the dataset they were trained on.', 'This capability can be used in drug discovery to aid in the identification and design of potential new drugs with desired properties<cit.>.', 'One of the key challenges in drug discovery is finding compounds that can interact with specific parts of the body.', 'GPT can help in this process by learning the patterns and relationships from large databases of known compounds <cit.>.', 'GPT can be trained on large sets of chemical databases to analyze chemical reactions and their outcomes.', 'This can help suggest potential combinations of new drugs using the analyzed data.', 'These new drugs can also be analyzed using GPT to test their efficacy and toxicity.', '* Diagnosis:\\n    GPT can be used in medical diagnosis by analyzing patient data.', 'It can help to analyze medical records and extract information such as patient demographics, symptoms, and medical history.', 'This can help medical professionals provide effective patient care and improve outcomes.', 'The recent release of GPT-4 has the ability to support multimodal information, allowing it to analyze images as input and produce text results as output <cit.>.', 'It is recommended to use AI systems such as a GPT, as clinical support tools to assist medical professionals in diagnosing and treating patients, but they should not be relied upon as the sole source of medical advice or decision-making.', \"GPT can also be used to identify rare diseases by analyzing patient's complete information.\", 'The authors in <cit.> have used a general-purpose GPT based on GPT-3 model for patient diagnosis and triage.', 'The model has given a triage accuracy of 70% which was worse than a physician.', 'But, in next subsequent weeks, the accuracy has improved to 92% which is close to the performance of a physician.', 'In diagnosis, GPT-3 model has given 88% accuracy.', 'For emergency cases, GPT-3 has given 75% accuracy whereas physician has given 94%.', '* Disease prediction: GPT has great potential in disease prediction <cit.>.', 'By analyzing large amounts of medical data, including patient records, medical images, and clinical trials, these pre-trained language models can learn patterns and make predictions about the likelihood of a patient developing a particular disease.', 'For instance, trained healthcare GPTs can be used to predict the occurrence of diseases such as diabetes, heart disease, and cancer by analyzing various parameters, including the patient’s medical history, age, family history, and lifestyle.', 'It can also be used to predict the likelihood of a rare disease This helps in the early detection of high-risk patients so that medical personnel can take necessary measures and suitable medicines to reduce the risk of developing the disease.', 'The medical practitioner and author in <cit.> have recommended using GPT-4 models’ ability of NLP in bariatric surgery.', '* Personalized medicine:\\n   The COVID-19 pandemic has highlighted that not all body systems are clinically similar.', 'For instance, during the pandemic, medicines like Remdesivir and Tocilizumab have been effective for one category of patients but do not affect another category of patients with similar clinical metrics, as they progress from a mild or moderate level of infection to a severe stage <cit.>.', \"This highlights the need for personalized medicine in today's world.\", 'GPT can be used to identify variable patterns of data to predict or classify hidden or unseen patterns, which can be used for exploratory data analysis.', 'GPT provide the possibility of identifying personalized medicines <cit.> based on the clinical, genomic, and nutritional data of patients.', 'The dietician and the author in <cit.> have observed that the utilization of Chat GPTs has significantly decreased obesity rates among patients by offering personalized recommendations regarding nutrition plans, exercise programs, and psychological support.', 'This approach allows for the development of customized treatment plans that cater to the specific needs of individuals, leading to a more efficient method of treating obesity with the assistance of Chat GPT.', '§.§.§ Challenges\\n\\nWhile GPT is a powerful language model with numerous applications in healthcare, it is not without its challenges.', 'The primary challenge is data bias.', 'As GPT models are also learning models, the significant drawback of biasing is also applicable to GPT.', 'GPT can be susceptible to bias.', 'If the data used to train the model is biased, the model will learn from it and replicate the bias.', 'This leads to incorrect treatment and predictions.', 'Another challenge is the transparency of the model.', 'GPT is complex to understand and interpret.', 'This lack of transparency in technology can make doctors and medical personnel not believe in the predictions, which may result in a hesitancy to trust and adopt technology <cit.>.', 'Another important concern is security and privacy issues.', 'As it is a model to be trained on data, there is a huge amount of sensitive information about the patients to be used to improve the algorithm and its performance.', 'This results in significant security and privacy concerns related to the use of GPT in healthcare.', 'The final and important challenge is limited clinical validation.', 'GPT are showing promising improvement in various fields of healthcare, such as drug discovery, and disease prediction.', 'But still, their effectiveness and accuracy in medical research and clinical settings have yet to be validated.', 'More research and clinical trials are required to prove that GPT can transform the medical industry with full trust.', '§.§.§ Summary\\n\\nGPT have the potential to revolutionize the healthcare industry by contributing to drug discovery, personalized medicine, clinical support in making decisions, diagnosis support, and disease prediction.', 'This can be helpful for human beings to predict the disease in advance and treat it through proper medicine.', 'However, there are significant challenges that are to be addressed, such as technology adoption, data bias, regulatory challenges, and security and privacy issues.', 'It is so important to analyze and evaluate the benefits and risks of using GPT in healthcare and to continue to monitor their development and implementation.', '§.§ Industry\\n\\n\\n\\n  §.§.§ Introduction\\n\\nAn important economic transition from agriculture and handicrafts to large-scale industry and automated production was achieved by the industrial revolution.', 'Efficiency and productivity were raised as a result of modern equipment, energy sources, and labour arrangements.', 'New opportunities and challenges have been created as a result of the quick development of new technologies in both the workplace and other industries <cit.>.', 'The utilization of big data is a well-known technology-driven trend.', 'Nowadays, companies have access to enormous volumes of data that may be examined to uncover insightful information.', 'Big data can help businesses make wise decisions and discover areas for development.', 'AI is another innovation that is changing industries.', 'AI systems have the ability to analyse complex data, automate procedures, and make wise conclusions <cit.>.', 'This improves production by increasing its dependability, adaptability, and efficiency.', 'The process of \"digitalization,\" which includes incorporating digital technologies into every element of business, is creating industries to become more flexible, efficient, and valuable.', 'Businesses may automate tedious work, improve client experiences, and streamline operations by implementing digital solutions.', \"In today's digitally-driven world, adopting technological advancements is essential for maintaining competitiveness and promoting growth.\", '§.§.§ Impact of GPT in Industry\\n\\nIn industrial scenarios, GPT has the potential to be applied as a sustainability tool, assisting businesses in evaluating and enhancing their sustainability goals.', 'Companies can improve supply chain tracking and query response by integrating pre-trained transformer models like ChatGPT with supply chain management platforms<cit.>.', 'Additionally, GPTs can offer modifications to the production process that might increase efficiency <cit.>.', 'GPT can also help users make knowledgeable decisions about how to use resources, allowing businesses to remain competitive while reducing their environmental effect.', 'For example, the GPT-2 model has demonstrated efficacy in sentiment analysis, providing insightful data for numerous applications <cit.>.', '* Hospitality sector\\nIn the hospitality industry, hotels place a high focus on providing satisfying guest experiences.', 'To ensure that every tourist is satisfied during their stay, this necessitates adapting to their requirements and preferences.', 'Hotels may improve the guest experience in a number of ways by integrating GPT into their website or mobile application.', 'Hotels may respond to consumer inquiries in a timely and precise manner by utilizing GPT <cit.>.', 'Customers do not have to wait for human assistance when looking up information about facilities, booking procedures, or room availability.', \"Customers' overall satisfaction with the hotel's services is increased as a result of the large reduction in client wait times.\", 'GPTs can also make it easier for visitors who speak multiple languages to communicate <cit.>.', 'Hotels can offer a more inclusive and welcoming experience for visitors from other countries by removing linguistic obstacles.', 'Hotels may provide their visitors with immersive and engaging experiences by combining GPT with AR technologies.', \"For instance, customers can use their mobile devices to get AR guided tours of the hotel or nearby attractions, offering a distinctive and entertaining way to explore the surroundings and learn more about the hotel's amenities.GPTs integration into various aspects of the hospitality industry gives hotels the ability to deliver streamlined, tailored, and effective services, increasing client happiness and loyalty.\", '* Fashion: By providing highly customized user recommendations based on personal style, brand preferences, and particular clothing or accessory demands, collaborative filtering and AI algorithms have undoubtedly revolutionized the fashion business.', 'The amount of personalization has been further increased in this context by the incorporation of GPT, dramatically altering the purchasing experience for customers <cit.>.', 'Fashion platforms may analyse a significant quantity of user data, such as browsing history, purchasing behaviour, and style preferences, using the advanced capabilities of GPT to produce tailored recommendations.', 'Fashion platforms can direct consumers towards clothing options that fit their desire for sustainable fashion by including eco-friendly fabric selections into the system.', \"GPT improve users' general fashion knowledge and confidence while enabling users to keep up with the most recent trends.\", 'The image-text retrieval skills of GPT significantly improve visual search capability in fashion platforms <cit.>.', 'Users may make more confident shopping decisions and minimize the need for returns by visualizing how various clothing items and accessories would appear on them without physically trying them on.', \"The model may recommend the proper size for various brands and apparel products by taking into account a user's measurements, preferred fit styles, and historical data.\", 'The overall purchasing experience is enhanced and the frustration of wrong size is decreased.', '* Sustainability:\\nSustainable development means addressing current demands without sacrificing the capacity of future generations to address their own needs.', 'Goals for sustainable development can be attained by implementing GPTs in a variety of sectors, including manufacturing and corporate operations <cit.>.', 'The models can estimate where energy saving measures would be most useful by analyzing past data and patterns to provide insights into energy usage, pinpoint problem areas, and recommend opportunities for improvement.', 'GPTs can aid in identifying sustainability-related problems, creating plans and strategies to solve them, investigating brand-new sustainable activities, keeping track of advancements, and conducting routine reviews.', 'Companies can choose activities that will have the biggest positive impact by grading tasks and actions according to their impact on sustainability <cit.>.', 'The models can optimize supply chains for decreased carbon emissions, minimized waste, and improved resource efficiency by assessing elements including transportation routes, packaging materials, and supplier practises <cit.>.', 'This results in more environmentally friendly production, distribution, and sourcing procedures.', '§.§.§ Challenges\\n\\nThere are many different industrial fields where GPT models can be applied; the three areas mentioned above are only a few.', 'However, for optimal use, the industrial sector needs to be ready to adapt to a constantly changing environment.', 'Public and corporate policies must be developed over the long term to promote the use of sustainable production techniques.', 'For enterprises, deploying pre-trained GPT models can be a costly task.', 'Continuous development and training are also required to accommodate new and evolving inquiries as client expectations change.', 'Companies have to carefully assess the benefits and costs before implementing the GPT model because these continuing efforts raise the deployment cost <cit.>.', 'For industries to fully benefit from GPT models, it is crucial to address issues with interpretability, data reliance, and ethical considerations.', \"Industry may therefore take advantage of these GPT\\xa0models' advantages, make wise decisions, and promote sustainable development.\", '§.§.§ Summary\\n\\nGPTs have the ability to have a positive impact on society and business operations.', 'They can speed up operations like accounting, sales, and marketing, increasing productivity.', 'But before they are widely used, ethical problems need to be fully investigated.', 'Technology products will change as GPT models develop.', 'To reap the benefits and reduce dangers, it is essential to solve interpretability and data concerns.', 'GPTs can have a tremendous positive impact on businesses, society, and the economy when they are used responsibly.', '§.§ Agriculture\\n\\n\\n\\n  §.§.§ Introduction\\n\\nTraditional agriculture, a time-honored practice passed down through generations, sustains civilizations with its crop cultivation and livestock rearing methods.', 'Rooted in a deep connection to nature, it emphasizes sustainability and local ecosystem understanding.', 'Beyond providing sustenance and livelihoods, traditional agriculture preserves cultural heritage.', 'However, it also faces challenges such as labor-intensive processes and shortages, inefficient resource utilization, vulnerability to pests and diseases, and limited access to real-time data and environmental impact.', 'Today, by merging tradition with modernity, we have the opportunity to leverage technological advancements to enhance productivity, sustainability, and resilience while honoring the profound legacy of traditional agriculture for future generations.', '§.§.§ Impact of GPT in Agriculture \\n\\nGPTs have the ability to overcome the challenges of agriculture.', 'It offers valuable advantages to the agriculture sector.', 'It acts as a comprehensive knowledge source, providing information on crop cultivation, pest management, and soil health.', 'By analyzing real-time data, GPT assists farmers in making informed decisions regarding optimal planting times and resource allocation.', 'It plays a crucial role in identifying and addressing crop diseases and pests accurately.', 'Moreover, GPT enables precision farming practices by utilizing sensor data and satellite imagery, ensuring precise irrigation, fertilization, and pest control.', 'Additionally, it provides market analysis and price prediction, empowering farmers to navigate market conditions and optimize pricing strategies.', 'GPT also supports farm management and planning, optimizing crop rotation and resource usage.', 'By facilitating agricultural research and innovation, GPT contributes to advancements in crop breeding and sustainable practices.', 'Embracing GPT in agriculture enhances decision-making, efficiency, and sustainability, ultimately promoting improved productivity and food security.', 'For instance, GPT-4 can educate farmers about new methods and goods and warn them of potential issues or possibilities by analyzing data from many sources <cit.>.', '* Improving Crop Yields:\\n\\n\\n\\nWith its data analysis capabilities and real-time recommendations, GPTs plays a crucial role in enhancing crop yields.', 'By examining historical yield data, weather patterns, soil conditions, and crop management practices, GPT identifies valuable patterns and correlations, providing insights and suggestions for optimal crop management techniques<cit.>.', 'It enables precision farming by integrating data from sensors, satellites, and IoT devices, granting timely guidance on resource allocation for improved efficiency.', 'Additionally, GPT aids in the early identification and management of crop diseases and pests, minimizing yield losses through precise and prompt recommendations.', 'Moreover, GPT supports crop breeding and genetic optimization by analyzing genetic data and plant characteristics, expediting the development of high-yielding and resilient crop varieties.', 'Therefore, GPTs data analysis and decision support capabilities significantly contribute to enhancing crop yields and maximizing agricultural productivity<cit.>.', '* Pest Control:\\n\\n    GPT offers significant support in the realm of pest control in agriculture.', 'By analyzing extensive data on pests, including their behavior, life cycles, and characteristics, GPT can provide valuable insights for effective control measures.', 'It aids in early pest detection by analyzing sensor data and satellite imagery, enabling proactive interventions to prevent pest spread and minimize damage <cit.>.', 'GPT also assists in determining suitable pest control methods tailored to specific crops and pests, considering factors like environmental impact and sustainability.', 'Additionally, it contributes to precision pest control by leveraging real-time data to optimize timing and dosage of interventions, reducing chemical usage and resistance risks.', 'It also aids in identifying natural enemies and beneficial organisms, promoting natural pest control mechanisms such as habitat diversification and companion planting.', \"Through GPT's data analysis and recommendation capabilities, it empowers farmers with informed decisions, leading to more effective and sustainable pest management strategies, ultimately reducing crop losses and enhancing agricultural productivity.\", '* Identifying Diseases and Soil analysis:\\n\\n    GPTs offer valuable assistance in disease identification and soil analysis within the field of agriculture.', 'With its ability to analyze extensive data sets, GPT can accurately identify crop diseases by processing information such as symptoms, historical data, and disease patterns.', 'This enables timely and effective disease management strategies<cit.>.', 'Additionally, It plays a significant role in soil analysis by analyzing diverse soil-related data, including nutrient levels, pH, organic matter content, and soil composition.', 'By interpreting this data, It provides insights into soil health and fertility, empowering farmers to make informed decisions regarding nutrient management, soil amendments, and cultivation practices.', 'Moreover, GPT can identify complex interactions between soil conditions and crop diseases, helping farmers understand the relationship and take preventive measures accordingly.', 'It also supports precision agriculture practices by integrating sensor data and satellite imagery to assess soil variations across fields, allowing for site-specific management strategies and optimized resource allocation.', 'Furthermore, it also facilitates knowledge sharing and collaboration by analyzing and disseminating research findings, best practices, and disease outbreak information among agricultural communities.', 'This collective intelligence enhances disease monitoring and control efforts on a broader scale.', '§.§.§ Challenges\\n\\nWhile GPT, provides significant benefits to agriculture, there are challenges to its implementation.', \"GPT's effectiveness depends on the availability and quality of data, making insufficient or biased data a limitation.\", \"The interpretability of GPT's decision-making process is challenging due to its black-box nature, hindering trust and understanding.\", \"GPT's computational requirements and infrastructure can be demanding, posing difficulties for resource-constrained farmers.\", 'Language and domain-specific nuances can affect its performance, impacting accuracy and relevance.', 'Ethical considerations surrounding data privacy and ownership need careful attention to ensure responsible use.', \"By addressing these challenges, researchers and practitioners can unlock GPT's potential while ensuring its practicality and ethical implementation in agriculture.\", '§.§.§ Summary\\n\\nGPT holds immense potential in agriculture, offering numerous benefits alongside notable challenges.', 'Its data analysis capabilities empower farmers with informed decision-making in disease identification, soil analysis, and precision farming, leading to improved crop yields and sustainable practices.', 'However, the effectiveness of GPT relies on data availability and quality, while its interpretability remains a challenge due to its black-box nature.', 'Additionally, computational requirements, language nuances, and ethical considerations require careful attention.', 'By addressing these challenges, the agricultural sector can harness the full potential of GPT, paving the way for more productive, efficient, and responsible farming practices.', '§.§ Travel and Transport\\n\\n\\n\\n  §.§.§ Introduction\\n\\nHistorically, animals have been used by people as their main source of transportation.', \"But as the world's population increased, the demand for more effective transportation systems increased.\", 'Transportation-related technological advancements have fundamentally changed the sector in several ways.', 'Business operations like order tracking, freight management, and customer support can be streamlined by automation employing AI-driven technologies.', 'Companies can enable their employees to concentrate on more beneficial and profitable duties by automating these tasks<cit.>.', 'With better transportation networks and logistics management systems that optimize routes and reduce transit times, technological developments also enable speedier delivery times.', 'In terms of product development, technical advancement has paved the way for the development of innovative vehicles, infrastructure, and logistics systems, leading to the production of more sophisticated and effective transportation choices.', 'Another noteworthy benefit of technology advancement in logistics and transportation is increased customer service.', 'Inquiries and problems can be handled quickly and efficiently by chatbots and customer support systems powered by AI, improving the entire customer experience<cit.>.', '§.§.§ Impact of GPT in Travel and Transport \\n\\nCompanies can learn about customer preferences in real time by using GPTs in logistics and transportation, which results in better personalization and more customer satisfaction.', 'GPTs leverage NLP approaches to interpret customer requirements and preferences, enabling customized suggestions as well as guidance in the logistics and transportation processes.', 'The most effective routes and forms of transportation can be recommended using GPTs, which can analyse a large amount of data, including traffic patterns, weather conditions, and delivery requirements<cit.>.', 'In addition, GPTs can be used as travel planners, allowing visitors to enter their travel budget, duration, and destination to create customized itineraries.', 'For travel agencies, this personalized approach increases consumer satisfaction and revenue.', '* Logistical Management: GPTs can be quite important in the context of shipping logistics.', 'They can automate the creation of shipping labels, eliminating up manual entry and lowering the possibility of mistakes.', 'Additionally, GPTs can\\xa0have access to real-time tracking data and can integrate GPS data and sensors to provide businesses and customers with precise and up-to-date shipment status information.', 'Companies can successfully monitor shipments with\\xa0the use of GPTs, geographic information systems (GIS), and routing algorithms<cit.>.', 'Organizations can track shipments in real-time and ensure visibility throughout the supply chain by utilizing GPS data and sensor technology<cit.>.', 'Customers can receive precise updates on their shipments using this real-time information, which will improve their experience overall.', 'Overall, the use of GPTs into shipping logistics results in increased automation, efficiency, and client satisfaction.', '* Intelligent Fleet Management and Tracking: Companies can get real-time fleet updates by utilizing GPT models, which enables them to track vehicles quickly and precisely.', \"GPT models' underlying technology also supports proactive fleet management.\", 'GPTs can identify possible problems or maintenance needs before they develop into expensive breakdowns or accidents by analyzing data from a variety of sources <cit.>.', 'With this knowledge, companies may take preventative measures, such as planning maintenance or quickly fixing developing problems, ultimately saving time and money by preventing unintended delays.', 'Additionally, GPTs can provide clever alerts and notifications.', 'Businesses can receive alerts when vehicles arrive at specified areas by setting up specific triggers, which enables better coordination and customer service <cit.>.', \"For instance, businesses can alert clients or storage facilities in advance of a truck's arrival, allowing for effective unloading and loading procedures.\", '* Real-Time Inventory Tracking:  GPTs enable businesses to manage their inventory levels while on the road with a cloud-based platform that makes it simple to access inventory data from anywhere in the world.', 'Better inventory management and decision-making are made possible by this real-time accessibility.', 'This ensures\\xa0that the appropriate quantity of stock is accessible when needed to fulfil consumer requests, while minimizing carrying costs and preventing lost sales as a result of stockouts.', 'GPTs can streamline inventory management procedures by eliminating the need for human data entry into spreadsheets, saving time and cutting overhead costs <cit.>.', 'With the advent of 5G technology, the cost of connected devices has dramatically decreased, making it more practical and affordable for businesses to set up and operate connected inventory monitoring systems.', 'This may make real-time inventory tracking solutions more widely adopted, thereby increasing the effectiveness and precision of inventory management <cit.>.', '* Streamlining Delivery Operations: GPTs are able to estimate traffic trends and improve routes for both drivers and passengers using real-time data <cit.>.', 'These models can produce effective routes that reduce travel times and enhance overall delivery performance by taking into account aspects like traffic congestion, road conditions, and delivery schedules.', 'Route optimization not only reduces travel time but also benefits the environment.', 'In order to improve air quality and create a more sustainable delivery process, it is possible to cut down on idle times and trip distances.', 'Businesses may streamline operations, improve the overall customer experience, and contribute to a more sustainable and environmentally friendly approach to logistics by automating procedures, optimizing routes, and utilizing real-time data <cit.>.', '* Tourism: GPTs have the potential to significantly improve a number of tourism-related aspects.', \"GPTs can offer customized solutions that suit the individual's preferences by understanding their needs and interests, resulting in a more pleasurable travel experience.\", 'GPTs are excellent at understanding and creating text that is human-like <cit.>.', 'This functionality can be used in the travel and tourism sector to enable chatbots or virtual travel assistants to communicate with users in natural language <cit.>.', 'Trip planning and information retrieval are made more simple and user-friendly by the ability of travelers to ask questions, look for advice, and obtain full details about destinations, modes of transportation, customs, and more.', 'GPTs are capable of producing in-depth and interesting descriptions of tourist sites, attractions, lodging, restaurants etc.', 'GPTs can provide time-efficient routes that guarantee a complete travel experience <cit.>.', 'Including advice on local legislation, emergency contacts, medical facilities, and potential risks, GPTs can offer helpful information and direction regarding travel safety.', '§.§.§ Challenges\\n\\nPrivacy issues may occur when using sensitive data in travel GPTs.', 'It is essential to manage user data sensibly and putting up strong security measures to safeguard private data.', \"The quality of the model's outputs is directly influenced by the correctness and completeness of the data utilized during the training phase.\", 'Ethical considerations should be taken into account when creating AI-powered applications employing GPTs.', \"It's crucial to check that the models are truthful, unbiased, and free from harmful presumptions or discriminatory procedures <cit.>.\", 'Although the models contain advanced features, they are difficult to tailor for specific use cases, need a lot of data to train, and have built-in limitations.', \"§.§.§ Summary\\n\\nEmerging GPTs have the potential to enhance productivity, communication, and the calibre of goods and services, which will benefit many aspects of people's life.\", 'GPTs can offer real-time updates, effective route optimization, and customized recommendations in the travel and transportation industries, enhancing the overall travel experience and increasing operational effectiveness.', 'Adopting them, however, comes with some difficulties.', 'As specific roles are replaced by automation, GPTs may result in job displacement <cit.>.', 'Additionally, the computational and memory requirements for GPTs make their deployment on compact or low-power devices difficult.', 'GPTs may not be accessible to growing businesses due to the high costs associated with obtaining and using them.', 'Despite these obstacles, attempts are being done to overcome them and improve the usability and value of GPTs for a larger range of users.', '§.§ E-Commerce\\n\\n\\n\\n  §.§.§ Introduction\\n\\nElectronic commerce, commonly referred to as e-commerce, is a way for conducting economic transactions and create relationships between groups of people and entities using digital information processes and electronic communications <cit.>.', 'Globally, this type of trade has experienced substantial growth, particularly in the retail sector.', 'The preference for internet shopping, especially among younger millennials, is a prominent trend in consumer behaviour.', 'Mobile devices have consequently taken over as the main method for carrying out internet transactions <cit.>.', 'Therefore, it is crucial for e-commerce companies to give the customer experience in their mobile applications top priority.', 'The provision of brief text summaries for titles and reviews is an essential component of this.', 'These summaries are essential for optimizing search results, helping consumers identify appropriate items, and ultimately raising customer happiness in the online purchasing space <cit.>.', '§.§.§ Impact of GPT in E-Commerce Realms\\n\\nThe e-commerce sector could significantly advance with the introduction of GPTs.', 'GPTs can be accessed by users or customers and are intended to answer commonly asked questions and give in-depth details about many elements of the e-commerce process, such as products, delivery, refunds, and more <cit.>.', 'One of the main benefits of GPTs is their capacity for quick responses, which decreases the amount of time customers must wait to hear back from businesses <cit.>.', 'By taking care of an important number of client inquiries, this function not only increases customer happiness but also lessens the workload on support workers.', 'Customers will ultimately have a better purchasing experience as a result of being able to quickly acquire the information they require and interact with GPTs <cit.>.', '* Proofreading: To improve the calibre and accuracy of written content in e-commerce, GPTs can be used for proofreading.', 'Written content is essential for product descriptions, marketing materials, customer reviews, and other text-based components in the e-commerce sector <cit.>.', 'For the purpose of projecting professionalism, fostering trust, and delivering a satisfying user experience, this text must be devoid of errors, well-written, and grammatically correct.', 'E-commerce companies can automate the process of identifying and correcting these problems by using GPTs for proofreading, which saves time and effort as comparison to manual proofreading <cit.>.', 'This can be especially helpful in situations when users are writing product reviews or interacting with customer service.', 'An improved user experience is facilitated by the early detection and rectification of errors, which also helps to avoid potential misunderstandings or miscommunications.', '* Order Processing: GPTs are useful in many areas of order management and customer service because they can comprehend and produce text that looks like human speech.', 'GPTs can help with handling consumer questions about orders <cit.>.', 'GPT is capable of interpreting the queries, providing important details like order status, tracking information, and expected delivery time, as well as suggesting corrections for frequent problems <cit.>.', 'By delivering real-time information, GPTs can assist customers in tracking their orders.', 'Customers can customize their purchase with the help of GPTs.', 'GPTs can help in the identification of possibly fraudulent orders by examining past transaction data, consumer behaviour patterns etc <cit.>.', \"Based on a customer's past purchases, browsing habits, and preferences, GPTs can offer tailored product recommendations.\", 'When a consumer puts a purchase, the model can examine the information and produce recommendations for related or supplementary products that the customer might find interesting.', '* Generating titles for products: Companies can use GPTs to produce interesting and educational material to improve the appeal of their product listings <cit.>.', \"Based on a product's category, brand, and special characteristics, GPTs can come up with attractive titles for it.\", 'The model can produce imaginative and memorable names that aid in brand awareness and differentiation by receiving relevant information such as the characteristics of the product and the target market.', 'GPTs are trained to produce in-depth and interesting product descriptions <cit.>.', 'These summaries can offer a thorough summary that aids clients in selecting products wisely.', 'GPTs are capable of coming up with clever and appealing captions for product images.', 'GPTs can be adjusted to better reflect the tone and aesthetic of a certain brand <cit.>.', 'As a result, the brand identity is consistent and unified throughout all product listings.', '* Strategy Planning: GPTs have the ability to come up with original and distinctive concepts for marketing campaigns <cit.>.', 'The model can provide recommendations for different campaign aspects, such as slogans, taglines, themes, contests, social media strategies, and more by taking into account relevant information about the product, target audience, marketing objectives, and desired outcomes.', 'GPTs can help with email writing that encourages readers to become partners, investors, or customers <cit.>.', 'To increase the likelihood of a favourable response or interaction, these emails can be customized to address the needs and potential benefits for the receivers.', 'To improve their comprehension and production of appropriate material, GPTs can be trained on domain-specific knowledge bases, such as e-commerce <cit.>.', 'The models can offer more precise and situation-specific recommendations for advertising strategies, product positioning, and target audience interaction because of this specialized training.', '* Data analysis: There are numerous ways to use GPTs for data analysis in e-commerce.', 'E-commerce data preparation can be aided by GPTs <cit.>.', 'Data normalization, cleansing, and formatting are a few of the duties involved.', 'GPTs can produce summaries, identify significant topics, and extract appropriate data by studying textual descriptions, reviews, and consumer feedback.', 'This helps you know the data more thoroughly, identify trends, and find insightful information.', 'Customer reviews and social media comments from e-commerce can be analyzed for sentiment using GPTs.', 'The sentiment expressed in text can be evaluated using GPTs and categorized as either positive, negative, or neutral <cit.>.', 'Understanding client views, recognizing product strengths and deficiencies, and making data-driven decisions all benefit from this analysis, which also helps to increase customer happiness.', 'Segmenting consumers based on preferences, behaviours, or past purchases can be aided by GPT models <cit.>.', 'For the purpose of detecting fraud in e-commerce transactions, GPTs can be used.', 'GPTs can support the identification of potentially fraudulent actions by examining past transaction data, user behaviour patterns, and recognized fraudulent tendencies <cit.>.', '§.§.§ Challenges\\n\\nWhile GPTs have a lot of potential for numerous e-commerce applications, they also have several drawbacks.', 'In order to produce responses, GPTs mostly rely on the context given in the input text.', 'They could, however, find it difficult to fully understand the broader context or details that are unique to e-commerce.', 'GPTs provide replies using training data and prior knowledge.', 'They are unable to access real-time data or carry out real-time calculations <cit.>.', 'They might not be appropriate for giving current information, such as pricing, product availability, or dynamic promotional offers.', 'GPTs gain their knowledge from a wealth of training data, which includes text taken from the internet, which may be biased, stereotyped, or otherwise offensive <cit.>.', 'The models may unintentionally provide biased or unsuitable replies if they are not rigorously managed and monitored, which could be harmful to the customer experience and brand reputation.', 'The use of ethical principles and the training data must both be given careful thought.', '§.§.§ Summary\\n\\nThe conversational interface offered by GPTs customizes the purchasing process and makes interactions with clients more interesting and appropriate to their individual requirements.', 'GPTs can also be utilized to get insightful customer feedback.', \"Businesses can learn about customers' preferences, issues, and opinions regarding their products and services by conversing with them.\", 'In order to better serve their target audience, organizations can use this information to discover areas for improvement, increase customer happiness, and make data-driven decisions.', \"It's essential to recognize that GPTs might occasionally make mistakes or give poor answers, particularly when dealing with complicated or ambiguous queries.\", 'This highlights the necessity of continual model training, thorough testing, and modification to guarantee that they consistently meet consumer needs.', 'To confirm the efficacy and dependability of using GPTs specifically in the e-commerce area, more research and testing are required.', \"When implementing GPTs, it's critical for businesses to take into account the particulars of their own e-commerce businesses, their target market, and the type of the client enquiries.\", 'Regular monitoring and feedback analysis, along with a systematic and iterative approach, can help make sure that the outcomes of using GPT models are in line with the objectives of e-commerce enterprises.', '§.§ Entertainment\\n\\n\\n\\n  §.§.§ Introduction\\n\\nIn the ancient days, the Entertainment meant about playing games with neighbors covers all outdoor games, indoor games and chatting with neighbours through telephone.', 'As digitization has bought greater advancements in computation and communication, in turn access to internet is also much easier.', 'This has changed the way people are entertained.', 'as people are connected and fully engaged in completing the target for the day.', 'And there was a radical shift from traditional employment to employment in the Industrial Revolution age.', 'Stress and pressure are common factors hindering people of different age groups.', 'The different forms of entertainment serve as stress busters.', 'Entertainment and mental health are interrelated; the former transfers happiness, bringing harmony and peace to mental health.', 'Some common forms of entertainment include playing games, watching TV series or movies, or funny videos, shopping, debugging, coding, browsing the internet, listening to music, dancing, chatting, painting, crafting, reading books, cooking, and many more, which can lessen the stress carried<cit.>.', 'Entertaining and getting entertained is the biggest motivation and medicine for all mental illnesses.', 'Entertainment helps to improve the motor skills of humans, thereby inducing a positive cognitive effect towards the work.', '§.§.§ Impact of GPT on the Entertainment Industry\\n\\nGPT is a potential game-changer in the entertainment field, delivering endless entertainment.', 'Since its evolution, GPT models have been adopted as an entertainer crosschecking their ability to produce content on funny and illogical questions.', 'GPTs entertain people in many ways, and of course, using GPT itself an entertainment as it reduces the burden of overthinking by providing immediate feedback to queries in seconds<cit.>.', 'The results are amazing and have been utilized for many purposes today.', 'When the GPT model was probed to complete a scene from the movie “Frozen,” it responded with an entertaining writeup<cit.>.', 'Some of the impacts of GPTs on Entertainment applications are given below:\\n\\n\\n\\n\\n  * Solitude with GPT: As the GPT itself is an entertainer, one can feel better alone with the GPT, which helps to come out of loneliness by exploring its savors<cit.>.', 'GPTs assist in providing soothing poems, mental healing quotes, and funny riddles.', 'People with loneliness may feel anxiety, especially with older ones at home.', 'In this case, GPT-4 helps people with its Voice Technology feature, enabling users to input their audio<cit.>.', 'In turn, the GPT model responds to user-specific speech output using NLP algorithms embedded with it.', 'The elderly can feel safe and attentive at home.', 'GPT-4 is multilingual and can understand various dialects and accents for personalized user experience.', '* Enhanced Customer Interaction: The advent of ChatGPT and Bard has improved customer interactions on content such as movies, Over-the-Top (OTT) platforms like Netflix, Hulu, Disney+ Hotstar, and prime video, sound recordings, song lyrics, pictorial works, comics, jokes, memes, viral videos, and other entertaining factors.', 'Further, GPTs provide human-like recommendations on user-specific fun activities based on user interactions for an immersive experience.', 'This has dramatically improved the interactions in the engagement industries.', 'User engagement can be further improved by providing dynamic and more realistic responses to user queries, such as creating virtual actors for interacting with real actors<cit.>.', '* Personalized Content Creation: GPTs can help generate user-specific personalized content by analyzing the user preferences and generating content like predicting future scenarios tailored to the user’s interest.', 'GPTs can be used for creating personalized, engaging, and high-quality content for online business advertising, ideas for content generation, marketing messages for attracting customers, descriptions for selling products, and captions for social media<cit.>.', 'In addition, it can be used for optimizing the contents for search engines, i.e., GPTs will provide relevant terms for search, thereby avoiding traffic to the web sources.', '* For the Film and TV industry: GPT-powered virtual assistants assist users in booking tickets and generating content and personalized recommendations using AI models.', 'The evolution of GPT-4 with advanced NLP and DL algorithms helps the scriptwriter to generate AI-driven content without the human author named virtual storytelling<cit.>.', 'GPTs create interactive stories, dialogues, and characters, recommending suitable characters.', 'Furthermore, GPTs can be used to create content for video games, voice-enabled applications, AR applications, and other VR experiences in virtual worlds<cit.>.', '* For Social media influencers: GPTs can generate personalized marketing ads for each customer based on their previous interactions and provides relevant suggestions for customer viewing experiences.', 'Youtubers and other social media content creators will potentially benefit from generating channel content based on demand and realistic societal activities.', '* Realistic Gaming Interactions: GPT helps to generate the players, gaming narratives, dialogues, user interface, and user-specific gaming recommendations and new game creation.', 'Powerful HCIs can render a better user experience for game developers and players.', 'Assistance to the game developers in debugging and enhancing the code developed.', 'GPT uses various NLP and AI algorithms trained with massive data to predict the next phrase/movements and provide human-like experiences in 3D gaming environments.', 'ChatGPT has been integrated with AR and VR to provide an immersive gaming experience.', '§.§.§ Challenges\\n\\nLatency is the major issue connected with rendering the voice-based response to the voice input.', 'As well, plausible misinterpretations may mislead the responses, and interruptions to the relayed output are difficult.', 'Enabling technologies like EC and 5G can help overcome this issue.', 'Also, GPTs must be capable of storing the facts with audio conversations to relay them while conversing the other day.', 'Furthermore, the AI system must be built in such a way that it can continuously learn (lifelong machine learning) and enhance over time.', 'The major ethical concern with virtual storytelling is the bias exposed in the training data and the obscurity of reproduced content on the generated stories.', 'Another issue with the generated content is plagiarism (i.e., producing content similar to the content in the published articles or books), raising disputes with intellectual property rights.', 'In addition to this, the source of the content generated remains unexplored.', 'The language barriers in using GPT must be lessened to improve user experience and utilize the features of GPT <cit.>.', 'The implication of the user to provide inputs in a certain format to GPTs can be further improved by providing different options in addition to voice-based inputs GPT4, like braille screen input for visually disabled people.', 'The user authentication can also be further enhanced to safeguard user-specific content generation and avoid repeated content generation for users with similar requests.', 'One of the primary concerns with the GPTs adoption is job loss.', 'Content creators, bloggers,  and poets may lose their jobs.', '§.§.§ Summary\\n\\nThe entertainment industry is the one which will be in demand always, as it is a lifeline for many individuals leading a stressful work environment or personal life.', 'Despite the stress, entertainment has become part of routine life due to its immersive nature, creating harmony in the mind and the environment.', 'GPTs have made a major contribution to enhancing the entertainment industry, but the job security of many professionals in this field remains unanswered.', 'GPTs must be trained on unbiased data and ensure transparency in source content generation to provide a secure, robust, and efficient contribution to the entertainment industry.', 'To attract all types of users, the multilingual capability and content rendering of GPTs can be further enhanced.', 'The issues constrained by providing user inputs to GPTs can be alleviated to all extent.', 'Furthermore, safer user content generation without plagiarism and relating facts with previous conversations can be guaranteed by abiding by the storage requirements to deal with a more personalized user experience.', '§.§ Lifestyle\\n\\n\\n\\n  §.§.§ Introduction\\n\\nLifestyle, the way of our living, is one of the prominent areas most people in today’s digital era of AI,  are bound to and look for constant improvement.', 'The “modus vivendi” is a Latin expression that semantically means a way of living and should be understood in terms of values and attitudes.', 'These two terms manifest self, influenced by family, society, and global media.', 'Directly or indirectly, these influence an individual’s lifestyle.', 'Adopted from a sociological perspective, an individual expresses oneself through different practices, viz., eating ways, drinking behaviours, mode of travel, travelling places, costume designs, body-shaping cloth to wear, media preferences, education choices, job preferences, entertainment modes, managing leisure time, means of communication and so<cit.>.', 'In all these practices, individuals would like to explore and learn about what, where, how, and when factors for reading sustainable development<cit.>.', 'The concept of lifestyle is all about “how one wants to live one’s life.”  Consumerism is the act of purchasing artifacts for societal status and is one of the thriving lifestyle factors.', 'Certain standard indicators like job, wealth, and physical and mental health determine the quality of one’s life.', 'Also, the choice of a healthy lifestyle moderately determines the health of an individual<cit.>.', 'Furthermore, few people believe that lifestyle reflects their socioeconomic status.', 'Many epidemiologic studies state that better lifestyles have dramatically reduced the risk of various chronic diseases and are the primary cause for their prevention<cit.>.', 'The lifestyle has been defined on different societal levels from individual, positional, and national to global<cit.>.', 'At the global level, lifestyle is adopted by general world-class influencers.', 'In contrast, at the national level, the influencing factors will be the government and different cultural patterns across the country.', 'The positional level concerns influence from different status groups, age categories, gender groups, and social classes.', 'And the individual level is influenced by a closely moving group of individuals concerned about self-identity.', 'The major source of information about these influencers is the Internet through social media networks and personal development advertisements.', '§.§.§ Impact of GPT in Lifestyle\\n\\nThe most remarkable application of AI, the GPT, paves the way for the betterment of mankind in offering human-like intelligent conversation on all whereabouts.', 'People will always prefer to interact with other peers to learn their attributes and tweak them for societal status.', 'Various GPTs have flourished for different lifestyle indicators, and they provide human-like assistance to all queries on fine-tuning the lifestyle by harnessing the power of AI<cit.>.', 'The advanced reasoning capability of GPT-4 serves the purpose better<cit.>.', '* Diet Planner: Free GPT applications for maintaining a balanced diet, helping the individual with a weight loss diet plan, followed by a brief list of meal plans, required shopping lists, physical activity plans targeting particular body parts<cit.>, motivational messages, and personalized sleeping patterns.', 'These apps act more like personalized training assistance and help to track progress with visualization charts or graphs.', 'Fitness level, available free hours, medications taken, and available exercise equipment will be given as input to GPT.', '* Travel Guide and Trip Advisor: Harnessing AI models, GPT provides an individual’s travel plan itinerary based on information like the place(multiple cities), budget, and the number of days.', 'These GPTs provide local recommendations on restaurants, hotels, and other attractions.', 'RoamAround, Roamr, and VacayChatbot are some of the travel planning GPTs<cit.>.', '* Personalized Stylist and Beauty Advisor: GPTs can act as personalized stylists for an individual by generating occasion-specific clothing and costume preferences.', 'GPTs can assist in organizing wardrobes based on seasonal outfits and provide recommendations on e-commerce fashion stores for purchasing favourite brands.', 'GPTs can provide tailoring design options, fabric choices, and design materials.', 'Furthermore, GPTs can provide updates on a stock based on the preferred searches and provide insight into fabric types suitable for weather conditions that suit personal style.', '* Personalized CookBook: GPTs can serve as cooking assistants by recommending new curated recipes suiting the family dietary plan, ingredients available, time, individual’s cooking skills, and new flavoured ingredients.', 'ChefGPT, PantryChef, and MacrosChef are some GPTs that generate unique and delightful recipes<cit.>.', 'Consequently, GPTs can assist in shopping list recommendations and the nutritional value of the recipe generated.', '* Hobby Curator : GPT assists an individual in identifying one’s enjoyable leisure time activity by learning new skills<cit.>.', 'Having a list of interests and ideas ready, the GPT helps narrow down various options, instructional videos to proceed, chatting and sharing with online communities, and researching the cited hobby to explore more fun.', 'Budget will also be an important factor in this perspective, as learning new hobbies may require joining paid classes or courses.', 'GPT provides step-by-step instructions and guidelines to learn a new skill faster.', '* Dream Maker: GPTs with multimodal learning helps to search for a job based on one’s qualifications and experience.', 'In turn, it assists in preparing the job-specific resume, cover letters, training for the interviews (coding and technical queries), and grooming sessions and can redirect to the training place where knowledge can be acquired<cit.>.', 'The futurist GPT models can assist in phase-by-phase questionnaires in the interview process\\n\\n\\n\\n  §.§.§ Challenges\\n\\nThe recent version of the GPT uses both reinforcement and supervised learning models so it can learn based on the interaction with the user and can use existing data to derive personalized decisions.', 'In the context of lifestyle, GPTs offer the most promising solution for almost all lifestyle influencers, but the still challenging part is the trustworthiness of the data and copyright issues.', 'Also, relying more on GPT as it solves all our problems may insipid human intelligence in upcoming generations.', 'Though the GPT provides weight advice, it can never be a substitute for the medical practitioner, as some information can be misleading.', 'Travel planning GPTs sometimes require users to update information in a specific format and may have outdated databases.', \"GPTs cannot access specific job openings' websites but can still provide insights into acquiring them.\", 'At times, it can produce nonsensical information <cit.>.', 'Therefore, before adopting the GPT recommendations fully, further instigation is recommended.', 'Furthermore, developing a large multimodal learning model abiding huge and dynamic datasets will be costly.', '§.§.§ Summary\\n\\nGPT is a personalized assistant for improving an individual’s lifestyle from various prospective influencers.', 'Generating personalized recommendations alleviates an individual’s fear of survival in the digitized society.', 'Individuals will be personally trained to adapt to different cultural and technological shifts in the sustainable development of themselves and the economy as a whole.', 'On the other hand, more stringent recommendations may incur huge budget overruns and sometimes provoke the individual to misinterpret, leading to dreadful consequences.', 'GPTs provide both positive and negative recommendations based on the input fed.', 'So, for the effective adoption of a GPT for lifestyle practices, adverse training and testing on extreme behaviours must be carried out.', 'GPTs must be trained in the realistic and dynamic perception of individuals in real life.', '§.§ Gaming\\n\\n\\n\\n  §.§.§ Introduction\\n\\nBefore the advent of technology and the gaming industry, entertainment was primarily centred around activities such as reading, listening to music, watching plays and movies, participating in sports and physical games, and socializing with friends and family.', 'People also engaged in traditional board games and card games, which were often played in groups and provided a fun and social way to pass the time.', 'After technology stepped into the gaming industry, the way games are created, and the experience it has given users have transformed tremendously.', 'Technology has enabled developers to create more immersive and engaging experiences for players.', 'It has contributed in various ways, like improving graphics, performance, online play, and mobile gaming.', 'Improved GPUs and other technologies allow for more detailed and realistic graphics, making games more visually stunning.', 'Faster processors and higher amounts of RAM allow for smoother gameplay and faster loading times, reducing lag and improving overall performance.', 'Technologies like AI, AR, and VR have created a new dimension of game development and experience.', 'Players can now immerse themselves in gaming worlds in a way that was not possible before.', 'With the help of advanced AI techniques, game developers can create more sophisticated and challenging opponents for players, as well as NPCs with more realistic behaviours.', 'Technology has greatly expanded the possibilities of gaming and enabled developers to create more immersive, visually stunning, and engaging experiences for players.', '§.§.§ Impact of GPT in Gaming \\n\\n\\nGPTs have the ability to contribute to all sectors, including the gaming sector.', 'GPT are not specifically designed for creating and playing games, but they have the potential to improve the gaming experience by improving enhanced dialogue and story telling, creating dynamic and personalized gaming worlds, generating more realistic and engaging characters <cit.>, game content creation, chatbot development.', '* Chatbot development: GPTs have been used in gaming through the development of chatbots that use NLP to communicate with players <cit.>.', 'Because it allows the chatbot to understand and respond to a wide range of user inputs and queries related to the game.', 'GPTs have been pre-trained on a large corpus of text data, which makes them adept at NLP.', 'It can understand and respond to user queries in a way that feels natural and intuitive.', \"It can also understand the context of a user's query, which means they can provide relevant and useful responses even when the user's query is ambiguous or incomplete.\", \"It can also generate game-related content, such as descriptions of game characters or settings, that can help to enrich the user's gaming experience.\", \"Furthermore, it can also personalize the user's experience by learning from their previous interactions with the chatbot and tailoring its responses accordingly.\", '* Game content creation: GPTs are used in game design.', 'They are used to create game content such as levels, items, and quests.', 'If the game designer is working on a new role-playing game, GPT can be used in creating characters to be used in the games.', 'To generate new character classes in the games, the developer has to give inputs that contain information about the game environment, game settings, player abilities, and game play mechanics.', 'GPTs has the ability to analyze the text and expectations given by the developer, and it can generate a list of potential character classes based on the expectations given as text.', 'The designer then refines the ideas and chooses a more suitable character to develop further with unique abilities and game mechanics.', 'The authors in <cit.> have used GPT2 and GPT3 to procedurally generate role-playing game with video game descriptions.', 'The resultant quest  was evaluated by 349 online RPG players.', 'The results concluded that one of the five quest descriptions was accepted for game development.', \"* Analyze player’s ability and skill: GPTs can detect and analyze players' abilities and skill levels and tailor the game accordingly.\", \"This analysis helps in making dynamic modifications to the game environment based on the player's abilities and skill levels.\", 'This feature helps achieve dynamic difficulty balancing.', \"GPTs can also assist in identifying the player's intent.\", 'Thus, when players ascend to higher levels, it can assist in making the games more challenging based on the player\\'s abilities and skill levels in the previous levels\\n    \\n  * NPCs: NPC stands for \"Non-Player Character.\"', 'In AI games, NPCs refer to characters or entities in a game that are not controlled by a player.', 'NPCs can take on a variety of roles within a game, such as enemies to fight, quest givers, merchants, or friendly characters that provide helpful information.', 'They are often controlled by AI algorithms that determine their behaviour and actions within the game world.', 'GPTs are not specifically designed for creating NPCs, but they can be used to generate dialogue and other character interactions that can be incorporated into NPCs.', 'Additionally, It can be used to generate character backstories and personalities, which can inform the development of NPCs.', 'The authors in <cit.> have trained and used GPT-2 for text generation of video games.', 'They have trained GPT-2 on a large corpus of video game quests and used a GPT model to generate the dialogue for quest-giver NPCs in role-playing games.', 'The output has shown that GPT can learn the structure and linguistic style of the games, and the quality of the content it has generated is high, making it a good alternative to writing new RPG quests by hand.', '§.§.§ Challenges\\n\\n GPTs are computationally expensive and require high computing resources to do their purpose.', 'This means that implementing them in a game would require powerful hardware and this could have an impact on the performance of the games.', 'Lack of training data: GPTs require large amounts of high-quality training data to be effective.', 'In the gaming industry, this could be difficult to obtain, as gaming data are likely to be fragmented and less structured than the kind of data used to train GPT models <cit.>.', 'In addition, GPTs can perform content creation based on patterns they have learned from their training data, which means that they can be unpredictable.', 'The content generated by GPT may be nonsensical or inappropriate content to the game.', 'In the context of gaming, this lack of control could lead to undesirable or even offensive game content.', \"GPTs can generate text based on user input, they can't interact with the game environment in the same way a human player can.\", 'This limits their usefulness in gaming and may make them less effective than other AI technologies.', \"§.§.§ Summary\\n\\nGPTs can transform the gaming industry by contributing to improved game dialogue creation, enhanced non-player characters, personalized gameplay, procedural content generation, chatbot generation, and analyzing players' abilities.\", 'However, it also has potential challenges that are to be addressed, such as the need for high computing resources, a lack of control over content creation, and restricted interaction with the game environment.', 'In addition, the most important challenge in adopting a GPT model in gaming is a lack of training data.', 'If the challenges are addressed and the gaming industry evolves with properly structured data to train a GPT model, then GPTs can revolutionize the field of gaming.', '§.§ Marketing\\n\\n\\n\\n  §.§.§ Introduction\\n\\n\\nTraditional marketing primarily relied on traditional media channels, such as television, radio, newspapers, and magazines, to reach consumers.', 'Companies used to develop marketing campaigns based on demographic data, and mass media channels were used to broadcast these campaigns to a broad audience.', 'However, the advancements in technology have brought about significant changes in the marketing industry, and companies are increasingly integrating new marketing strategies evolved through technologies to reach and engage with customers.', 'One of the significant transformations has been the rise of digital marketing channels such as social media, search engines, Email, and mobile applications that allow companies to target specific populations with precision and provide real-time feedback on campaign performance, allowing for more effective and efficient marketing.', 'Technology has also given rise to marketing automation tools such as customer relationship management systems, chat-bots, and personalized email marketing, which have made marketing more efficient and effective.', 'Another significant transformation has been the use of big data and analytics to better understand customer behaviour and preferences.', 'This has allowed companies to create more personalized and targeted campaigns based on specific customer needs and preferences.', '§.§.§ Impact of GPT in Marketing \\n\\nThe marketing industry has evolved with various AI-powered techniques.', 'This revolution started in marketing by providing businesses with powerful tools for generating insights, automating processes, and improving customer experiences.', 'GPTs are also being used in marketing to generate engaging and personalized content.', 'Some of the applications of GPT in marketing include content creation, customer service, and personalized advertising.', '* Content creation: GPTs can contribute to marketing in various ways, such as by improving speed and efficiency in content creation, ensuring consistency and quality of content, generating personalized content, creating multilingual content, and repurposing existing content.', \"It can be trained on a company's existing marketing materials and customer data, allowing it to create new content, such as blog posts, social media updates, and product descriptions, in a fast and efficient manner.\", 'Despite its speed, it maintain high standards for quality and consistency.', 'Moreover, GPTs <cit.> can generate personalized content based on customer data, such as search history and past purchases.', \"This helps create content that is relevant to the users' desires, leading to better engagement and conversion rates.\", 'GPTs can also generate content in various languages, allowing marketers to expand their reach across regions.', 'Copy.ai <cit.> has used GPT-3 to generate human-like text that is optimized for marketing purposes such as website copy, social media posts, advertisement copy, and email campaigns.', 'This means that marketer personnel no longer focus on content creation.', 'Instead, they can spend productive time improving the other aspects of marketing.', '* Customer service: GPTs can be trained on customer service conversations and chat logs to generate more natural responses, like humans.', 'This can help business personnel provide better customer service 24/7 and save time and resources.', 'It can be trained to generate automated responses for frequently asked questions, providing faster responses to customers and ensuring consistency in the quality of replies.', \"GPTs can also analyze customers' emotions and sentiments, enabling businesses to proactively address negative feedback.\", \"This is particularly helpful in maintaining customers' trust.\", 'The authors in <cit.> have used GPT-3 model for automated drafting of responses for incoming mails.', 'They used it to understand the mail, and then software engineering and business studies were used to understand the challenges encountered and finally, the response generated after a thorough understanding of the context of the mail.', 'The authors have concluded that applying GPT-3 to rationalize email communication is feasible both technically and economically.', \"* Personalized advertising: GPTs can generate personalized content such as product descriptions, blog posts, and social media captions tailored to individual customers' preferences and interests.\", 'This can help businesses create content that resonates with their target audience, leading to higher engagement and conversion rates.', 'By analyzing customer data, GPTs can segment customers according to their behaviour, interests, and preferences.', 'As a result, businesses can tailor their marketing campaigns to each segment and provide personalized messaging and offers that are more likely to connect with each customer group.', 'The authors in <cit.> have proposed a generative model to identify the name of the product from the product text and use this information filter to improve the product recommendation based on the product retrieval model.', 'This method has been implemented in the dynamic product advertising system of Yahoo.', \"It is observed that the recommendation system has recommended the product based on the user's interest, and it was evaluated using an A/B test to serve similar products in an ad carousel, which can help the system to explore more products efficiently.\", '* Forecast analysis: Using customer data analysis, GPTs can forecast future behaviour and buying patterns.', \"This allows businesses to customize their marketing campaigns to each customer's desires based on their purchase patterns, increasing the likelihood of conversion or purchase.\", 'The authors in <cit.> have used chatGPT to perform predictive modelling based on past data.', 'They have used the GPT model to predict the future based on the customer’s behaviour and buying pattern.', 'This primarily helps the system to recommend the products to the customers as per their desires.', \"§.§.§ Challenges\\n\\nGPTs are designed to generate content that imitates human writing, but the content generated may not align with the brand's image or message.\", 'This lack of control can be a potential challenge for marketers.', 'Another challenge that applies to all learning technologies is that data bias is possible in GPTs <cit.>.', 'Based on the large dataset of text used for training, if the data is biased, it will affect the generated content, which may also exhibit the same biases.', 'GPT is complex and difficult to interpret, making it challenging to explain how the model arrived at its conclusions.', 'This lack of transparency can lead to a lack of trust in adopting GPTs, and marketing teams may struggle to make improvements in their strategies.', 'As like every AI technology, there are ethical concerns associated with GPT models.', 'For instance, the use of GPT in marketing could raise concerns about the use of personal data and privacy, particularly if the model is used to generate targeted advertising or personalized content.', 'To avoid any negative consequences, companies must ensure they use these models ethically and transparently.', '§.§.§ Summary\\n\\n\\nUsing GPTs in marketing can provide various benefits, such as better content creation, personalized messaging, increased efficiency, competitive advantage, and enhanced customer experience.', 'However, this strategy also involves potential challenges, such as limited control, data bias, lack of transparency, and ethical considerations.', 'Therefore, companies must consider the advantages and drawbacks of GPT adoption in marketing, and implement these models ethically and transparently to avoid negative outcomes.', 'Successful integration of GPTs in marketing requires proper planning, a skilled workforce, and continuous monitoring to ensure the desired results and mitigate any potential risks.', '§.§ Finance\\n\\n\\n\\n\\n  §.§.§ Introduction\\n\\nThe finance industry, also known as the financial sector, is a broad term that encompasses a wide range of institutions and businesses that provide financial services to individuals, businesses, and governments.', 'The finance industry plays a critical role in the global economy, facilitating the flow of funds between savers and investors, managing risk, and providing financial services and products to support economic growth.', 'The finance industry has been the leader in technology adoption in recent years, with a focus on improving efficiency, reducing costs, and delivering better customer experiences.', 'The adoption of technologies like big data and analytics, mobile and digital payments, blockchain and distributed ledger technology, AI and ML, and cloud computing make the sector more flexible, scalable, trustworthy, transparent, secured, and easier to access.', '§.§.§ Impact of GPT in Finance\\n\\nGPT has greatly influenced finance by automating customer support using chatbots and virtual assistants, enhancing fraud detection, offering investment insights and recommendations based on financial data and news, assisting with risk assessment for investments and loans, impacting algorithmic trading strategies, simplifying compliance with regulations by analyzing legal documents, improving credit scoring and loan processes, and emphasizing the importance of handling sensitive financial data securely and transparently.', '* Sentiment analysis: Sentiment analysis is a technique used in the finance industry to evaluate the sentiment of investors <cit.>  and the general public towards specific companies, industries, or markets by analyzing news articles, social media posts, and other text-based sources of information.', 'GPT has the potential to improve sentiment analysis in finance by providing more accurate and detailed analyses of financial data.', 'With sentiment analysis, the industry can predict stock prices by assessing the sentiment of news articles, social media posts, and other sources of information about a particular company or industry to make informed investment decisions.', 'By utilizing sentiment analysis, GPTs can aid financial institutions in identifying potential risks and taking appropriate action to mitigate them.', 'The authors in <cit.> have investigated how incorporating a lexicalized ontology can enhance the performance of aspect-based sentiment analysis by extracting indirect relationships in user social data.', 'The investigation results show that the analysis has given 98% accuracy.', '* Financial forecasting: GPTs can be trained on past financial market data to predict future trends in the stock market, exchange rates, and other financial metrics.', 'This can help investors and financial organizations make more accurate predictions and reduce their risk exposure.', 'With the ability to analyze and process the natural language, GPTs can be used to analyze and interpret financial data, news, and other related information.', 'Financial analysts and researchers can use the ability to analyze natural language to extract insights from unstructured data like news articles, social media content, and other information that is relevant to forecasting.', 'This can help improve the accuracy of financial forecasting models by providing a more comprehensive view of market trends and sentiments.', 'This analysis may help improve the accuracy of prediction.', 'Financial analysts can use the model to identify the relationship between the financial parameters that could change the market conditions in advance.', 'This prediction may be helpful for investors as they make investment decisions.', '* Trading strategies: GPTs can also be used to analyze market trends and historical data to develop trading strategies.', 'This can help traders make better decisions in terms of trading to increase their profitability.', 'GPTs can be used to identify the potential risks in trading portfolios.', 'By analyzing the large volume of information related to trading, GPT will get the potential to identify the risk parameters and provide insights into how to mitigate these risks.', 'The authors in <cit.> have used a popular GPT for stock market trend prediction.', 'The results show that the method used is simple but the efficiency and accuracy of the method are very effective.', 'The prediction it has made is very close to the reality.', '* Risk prediction and management:The adoption of GPT can enhance the process of risk prediction and management in several ways.', 'It can improve data analysis by detecting patterns that may pose a risk.', 'It can also help in enhancing fraud detection by analyzing transaction data and identifying fraudulent activity based on patterns.', 'Additionally, GPT can be utilized to make better portfolio management decisions by analyzing historical industry data, company financial statements, and news articles, as well as social media feeds.', 'This portfolio management process can provide valuable information about the investment risk of a given organization, enabling informed investment decisions and effective risk management.', '§.§.§ Challenges\\n\\nGPTs have more challenges in the finance sector.', 'Primarily, they demand significant computational resources to train and deploy, which can be expensive and time-consuming for financial organizations to implement.', 'Another challenge is that, even though GPTs are capable of producing precise predictions, they can be challenging to interpret, which can present a problem for financial institutions seeking to comprehend the reasoning behind specific predictions <cit.>.', \"This lack of interpretability can harm risk management objectives.Implementing GPT in finance sector can be vulnerable to adversarial attacks, which are designed to manipulate the model's output by injecting false data.\", 'This can be particularly problematic for financial institutions that rely on GPTs for risk management and investment decisions.', 'It also require large amounts of training data to achieve high accuracy.', 'However, in some cases, financial institutions may not have access to sufficient data to train the model effectively.', 'GPTs can also be biased if the training data used to develop the model is biased.', 'This can lead to inaccurate predictions and unintended consequences.', '§.§.§ Summary\\n\\nThe use of GPTs in the finance industry has promising benefits such as improved risk management, enhanced fraud detection, better portfolio management decisions, and increased efficiency.', 'However, it also has potential challenges that need to be addressed, such as high computational requirements, the complexity of implementation, limited interpretability, vulnerability to adversarial attacks, limited training data, and bias in training data.', 'So, the use of GPTs in the finance industry presents significant benefits but also requires careful consideration of the challenges involved to ensure the effective and secure deployment of these models.', '§.§ Summary On Impact of GPT models in Applications\\n\\nThe impact of GPTs in various applications and challenges was highlighted.', 'GPT  with its varied usage has changed the way people perceive facts such as content creation, enhanced user interfaces, personalized learning, item tracking, self-awareness, market risk analysis, business forecasts and introspection.', 'However, there are concerns about the potential negative impact of GPTs, such as the spread of fake news, bias in data and decision-making, not domain specific, ethical issues, data reliability, the complexity of implementation, multimodal and multilingual support, security and privacy concerns, vulnerable to data attacks, limited input data, explainability of results, large model size, high computational requirements and job loss.', 'Despite these concerns, it is clear that GPTs will continue to be a powerful tool for industries seeking to leverage the power of NLP and generative AI.', 'As the technology improves and new applications emerge, it will be interesting to see how GPTs continue to shape the future of industries around the world.', '§ PROJECTS\\n\\n\\n\\n\\nProject Summary Table.', '!', \"[HTML]EFEFEF \\nProject    \\n  DeepScribe    \\n  Meena    \\n  Jukebox    \\n  Uber's plato research dialogue   system    \\n  Polyglot AI    \\n  SiriGPT \\n Application widely used for    \\n  Healthcare    \\n  Lifestyle    \\n  Entertainment    \\n  Transport    \\n  Education    \\n  Lifestyle \\n Purpose    \\n  Medical documentation and to improve doctor-patient association    \\n  Personalized product recommendation    \\n  Enables the original music    creation both artistically compelling and commercially viable in a   variety of styles and genres    \\n  Enhances user experience using Uber rides, helps drivers and riders in   scheduling rides, navigating routes, providing real-time updates on traffic   and weather conditions.\", 'enables absolute communication irrespective of the language barrier   across  different regions and   cross-culturalism    \\n  Assist with voice-based assistants \\n GPT Adoption    \\n  Customized version of GPT\\'s    \\n  Google\\'s seq2seq transformer-based neural network architecture similar to   Open AI\\'s GPT    \\n  GPT-2 extension called \"Multi-Scale Transformers for Music   Modeling\" (MST) model    \\n  GPT-2    \\n  GPT-0, GPT-1,GPT-2,GPT-3    \\n  GPT-3 \\n Dataset    \\n  Not Disclosed    \\n  Meena dataset over 40 billion words , 341 GB captured from public domains   like Reddit and social media platforms    \\n  1.2 million songs, 600,000 pieces of sheet   music, 45,000 MIDI files    \\n  Persona-Chat with 160,000 conversational dialogues, Cornell Movie-Dialogs   Corpus with 200,000 movie conversation, DailyDialog over 13,000 dialogues,   and    \\n  CONLL-2003, Sentiment140 dataset, Reuters Corpus, 20 Newsgroups dataset,   WMT (Workshop on Machine Translation) datasets and SQuAD (Stanford Question   Answering Dataset)    \\n  Information not publisized \\n Building Blocks    \\n  Recurrent Neural Network and Attention mechanism fueled by NLP techniques    \\n  Seq2Seq Transformer-based Architecture    \\n  Transformer-based Language Model and Autoregressive model    \\n  Language modeling, Dialogue modeling, Discrete latent variabe modeling   and response ranking    \\n  Language Identification, Named Entity Recognition (NER), Sentiment   Analysis, Text Classification, Machine Translation, Question Answering    \\n  Transformer-based neural network architecture \\n Evaluation Metrics    \\n  Bleu score, perplexity    \\n  Bleu score, perplexity    \\n  Frechet Audio Distance (FAD) and Pitch and Rhythm Similarity    \\n  Bleu score, Perplexity and Distinct n-gram    \\n  accuracy, precision, recall, , F1-score, Bleu score as well as   cross-entropy loss or perplexity    \\n  Perplexity, BLEU score, F1 score, ROUGE score, Human evaluation \\n Addressed Challenges    \\n  Reduced Transcription errors and enhanced patient care    \\n  Natural and Engaging conversations    \\n  Fresh orginical music content creation and drastically reducing the cost   and time by creating high-quality music contents, and also to preserve and   advance musical heritage.', 'customer service, user experience, and operational efficiency    \\n  Multilingualism and Sentiment Analysis are the key challenges in NLP and   Polyglot AI solved this problem by offering a tool for supporting morethan 40   languages and pre-trained sentiment analysis model    \\n  Language understanding and generation, Data scarcity, Contextual   understanding, Text summarization, Sentiment analysis, Named entity   recognition \\n Input data    \\n  Audio    \\n  Text    \\n  Audio    \\n  Text    \\n  Text    \\n  Audio \\n Owned By    \\n  DeepScribe    \\n  Google    \\n  OpenAI    \\n  Uber    \\n  Uizard Technologies    \\n  Apple \\n \\n\\n\\n\\n\\n\\n\\nProject Summary Table (continued).', '!', \"[HTML]EFEFEF \\nProject    \\n  AI Dungeon    \\n  Copy.ai    \\n  Bond AI    \\n  Viable    \\n  AI Channels    \\n  Fireflies.ai \\n Application widely used for    \\n  Gaming    \\n  Business and marketing    \\n  Finance    \\n  Business Analytics    \\n  AI Industry    \\n  Business \\n Purpose    \\n  Interactive and engaging storytelling experience for players    \\n  help clients create written content more quickly and easily    \\n  To enhance the financial well-being of clients    \\n  provide businesses with intelligent insights to help them make better   decisions    \\n  provide a platform for developers, data scientists, and machine learning   practitioners to create, deploy, and manage their AI models    \\n  to simplify the meeting process and reduce the time and energy required   for note-taking and collaboration \\n GPT Adoption    \\n  GPT-3    \\n  GPT-3    \\n  GPT-3    \\n  GPT-4    \\n  GPT-3    \\n  GPT-4 \\n Dataset    \\n  Common Crawl, OpenAI GPT-2, and various text datasets from Kaggle    \\n  books, articles, and websites    \\n  likely use of a combination of publicly available financial datasets,   proprietary data, and client data    \\n  Information not publisized    \\n  Users' own dataset    \\n  Possible datasets: the Common Voice dataset from Mozilla   having over 9,000 hours of speech data in multiple languages \\n Building Blocks    \\n  Machine Learning Models, Text Input Interface, Game Engine, Content   Database, Player Feedback System, Cloud Infrastructure    \\n  NLP, Language Models, Neural Networks    \\n  NLP, Personalization, Conversational User Interface, Data Analytics    \\n  Unsupervised learning, Contextual understanding, Sentiment analysis,   Topic modeling, Entity recognition    \\n  Pre-built models, Model training, Data preparation, Collaboration    \\n  Speech-to-Text Technology, NLP, Cloud Computing, Integration technologies \\n Evaluation Metrics    \\n  Response Coherence, Response Diversity, Player Satisfaction, Engagement,   Realism, Novelty    \\n  Perplexity, BLEU score, ROUGE score, F1 score    \\n  Intent recognition accuracy, entity extraction accuracy, and language   model perplexity    \\n  Perplexity, Accuracy, F1 score, Word similarity    \\n  Accuracy, Precision and Recall, F1 Score, Perplexity, User satisfaction    \\n  Speech Recognition Accuracy, NLP Performance, Integration Performance,   Task Completion Time, User Satisfaction \\n Addressed Challenges    \\n  Narrative Generation, Content Creation, Personalization, Replayability,   Accessibility, Creative Expression    \\n  Lack of writing skills, Inconsistency, Multilingual content creation    \\n  Personal financial management, Customer engagement, Fraud detection and   prevention    \\n  Understanding unstructured data, Contextual understanding, Visualization   and exploration of data, Customization and integration    \\n  Natural language understanding, Scalability, Personalization, Integration   with other systems, Maintenance and updates    \\n  Time-consuming manual note-taking, Difficulty in capturing important   details, Lack of visibility and accountability, Communication barriers \\n Input data    \\n  Text    \\n  Text    \\n  Audio and Text    \\n  Text    \\n  Text    \\n  Audio \\n Owned By    \\n  Latitude    \\n  Copy.ai    \\n  Bond.AI    \\n  Viable AI    \\n  MiroMind AG    \\n  Fireflies AI \\n \\n\\n\\n\\n\\n\\n\\n\\n\\nThis section presents the exciting projects developed using GPT model technologies for different applications mentioned in the above sections.\", 'Table.', '<ref>, Table.', '<ref> shows the different levels of such projects along with different parameters to compare their characteristics leveraging the capabilities in many real-life applications.', '§.§ SiriGPT\\n  \\nSiri <cit.> is an intelligent digital assistant that enables Apple device users to complete tasks more efficiently and with ease, often anticipating their needs even before they make requests.', 'SiriGPT  <cit.> <cit.> is a voice assistant powered by a GPT model and developed entirely using shortcuts.', 'Apple device users can utilize ChatGPT, fueled by GPT-3, by using an API key provided by OpenAI.', 'This novel combination offers the best of both worlds, allowing users to utilize SiriGPT for voice commands and ChatGPT for generating text.', 'SiriGPT utilizes a tokenizer exclusively developed by Apple that has been optimized for processing natural language tasks.', \"SiriGPT's training data is not publicly available as it is exclusive to Apple.\", 'However, the language model is trained on diverse text data from various sources such as books, news articles, web pages, and other text data sources.', 'This ensures that SiriGPT can handle different natural language tasks accurately and efficiently.', 'It has been reported that SiriGPT is one of the largest language models available, with over a trillion parameters.', '§.§ AI Dungeon\\n \\nLatitude, a startup based in Utah, created a groundbreaking online game called AI Dungeon <cit.>, which showcased a novel type of collaboration between humans and machines.', 'It is a  free-to-play, single-player, and multiplayer adventure game that caught traction within the gaming community.', 'It combines fantasy and AI to create endless possibilities, e.g., one can take charge of a military operation to defend against aliens or become a famous detective investigating an attempted murder of the queen of the fairies.', 'Unlike games with predetermined storylines, AI Dungeon allows you to guide the AI to generate unique characters and scenarios for your character to interact with.', 'The game boasted about incorporating the GPT-3 text generator, but then the algorithm began producing unsettling narratives, including graphic depictions of sexual encounters involving minors <cit.>.', '§.§ Copy.ai\\n  \\n\\nCopy.ai <cit.> is a mighty AI startup founded by Paul Yacoubian in 2020.', 'This project is created using GPT-3, mainly targeting business and marketing campaigns.', 'It has the following use cases: (i) For Teams: It assists with producing customized sales copy, composing long-form articles and pages on a large scale, reusing content on various platforms, and creating product descriptions; (ii) For Emails: The AI-powered email writer takes care of the most challenging parts of marketing by creating email campaigns that are highly effective at converting leads, all with just a few clicks of a button; (iii) For Blogs: By generating content briefs and crafting one-of-a-kind SEO-focused blog articles every month, it can save a significant amount of money for the business.', \"In addition, it's feasible to create briefs, outlines, and even initial drafts in mere minutes, which can be utilized as an excellent source of inspiration for writers to create high-quality content; (iv) Social Media: It aids in generating social media posts quickly and efficiently, allowing for a rapid expansion of the social media following.\", 'Additionally, Copy.ai includes a suite of other tools, such as a headline analyzer, a language translator, and a content rephrase.', '§.§ Bond.AI\\n \\n\\nBond.AI <cit.> is a company focused on AI for financial institutions, which has a headquarters in Little Rock, Arkansas.', 'It was established by Uday Akkarajuin in 2016 and prided itself on providing AI technology centred around human needs.', 'This innovative project offers a product named BondBot, which is powered by Empathy Engine 3.0 and ChatGPT, to enhance the financial health of clients.', \"It assists financial institutions and employers in promoting interconnected finance by offering various tools to improve the institution's profitability and the financial health of its clients on a single network.\", 'It uses customer data to create individual personas for every bank customer or small business, considering their behaviours, strengths, and potential needs.', \"This approach enables the platform to develop multiple customized pathways to holistically enhance clients' financial well-being.\", '§.§ Viable\\n \\nViable <cit.> is a platform powered by GPT-4 that utilizes the latest advancements in NLP and AI to offer businesses intelligent insights to aid their decision-making processes.', 'Companies can extract actionable insights from unstructured data sources, such as social media posts, customer reviews, and survey responses, by employing Viable.', \"GPT assists in comprehending the sentiment and context behind the data, resulting in valuable insights that can enhance a company's services, products, and customer experience.\", 'Viable\\'s \"Insight Explorer\" is a distinctive feature that enables users to interact with and visualize their data via a user-friendly interface.', 'In addition, the platform offers advanced analytics capabilities, including entity recognition, topic modelling, and sentiment analysis.', 'The GPT-based technology of Viable is continually evolving and advancing, which allows the platform to deliver more precise and insightful data.', 'Moreover, Viable provides customized integration and solutions to cater to the specific requirements of each business.', '§.§ AI Channels\\n  \\nAI Channels <cit.> is a platform that provides a comprehensive set of tools for developers, data scientists, and machine learning practitioners to develop, launch, and manage their AI models.', 'The platform offers an all-in-one solution for creating personalized AI models, starting from data preparation and model training to deployment and monitoring.', 'Users can train their models on their data or, on pre-trained models provided by AI Channels.', 'These models can be deployed as APIs or Docker containers on various infrastructures, including different cloud platforms.', 'It also provides a dashboard for tracking model performance and managing configurations.', 'It covers various use cases, including computer vision, NLP, and speech recognition.', 'The platform includes pre-built models for tasks such as image and text classification, object detection, and sentiment analysis.', 'Additionally, users can create their models using popular frameworks.', 'The main objective of AI Channels is to make building and launching AI models more accessible to developers and businesses without specialized AI skills.', '§.§ Fireflies.ai\\n \\nFireflies AI <cit.> is a privately held company based in San Francisco, California, founded by Krish Ramineni and Sam Udotong.', 'Fireflies AI  software is powered by GPT-4 to automate notes-taking tasks and collaborations during meetings.', 'It is compatible with various video conferencing platforms, including Zoom, Google Meet, and Microsoft Teams, and it can transcribe meeting audio and video content in real time.', 'Its primary function is based on speech-to-text technology, which enables it to generate a searchable transcript of the meeting, which can be used for later review and to recall essential points and action items.', 'Additionally, the software utilizes NLP capabilities that can identify significant keywords and phrases within the conversation.', 'Apart from note-taking, Fireflies AI includes collaboration tools such as assigning tasks and sharing notes with other team members.', 'It can integrate with project management and task tracking tools to automatically generate tasks based on the identified action items during the meeting.', 'Fireflies AI provides several customization options to suit particular use cases and workflows.', 'Users can configure the software to automatically join specific meetings or capture audio only from specific speakers.', 'It allows users to specify particular words and phrases to highlight in the transcript, making it easier to identify critical points during the later review.', 'Thus, Fireflies AI aims to simplify the meeting process and reduce the time and energy required for note-taking and collaboration.', \"§.§ Uber's Plato Research Dialogue System\\n   \\n\\nUber's AI Lab introduced Uber's Plato Research Dialogue System in 2020 developed by a team of researchers and engineers to enable the intelligence in riding experience.\", 'PLATO - Pre-trained Dialogue Generation Model with Discrete Latent Variable <cit.>.', \"Uber's Plato Research Dialogue System uses GPT-2, a large-scale language model developed by OpenAI in 2019.\", \"Uber's Plato Research Dialogue System project used several datasets to train and evaluate their conversational agents such as Persona-Chat contains 160,000 conversational dialogues, Cornell Movie-Dialogs Corpus with 200,000 movie conversations, DailyDialog over 13,000 dialogues, and EmpatheticDialogues over 25,000 user dialogues.\", 'The main components in developing the GPT-powered PLATO project are\\nlanguage modelling, dialogue modelling, discrete latent variable modelling and response ranking.', 'The Plato Research Dialogue System was trained on a massive corpus of text data consisting of over 40 GB of uncompressed text while Bleu score, Perplexity and Distinct n-gram are the evaluation metrics used for training and testing the PLATO project.', \"Uber's AI PLATO has addressed many key challenges like customer service by personalizing user feedback with conversational AI agent, user experience using the Uber platform for scheduling rides, navigating routes, and providing real-time updates, and increasing operational efficiency by reducing the need for human customer service representatives and enabling faster and more accurate communication between riders, drivers, and the Uber app.\", \"§.§ Jukebox\\n    \\n\\nJukebox, a GPT-powered music creation, was developed in 2020 as an extension of Open AI's GPT language model <cit.>.\", \"Jukebox's goal is to push the boundaries of what AI can accomplish in the world of music creation and to investigate fresh applications for AI.\", 'A variation of the GPT architecture, the \"Multi-Scale Transformers for Music Modeling\" (MST) model, was created specifically to handle the intricate and multi-scale nature of musical data.', \"Additionally, Jukebox can produce lyrics that match the music's tone and style.\", 'A sizable and varied dataset of musical recordings, lyrics, and related metadata was used to train Jukebox such as 1.2 million songs sourced including Lakh MIDI Dataset, Free Music Archive, Spotify and Tidal, 600,000 pieces of sheet music were sourced from IMSLP (International Music Score Library Project), and 45,000 MIDI files from Lakh MIDI Dataset and the MIDIworld collection.', 'Faster training times and more effective use of computational resources were made possible by the distributed computing setup with 2048 TPU( Tensor Processing Unit) cores used to train the Jukebox model.', 'Training the model required significant computational resources demanding faster training times by the distributed computing setup with 2048 TPU  (Tensor Processing Unit) cores used to train the Jukebox model.', 'A combination of subjective and objective metrics was used to assess and test Jukebox.', 'In a large-scale subjective assessment, more than 1,000 participants listened and rated each one individually determining the overall score for each song produced.', 'On the other side, objective assessments were conducted by evaluating Frechet Audio Distance (FAD) and Pitch and Rhythm Similarity.', 'Overall, Jukebox revolutionizes with its significant advancement in the music industry through creative inspiration, music production, music education and preservation of music heritage.', \"§.§ Meena\\n   \\n\\nGoogle's Meena project was developed by Google Research Team in 2020 for providing personalized product recommendations <cit.>.\", 'The primary goals of the Meena project empowered the lifestyle sector to enhance the user experience and customer service by recommending goods and services on a personalized basis.', 'The project designed a GPT using the seq2seq transformer-based neural network architecture, in particular for open-domain conversational agents.', \"The architecture was pre-trained over 341 GB of text captured from Reddit and other social platforms containing over 40 million words and called this massive collection as 'Meena Dataset'.\", 'Meena was tested using the automated performance metrics known as Bleu score and perplexity on a cluster of HPC nodes with a total of 2048 NVIDIA V100 GPUs.', 'One of the biggest challenges solved Meena was building trust and generating reliable engaging human-like conservation that typically enhances user satisfaction and personalization.', 'Meena has achieved state-of-the-art performance compared to other open-domain chatbots and revolutionized the wide range of applications in the lifestyle industry and a way beyond by providing natural and engaging responses through virtual assistants, customer service bots and personal shoppers.', '§.§ DeepScribe\\n\\n\\nDeepScribe was a GPT-based medical project developed in 2019 by the student team at the University of California by partnering with giant US-based healthcare providers such as One Medical, Stanford Medicine, Mount Sinai and Sutter Health <cit.>.', \"The DeepScribe's technology aims at transcribing medical conversation allowing doctors to treat the patients rather than noting down the patient's history, enhancing the doctor-patient relationship and targeting the overall quality of patient care.\", 'Although DeepScribe used the customized variants of Open AI, the technical details of the GPT model used for customizing the model were not disclosed which was optimized for medical transcription tasks.', '§.§ Polyglot AI\\n\\nPolyglot AI is a communication platform designed to generate text in multiple languages and process the data by performing several tasks such as advanced NLP techniques, text translation, and sentiment analysis.', 'The potential features of Polyglot AI have been exploited in the following application areas such as language translation, chatbots, language learning tools, content creation, customer support, and data analysis across different languages and regions.', 'Polyglot AI is built based on different variants of GPT models, and state-of-the-art language model architecture for NLP tasks, which uses the self-supervised learning approach.', 'The Polyglot AI was pre-trained using a large amount of textual data on multiple languages simultaneously in an unsupervised environment using a shared architecture, Multilingual Universal Sentence Encoder (MUSE).', 'MUSE developed by Google, is a pre-trained DL model used for cross-lingual TL, that encodes the text into common vector space for multiple languages.', 'Thus, the Polyglot language model was created with the following pre-training techniques as Masked Language Modeling (MLM), Translation Modeling Language (TML), sequence-to-sequence modelling and cross-lingual TL.', 'The pre-trained language model is fine-tuned and evaluated by standard benchmarks and metrics such as the BLEU score (Bilingual Evaluation Understudy), METEOR (Metric for Evaluation of Translation with Explicit ORdering) or F1-score.', 'Remarkably, Facebook used new Polyglot AI to translate between 100 languages <cit.>.', 'Thus Polyglot AI enables absolute communication irrespective of the language barrier across different regions and cross-culturalism.', 'Thus, this section focused on several exciting real-life projects which are developed and used for humankind.', 'These projects were discussed by presenting Table <ref> highlighting the details of the project with model architecture, datasets used, training and testing, and evaluation metrics involved with the challenges addressed.', 'The next section will discuss the open research issues and future directions for the potential benefits of GPT models.', '§ OPEN RESEARCH ISSUES AND FUTURE DIRECTIONS\\n\\nThis section highlights the various open research issues concerned with the implementation and adoption of sustainable GPT models.', 'It also provides insights into future research directions for the betterment of researchers in the field of GPT development.', 'Fig.', '<ref> outlines the many issues that can develop while using GPT models, as well as the various future approaches that need to be considered for the effective usage of GPT models.', '< g r a p h i c s >\\n\\n\\tChallenges and Future Directions.', '§.§ Domain Specific GPT models\\n\\nDomain-specific GPT models are mandated in almost all applications; developing these models is still challenging and an open issue within GPT.', 'While the current GPT models have been developed to understand natural language and generate content effectively, their performance may not be equally effective when handling specific domains, such as medicine, agriculture, etc.', 'One of the key challenges in adapting to a particular domain is the availability of domain-specific data.', 'It is well known that the performance of GPTs is directly proportional to the quality and quantity of data used for training the model.', 'So, obtaining such quality data for a specific domain is expensive and time-consuming, as the data are heterogeneous.', 'Also, these data accumulations may even make these models much larger, sometimes catastrophic too, leading to forgetting the knowledge attained during the process.', 'To overcome this issue, pre-training tasks and domain-specific model generation are integrated by data augmentation<cit.>.', 'Another challenge is fine-tuning the model to accustom to the unique characteristics and vocabulary of the domain.', 'A few domain-specific GPT models have been developed and implemented despite these challenges.', 'There is a growing interest in creating more domain-specific GPTs for various domains.', 'Moreover, these models will be trained using the knowledge acquired from large language models specific to domains.', 'Therefore, these models can be fine-tuned for specific tasks or domain-specific requirements with gradually improving performance.', 'GPT models have the potential to be trained in any context, and researchers are exploring new approaches and methods to address these challenges.', 'Furthermore, these models will be more efficient, enhanced interpretability, and domain generability than the existing Large language models as they are customized to specific domain concerns and can provide more concise and informative solutions.', 'TL can be used for developing domain-specific GPT models.', 'Domain-specific GPT models were developed to summarize products based on customer reviews on an E-commerce site, where the language model is pre-trained on the Chinese-short summarization dataset and has obtained fine-tuned results<cit.>.', 'Besides these challenges, domain-specific models require higher computation costs for the resources and time spent in pre-training and relearning in downstream tasks during fine-tuning of pre-trained domain-specific models.', 'Therefore, domain-specific model development must focus on optimizing resource consumption and fine-tuning the pre-trained model to alleviate the forgetting problem involved in existing models.', '§.§ High Computational requirements\\n\\n As the Transformer model utilizes varied heterogeneous datasets for training and learning from the knowledge acquired, one of the key challenges of GPT models is high computational resources for pre-training and inference.', 'The computational requirement continuously increases as the models become more complex and larger.', 'Depending on the size and complexity of the model and the available resources, the time required to train the model can take days, weeks, or even months.', 'Moreover, the inference time for these models is typically slower, making it challenging to use them for real-time applications.', 'This poses a significant obstacle to adopting GPT models for many practical applications.', 'Despite these challenges, significant efforts are underway to overcome them.', 'To accommodate the increasing data size and pre-training computational requirement, data enhancement-based GPT models were developed<cit.> by joining the downstream tasks and pertaining process by reconstructing the domain-specific text before proceeding for pre-training and utilizing the empirical knowledge rather than learning for falsy domain data.', 'Researchers are exploring various ways to optimize and speed up the training and inference process, such as using specialized GPUs and TPUs.', 'They are also developing more efficient algorithms and attempting to reduce the model size without sacrificing performance.', 'In addition, ChatGPT has evolved to include plugins <cit.> that enable statistical analysis for real-time applications.', 'By integrating these plugins with the help of third-party services, ChatGPT can now be used for analyzing real-time applications as well.', '§.§.§ Increasing Model size and Space Constraints\\n\\nDeveloping and training large language models, such as a GPT model, can be a challenging task due to significant technical and computational difficulties as discussed.', 'The size of GPT models presents a major challenge, as the computational resources required for training and inference increase with the number of parameters that need to be trained.', 'As the model size increases, it also requires more memory to store and manipulate parameters during training and inference<cit.>.', 'Acquiring and processing vast amounts of high-quality training data is another challenge in training large language models like GPT.', 'For instance, GPT-4, which is the largest GPT model to date with 1 trillion parameters, demands a massive amount of computational resources, such as specialized hardware like GPUs and TPUs, spatial requirements, and high-speed network connections to transfer data between different parts of the system.', 'Model evaluation and interpretation are also critical challenges.', 'Since large language models like GPT are trained on a massive scale, understanding how the model makes predictions and why it generates specific outputs is difficult.', \"Evaluating the quality and accuracy of the model's output and identifying and addressing biases or errors in its performance can also be challenging.\", 'As these and other significant efforts continue, we can expect the challenge of computational resource requirements for GPT models to transform into a strength in the future.', '§.§ Explainability and interpretability\\n\\nExplainability and interpretability are currently major challenges for GPTs for specific applications.', 'Explainability refers to providing a clear and understandable explanation of how the model has arrived at any output.', 'Interpretability, on the other hand, refers to the ability to understand the internal processes of the model.', 'GPT models are highly complex and difficult to understand and interpret due to their size and architecture.', 'The outcomes and decisions of the model are based on previous learning and training, and the models learn from vast amounts of data to make decisions.', 'These decisions may not be easily explainable to humans.', 'This lack of transparency and interpretability raises concerns about the reliability and safety of the model, particularly in critical applications such as healthcare and finance.', 'Researchers are currently conducting much research to make GPT models more explainable and interpretable <cit.> by utilizing EXplainable Artificial Intelligence (XAI) to provide explanations for the decisions arrived at, specifically to different users at stake.', 'As well, XAI models enable interpretability by providing detailed explanations for the internal process.', 'As GPT can generate any type of unconstrained output for instance code generation for the given problem, it requires proper justifications and explanations for the output.', 'So, to assure these codes by GPT are reliable, a metric model to evaluate and validate this GPT code was developed using NLP metrics and XAI for model interpretability<cit.>.', \"Also, some domain-specific GPT models of GPT-3 have evolved with solutions <cit.> <cit.> to ensure that the GPT model's decisions are understandable, explainable, and trustworthy enough to be used for critical applications like healthcare and finance.\", '§.§ Data Bias\\n\\nData bias is an open issue concerned with the adoption of any advancements in AI, till GPT<cit.>.', 'This is also a prominent challenge for GPT and other machine-learning models.', 'It refers to patterns or relationships in the data that do not accurately reflect the true distribution of the target population or domain.', 'GPT models are trained on vast amounts of text data which may contain bias in language use or cultural assumptions.', 'Still, the source of data remains undeclared, considering GPTs are trained using internet data which may have faulty, fake, and error data, GPTs may generate biased texts or information imitating the training data<cit.>.', \"Such biases can be amplified in the model's output, resulting in false or unfair results.\", 'Data bias can arise from various sources, such as selection bias, labelling bias, concept drift, confounding variables, and changes in input data distribution over time.', 'For example, suppose a dataset used to train a GPT model is dominated by a particular demographic group.', 'In that case, the resulting model may be biased in its predictions towards that group, leading to inaccurate or unfair predictions when applied to new data.', 'This bias can have serious consequences, especially in healthcare, finance, and law enforcement, where biased results can significantly impact human lives.', 'To mitigate these issues, researchers have developed strategies such as diversifying the training data, debiasing the training data, modifying the model architecture, and using post-processing methods to normalize the data and create more fair and inclusive GPT models.', 'The authors in <cit.> have made an in-depth analysis of the most downloaded text generation model GPT2.', 'By examining the intersections of gender with religion, sexuality, ethnicity, political affiliation, and continental name origin, the authors evaluated prejudices associated with occupational associations among various protected categories.', 'These biases may have inaccuracies in climatic data prediction or global warming<cit.>.', 'Therefore, data bias must be of greater concern in GPT model development as the data quality of the internet is limited to avoiding producing disturbing content.', '§.§ Multimodal support\\n\\n\\nThe challenge of developing multimodal learning ability in the GPT model remains unsolved.', \"Multimodal support refers to the GPT model's ability to process and generate text along with other modalities, such as audio, images, and videos.\", 'GPT models have shown impressive results in generating high-quality text and NLP tasks, but it was primarily designed for text-based tasks and cannot handle other modalities.', 'However, due to its success in text processing, users expect its integration with other modalities, such as speech recognition, video summarization, and image or video captioning<cit.>.', 'Several research initiatives have been proposed to integrate multimodal support to address this issue.', 'One approach is to feed the visual and audio information with the corresponding text to the model as input.', 'The other is to handle this input modality process as a separate model and use the output as input to GPT.', 'Multimodal video captioning is done using GPT in the unlabelled videos<cit.>.', 'Multimodel learning has been applied for information retrieval<cit.> and image generation for illustrating the news<cit.> to assist the GPTs.', 'However, the primary challenge in both approaches is effective integration, requiring architectural changes and techniques to handle various modalities.', \"Recently, OpenAI's GPT4 has launched with multimodal support, enabling it to read images, analyze the input, and generate text as output.\", 'It cannot create images as output, though.', 'Nevertheless, the field of multimodal processing is still an active area of research, and much work must be done to effectively and efficiently process and understand multimodal data.', '§.§ Robustness\\n\\n\\nThe robustness is a major requirement to be imposed by any type of GPT model, and it is a global problem for all learning-based prediction technologies.', 'Robustness refers to the ability of the model to maintain high performance and accuracy even in the face of unexpected or adversarial inputs.', 'Although GPT models have shown impressive performance in a wide range of NLP applications and have set a benchmark for high-quality text generation, they are still vulnerable to certain types of errors and attacks.', 'In particular, handling adversarial inputs is a challenging task in GPT models.', 'GPT models are particularly susceptible to adversarial attacks<cit.>.', 'Adversarial inputs are specifically designed to make a learning model collapse and misbehave.', 'GPT models can be highly prone to these attacks because they are trained on a large volume of text.', 'As a result, they may be influenced by subtle patterns or biases in the training data.', 'If such biases or patterns exist in the data, the GPT model may amplify or perpetuate existing biases, leading to unfair outcomes.', 'A few techniques may be used, such as adversarial training <cit.><cit.>, defensive distillation <cit.>, and regularization techniques <cit.> such as dropout, weight decay, and batch normalization, to mitigate and handle adversarial inputs.', 'Therefore, GPT development must focus on developing models with more robustness, enabling them to be tolerant of various vulnerabilities, and thus to be used reliably and susceptible in a wide range of applications.', '§.§ Multilingual support\\n\\nWhile GPT models have demonstrated remarkable proficiency in NLP tasks for individual languages, achieving multilingual support remains a significant challenge.', 'The primary difficulty in developing multilingual GPT models lies in the significant differences in syntax, grammar, and vocabulary across various languages.', 'As the number of internet users day by day increasing irrespective of literacy rate, multilingual support will target all types of end users.', 'To create models that can effectively process multiple languages, researchers need to train GPT models on extensive, diverse datasets that span a broad range of languages and language families.', 'Additionally, designing language-specific pre-processing techniques to prepare input data for the model is another obstacle to overcome.', 'Various languages possess distinct writing systems, word orders, and linguistic features, necessitating specialized pre-processing techniques to ensure that the model can process the input data effectively.', 'Despite the challenges, researchers continue to explore new methods to improve the multilingual capabilities of GPT models.', 'Some techniques involve training separate models for each language or developing language-specific fine-tuning techniques.', 'Others include developing cross-lingual TL techniques that allow the model to transfer knowledge and skills learned in one language to another.', '§.§ Limited understanding\\n\\n\\nGPT models have a limited understanding of context and meaning, despite their ability to generate coherent text.', 'This problem arises due to issues such as a lack of semantic understanding, bias, stereotyping, and handling nuances and figurative language.', 'As a result, the outputs generated by the model may contain errors or inaccuracies, even if they are grammatically correct.', \"Researchers are exploring various techniques to enhance the model's contextual understanding.\", 'Understanding GPTs will be more reactive and may attract more users for accurate results<cit.>.', \"These methods include incorporating external knowledge sources like knowledge graphs and ontologies into the training process, developing common sense reasoning capabilities, and improving the model's ability to handle nuances and idiomatic expressions.\", 'By enhancing the contextual understanding of GPT models, their outputs will be more accurate, relatable, sequential, less biased, and more useful for a variety of applications.', '§.§ Ethical Concerns\\n\\nThe ethical concerns in GPT models are an active area of discussion and debate due to the potential negative impacts that the use of GPTs could have on society.', 'Although GPT models have demonstrated remarkable abilities in generating coherent and realistic text, there are concerns about the perpetuation of biases and stereotypes, the possibility of malicious use, and the effects on employment and economic inequality.', 'Some of the ethical characteristics to be possessed by GPT include functional Morality, operational morality, abiding by the right for explanation law, improved transparency with human involvement, unbiased data, and adhering to government regulations on data usage<cit.>.', 'The responsibility of developers and companies to address these ethical concerns and ensure the ethical use of a GPT model is also a topic of debate.', 'The ethical implications of GPT models are being actively researched and discussed in the fields of AI, computer science, and philosophy.', '§.§ Security and privacy concerns\\n\\nGPT models raise concerns about security and privacy, particularly as they become more widespread.', 'One of the main concerns is that GPT could be used for harmful purposes, such as creating fake news or deep fakes, as it can generate text that looks real and convincing, making it difficult to distinguish between genuine and fake content.', 'Another concern is the potential for privacy violations when using a GPT model.', 'Large language models like GPT require a significant amount of training data, which could contain sensitive or personal information.', \"This raises concerns about privacy and data protection as per European Union's General Data Protection Act<cit.>, particularly if the training data is not properly anonymized or if the models are used to generate text based on user data without their explicit consent.\", 'Some of the problems concerned with confidentiality related to the pre-training dataset are Data tracing, Membership Inference Attacks, reconstruction attacks, and property inference attacks and the vulnerabilities concerned with a model encoder are hyperparameter stealing attacks and encoder parameter stealing attacks.', 'Poisoning, Backdoor, and evasion attacks are the vulnerability related to the integrity of self-supervised learning.', 'Resource depletion attack is one major issue with data availability, which may lead to tremendous effects incorrect results, and may cause greater deviations too<cit.>.', \"Additionally, the GPT model's ability to generate text based on user input could inadvertently disclose sensitive information, such as personal or financial details, or trade secrets.\", 'This could happen if a GPT model is used in an insecure environment or if it is targeted by malicious actors seeking to obtain sensitive information.', \"Researchers and developers should focus on assuring authenticity in using users' data in case of interactive information generation based on privacy data shared.\", 'These include using differential privacy to protect training data privacy <cit.>, implementing secure hardware or software protocols to protect models from cyberattacks, and developing techniques to detect and prevent the malicious use of GPT models.', \"It's crucial to adopt and follow these measures to ensure the ethical and safe use of GPT models before using them in various applications.\", 'Therefore, GPT model development must focus on developing more robust, reliable, safest, multi-lingual, multimodal support-enabled solutions for delivering domain-specific or human-specific solutions with optimal resource utilization.', 'Various lessons learned and future research directions.', '!', 'Sl.No    \\n  Lessons Learned    \\n  Open Issues    \\n  Future Directions \\n 1.', '∘ Huge volume of data usage is critical\\n    \\n    \\n  ∘ Data privacy - may unknowingly reveal sensitive information\\n  \\n  ∘ Varied data quality - Inconsistency in quality of data used for training \\n  \\n  ∘ Scalability - Models should be able to handle an increase in data set size and complexity    \\n  ∘ Optimized architecture and algorithms \\n  \\n  ∘ Cloud-based computing\\n  \\n  ∘ Hardware advancements\\n  \\n   \\n 2.', '∘ Importance of Proper Pre-processing of data \\n  \\n     \\n  \\n  ∘ Data bias - Overrepresentation of certain groups or perspectives \\n  \\n  ∘ Poor model performance \\n  \\n  ∘ Reduced efficiency of the model \\n     \\n  ∘ Continuous monitoring\\n  \\n  ∘ Testing model for potential biases\\n  \\n  ∘ Diversifying the training data\\n  \\n  \\n \\n\\n  3.', '∘ Importance of explainability and interpretability\\n    \\n    \\n ∘ Complexity of models\\n  \\n ∘ Inability to explain predictions\\n \\n ∘ Need of User-tailored Explanations generation\\n \\n ∘ Developing Interpretable models\\n \\n  ∘ Lack of transparency in the data source\\n  \\n     \\n\\n  ∘ AI governance models can be used\\n  \\n  ∘ Model Summaries can be provided\\n  \\n  ∘ Techniques like LIME(Local Model-Agnostic Explanations can be used\\n  \\n  ∘ Uncertaining estimates can be obtained from a model\\n  \\n   \\n \\n\\n    4.', '∘ Ethical concerns\\n    \\n    \\n  ∘ Data privacy and data protection\\n  \\n  ∘ Misuse of data\\n\\n  ∘ Accountability  and transparency concerns\\n\\n  ∘ Societal implications - displacing jobs and exacerbating equalities\\n  \\n  \\n     \\n\\n  ∘ Counterfactual analysis can be used\\n  \\n  ∘ Federated learning can be used\\n  \\n  ∘ Ethical guidelines, Legal frameworks and regulations can be developed to avoid harmful use\\n    \\n   \\n \\n\\n   5.', '∘ Lack of contextual understanding in AI systems\\n    \\n    \\n  ∘ Possibility for ambiguous, contradictory, incorrect results leads to misunderstandings\\n  \\n  ∘ Inconsistency in responses or outputs\\n\\n  ∘ Lack of ability in distinguishing true and false information\\n\\n   \\n  \\n     \\n\\n  ∘ Incorporation of knowledge graphs and semantic embeddings into the training process \\n  \\n  ∘ Usage of attention mechanisms to focus on relevant parts of the input\\n  \\n  ∘ Imparting reasoning and inference capabilities\\n\\n  ∘ Task or domain-based fine-tuning\\n    \\n   \\n \\n\\n\\n\\n   6.', '∘ Pre-trained models may not perform well for Domain-specific task\\n    \\n    \\n  ∘ Possibility for ambiguous, contradictory, incorrect results leads to misunderstandings\\n  \\n  ∘ Inconsistency in responses or outputs\\n\\n  ∘ Lack of ability in distinguishing true and false information\\n  \\n     \\n\\n  ∘ Incorporation of knowledge graphs and semantic embeddings into the training process \\n  \\n  ∘ Usage of attention mechanisms to focus on relevant parts of the input\\n  \\n  ∘ Imparting reasoning and inference capabilities\\n\\n  ∘ Task or domain-based fine-tuning\\n    \\n   \\n \\n\\n\\n\\n\\n\\n\\n\\n\\n§ CONCLUSION\\n\\n The impact of GPT and other large language models is far-reaching and profound.', 'As these technologies continue to evolve and improve, they have the potential to transform the way we interact with technology and each other.', 'From personalized recommendations and customer service to language translation and text generation, the possibilities are endless.', 'However, as with any technology, there are potential ethical and societal concerns that must be addressed.', 'As we continue to rely more heavily on these language models, we must ensure that we are using these tools responsibly and with consideration for their impact on society as a whole.', 'These include challenges related to biases in the data used to train the models, safeguarding privacy and security, understanding the implications of human creativity, and the potential impact on employment and job displacement.', 'We need to continue to evaluate and reflect on the impact of GPT and other language models, to ensure that they are being used in a way that benefits society as a whole.', 'By doing so, we can help to ensure that these technologies are used to their fullest potential while minimizing any negative impact that they may have.', 'IEEEtran']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Function to semantically chunk text into paragraphs based on sentence boundaries\n",
        "def semantic_chunking(text, max_chunk_size):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_chunk_size = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_size = len(word_tokenize(sentence))\n",
        "        if current_chunk_size + sentence_size <= max_chunk_size:\n",
        "            current_chunk.append(sentence)\n",
        "            current_chunk_size += sentence_size\n",
        "        else:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "            current_chunk_size = sentence_size\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    input_text = final_data\n",
        "\n",
        "    max_chunk_size = 50  # Define the maximum chunk size in terms of words\n",
        "\n",
        "    chunks = semantic_chunking(input_text, max_chunk_size)\n",
        "\n",
        "    # Write the chunks to a file\n",
        "    output_file_path = \"semantic_chunks.txt\"\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        for chunk in chunks:\n",
        "            output_file.write(chunk + \"\\n\\n\")\n",
        "\n",
        "    print(f\"Semantic chunks have been written to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE3T6ekw0pjg",
        "outputId": "728c055e-e85b-44d2-9bd0-38f8906b805e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic chunks have been written to semantic_chunks.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5358cb1ea2ce477dab4a95e8aee5f0f5",
            "203fd0148b764feea5eac327d6345844",
            "9d107721958d442ba708e961481d85e5",
            "4b8cf5e225ec48378759eda53d635e7c",
            "ad9d9290120842aba0ee9b81203fc568",
            "8e2bea51d39e4d54875d602b0925f80b",
            "044a868d097f4cce8b2405c66ce2b66d",
            "193e3798028f489fbd5185595c9c7171",
            "afeb3a7ac9cf4113a01a0711c9a3b7fe",
            "4187b38b2d6944ecaef29f6a5946e2fa",
            "feeafbc07afd4d6b84fd03a106d1d30f",
            "fadaf8dce403407b8a7525e962bf25d8",
            "4fe50353120044bc8142b8af59de727b",
            "59d5538546844920ae3181487d106062",
            "ca6c547569dd4eabb6ce25a8cfd7a321",
            "c99b1417bb9347089e1fffecca2cbe38",
            "a63d74c6556e484eb1fc72d0708cf43b",
            "ce1bf64aa8ff4e78aa950f89b88d62b8",
            "f1339cf9cb7b4e4d8f6274d5bd001448",
            "3e92d270fdc3494585f1d884952fc41f",
            "99bf50624b254b83a0f20a0cbd0eb59b",
            "2885db0068ca4a0095268224230433b3",
            "6a7df09805054b349f86dfd5ef1162cb",
            "5bf8755dface4f3183ace1c247502f6b",
            "f699de78420c4dd3b3631cf3989ad337",
            "8016cabaa67043439a570e3aec39f821",
            "a450111b914f49328a029e76a9d64239",
            "2bf7c3c62ec745488983560f58f1cc86",
            "99a4f90dcc034e34b7d7548ff2fbd563",
            "b5f6a53717854ce7934efc7fd7b20fa0",
            "c937b246886045688bfa46a2562bdf66",
            "318173c0ca7f4312804c75602a6edf91",
            "26537a77b9a84d40b6225504848d04c9",
            "07a1cc5a5c424b049da3c451b8eece7a",
            "c9388c8db6724847bffb0a68d4bf544b",
            "b13a4a09ecec43d494050695669b39a8",
            "81849ae324a442b2ae85b1ff6e48be69",
            "e618ff4094804f87b8b5a07a0fd59c86",
            "63cd207de39345b8a4c7179d5aadd8d9",
            "6dc551ccedb043eeba683b69f20a3456",
            "4a5ff8310df14d978c419d7dba7ae3e1",
            "8ce4877df7da4adca83cd7cff114a0da",
            "91c2d32ce4164be7855ea19c16307a88",
            "67fecae33dc44a23b6ff5f59ed06595b",
            "c8678b2e38b046359de148ce7e9d4194",
            "56c5c274ad914804ab7899664677d002",
            "1bed7604b70d478aa590d76c4193d47e",
            "33b2af83cf454804a83446f44d487633",
            "1bac9468e6e9425caf714493f22dffd9",
            "d4b28781450e4342b82cfcc4dbfa0496",
            "2feabc4b94f64b2bac81f1a6257a16c1",
            "9e1c7d4fc5b54d2494e439f03531ee36",
            "caba233f75144150b09f6917955b0967",
            "e4ccb45152d647af9b759154a6e468c5",
            "3e149bd0e0634824af5aa92372acf21b",
            "f0ee001569a544698a30de77793f3f69",
            "db235ea6368d4b4db4cc616042aeb0ff",
            "76207d93f3ac4428a8a596c7a67d2fdc",
            "7db176ddec724e17b4c4b6d8d7ee2855",
            "94d4313c431f437983b723bfc9443059",
            "1f8d52368c7f4dcfb8a3e22cf0a565b4",
            "b6e02dd9b2344363823e04e7965386a0",
            "c23b9931652c4ea9aac105e2a9a0c016",
            "21d93f3a57de49a3b2a427929ab7e5ba",
            "e4535b5e11ef4ead8c161b7f74a9fd1c",
            "1cc0e7e8a2eb49d6b576ec5e3b5abfb5",
            "7cfd25e13aff40478ba6c577cd8867f6",
            "d94169739a1048b0992db411af262950",
            "974ced3e6bef4c21afc1fad4bd7ee383",
            "293d0bb866d24e32bd6602534b37821f",
            "1fd010309afc40c185c0cc482a446102",
            "53d607e7c2d84db581839afc232e8559",
            "d49e8b09b55e4c498c9c0bb3ac552dd0",
            "3060c38212fc4456a19628ee4702f92d",
            "3161bb61eb26474f9c8ffdd7612036d9",
            "b7536f970ec14e14a02ad4a4b7886e04",
            "fc6c5f8608f145df9d84148ba95f84b5",
            "973ed2ddcba8499e8cbd0a2493ff56f3",
            "8e3d822b0d5d433e81e7de9ade3b527d",
            "42d5205ad62547eea87b09788f4c15a7",
            "be0984b53d854527bc9789b4ae9fc7a2",
            "8d553dd661e34f2d848064f6d3110ef5",
            "f5cb88ade43149509261f2795c4981aa",
            "927ba920a3e84595a82e7c93ac1a69f7",
            "a5e6d72bc46b4f85a1d634990075b020",
            "29ba540c3c2348abbb0a94e72f43d92f",
            "4af685e6672047018b33d64cdf75feee",
            "031bff1773044d65aaae394906b4bb4c",
            "3bcc817216b647ae8d1d499267622b4f",
            "42964885fce74d138a7b25aa7cde1c38",
            "445dbf8bdcbd4b8fbb52d1af720505ce",
            "cadf332472654b7a95e8a0efd56a83d9",
            "d5cebd76ca6344dda93a51659d5ef40f",
            "137e8157f0f14437a09d589a37fd790a",
            "42ddc6c46dd1486e8838dd4cd29fd4de",
            "ceef743de0e844dcb06836826ada9223",
            "ac41635a6bb6425a864723626e2e3f6f",
            "ed1e6c4d117b4611b54fc50c4cce6671",
            "f07ce5a0375f4d6a90c8828958e96b43",
            "0e1b6416b59449779e89225a28436622",
            "22b176934bb04fc0b8676161b6dd5ee7",
            "f454bd94ae5e494185258fcd5123cba2",
            "158deb3ad6a848178145c59a291a0784",
            "9d55faf6a89545bd8f6b5ec931da1c74",
            "d096b90de81a46629b2954ccda0a1bf1",
            "045ca7f3746547c9afe4a070e4c706c6",
            "e3467237234646329a2b2e6cdef0fbc4",
            "3bfd2e0794ff4e3b9fa89ff6589de2fb",
            "91005ae3f8e940bc8692e4baae09d3b9",
            "37e22431981c4e85bdbc246a127fd619",
            "df499522bc0d406985e1457e70e11867",
            "f7169f4866034f38afbdfbc9301151fc",
            "10ffec29c1fb45458a17a7319f47da84",
            "cbf55cdb9fb64643af331e1982531c37",
            "15057f96999e4ecb9e1644a9ff53b0ac",
            "5664ddaf582d4537af6e98dd69738807",
            "223d687a83b24a12876af784cbc673b8",
            "0f4ce65b42e244519a0a5f6902824431",
            "930538bf8ed04e92bf6bd88dcc19b618",
            "587cbb83a71e4d55875e2ffaf2b7d721",
            "29913ece80714fbc90c87870ca439949"
          ]
        },
        "id": "3VKI6jVJtueg",
        "outputId": "3dfa2cc7-7991-41aa-d11d-ec0540673c0f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5358cb1ea2ce477dab4a95e8aee5f0f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fadaf8dce403407b8a7525e962bf25d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a7df09805054b349f86dfd5ef1162cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07a1cc5a5c424b049da3c451b8eece7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8678b2e38b046359de148ce7e9d4194"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0ee001569a544698a30de77793f3f69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cfd25e13aff40478ba6c577cd8867f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "973ed2ddcba8499e8cbd0a2493ff56f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bcc817216b647ae8d1d499267622b4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e1b6416b59449779e89225a28436622"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df499522bc0d406985e1457e70e11867"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_data = model.encode(chunks)\n",
        "\n",
        "# Print the embedded data\n",
        "print(embedded_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlrkLp_otvDb",
        "outputId": "9eb9aa27-c687-4fe9-b0d0-cba3602c75f7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.03310174  0.04010018 -0.03381789 ... -0.00192831  0.00020426\n",
            "  -0.00235523]\n",
            " [ 0.0374491  -0.07713708  0.0507256  ...  0.09710224 -0.02382364\n",
            "   0.03633789]\n",
            " [ 0.02720055 -0.14546096  0.02690268 ...  0.08816759  0.03293575\n",
            "  -0.03847821]\n",
            " ...\n",
            " [ 0.05884962 -0.03653542 -0.01376635 ...  0.00893309  0.00372487\n",
            "  -0.00231304]\n",
            " [-0.00216637  0.01126758 -0.06289933 ...  0.04957288  0.07059541\n",
            "  -0.01148926]\n",
            " [-0.11681347 -0.04598839 -0.04112325 ... -0.083582   -0.04062299\n",
            "   0.00897704]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b.T) / (np.linalg.norm(a, axis=1)[:, np.newaxis] * np.linalg.norm(b, axis=1))"
      ],
      "metadata": {
        "id": "XKpqusfXvDP1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Assuming `model` is already defined and loaded\n",
        "# Assuming `embedded_data` is your embedded dataset (numpy array of shape (num_sentences, embedding_dim))\n",
        "# Assuming `chunks` is the list of original sentences corresponding to `embedded_data`\n",
        "\n",
        "# Encode the query sentence\n",
        "query_sentence = 'Do Pre-trained models perform well for Domain-specific task?'\n",
        "query_vec = model.encode(query_sentence)\n",
        "\n",
        "# Ensure query_vec is in the correct shape (1, embedding_dim)\n",
        "query_vec = query_vec.reshape(1, -1)\n",
        "\n",
        "# Compute similarities between the query vector and embedded data\n",
        "similarities = cosine_similarity(query_vec, embedded_data)\n",
        "\n",
        "# Get indices of top 4 most similar sentences\n",
        "top_indices = np.argsort(similarities[0])[::-1][:5]\n",
        "\n",
        "# Retrieve the top 4 most similar sentences\n",
        "top_doct = [chunks[index] for index in top_indices]\n",
        "# list_sent= []\n",
        "# # Print the top 4 most similar sentences\n",
        "# for i, doc in enumerate(top_doct):\n",
        "#     top_sentence = f\" {doc}\"\n",
        "#     print(top_sentence)\n",
        "#     list_sent.append(top_sentence)\n",
        "\n",
        "# print(list_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjEmWjXAxrOj",
        "outputId": "bfe49bd1-2af0-4d54-fec7-a59e8029a025"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale.\n",
            " If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models.\n",
            " In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence.\n",
            " Note: in-context learning was needed to give the model an idea of what a prompt looks like.\n",
            " However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks.\n",
            "[' They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale.', ' If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models.', ' In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence.', ' Note: in-context learning was needed to give the model an idea of what a prompt looks like.', ' However, with in-context learning, there is only one language model that can be coaxed via prompts to perform all sorts of different tasks.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as gen_ai\n",
        "\n",
        "# Configure the generative AI with the provided API key\n",
        "gen_ai.configure(api_key=\"AIzaSyBVzWqDERtSki5H5siTRKAJvt1xZNB4Edw\")\n",
        "\n",
        "# Set up the model with generation and safety settings\n",
        "generation_config = {\n",
        "    \"temperature\": 0.5,\n",
        "    \"top_p\": 1,\n",
        "    \"top_k\": 1,\n",
        "    \"max_output_tokens\": 10000,\n",
        "}\n",
        "safety_settings = [\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
        "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Initialize the GenerativeModel with the specified parameters\n",
        "Gemini = gen_ai.GenerativeModel(model_name=\"gemini-1.5-flash\",\n",
        "                                generation_config=generation_config, safety_settings=safety_settings)\n"
      ],
      "metadata": {
        "id": "o5cDAP7g3Im0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def run_block(query):\n",
        "    queries = [query]\n",
        "    query_vec = model.encode(queries)\n",
        "\n",
        "    # Ensure query_vec is in the correct shape (1, embedding_dim)\n",
        "    query_vec = query_vec.reshape(1, -1)\n",
        "\n",
        "    # Compute similarities between the query vector and embedded data\n",
        "    similarities = cosine_similarity(query_vec, embedded_data)\n",
        "\n",
        "    # Get indices of top 5 most similar sentences\n",
        "    top_indices = np.argsort(similarities[0])[::-1][:5]\n",
        "\n",
        "    # Retrieve the top 5 most similar sentences\n",
        "    top_doct = [chunks[index] for index in top_indices]\n",
        "\n",
        "    argumented_prompt = f\"\"\"You are developing a conversational question-answering bot designed to provide informative responses using structured textual and embedded data.\n",
        "Your system should efficiently interpret natural language queries and generate clear, accurate answers.\n",
        "Use existing lecture notes and milestone papers as your primary sources of information.\n",
        "Ensure your responses are both conversational and abstractive, addressing the query comprehensively.\n",
        "Query: {query}. Context: {top_doct[0]}\"\"\"\n",
        "\n",
        "    # Generate content using the Gemini model with the constructed prompt\n",
        "    model_output = Gemini.generate_content(contents=argumented_prompt)\n",
        "    final_response = print(model_output.text)\n",
        "    return final_response\n",
        "\n",
        "# Example usage\n",
        "output = run_block('Do Pre-trained models perform well for Domain-specific task?')\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "Rbppl0mMzGHd",
        "outputId": "6d727821-928d-414f-c142-d414d0318ae2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's a great question!  Pre-trained models, while incredibly powerful, often require some adaptation for optimal performance on domain-specific tasks. \n",
            "\n",
            "Here's the breakdown:\n",
            "\n",
            "**The Good:**\n",
            "\n",
            "* **Strong Foundation:** Pre-trained models have been exposed to vast amounts of text data, giving them a strong foundation in language understanding. This means they can be fine-tuned for specific domains with less data than training from scratch.\n",
            "* **Efficiency:**  Fine-tuning a pre-trained model is often faster and more efficient than training a model from scratch, especially for smaller datasets.\n",
            "* **Distillation:**  Pre-trained models can be distilled into smaller, more computationally efficient models that are easier to deploy and serve at scale. This is crucial for real-world applications where resources are limited.\n",
            "\n",
            "**The Not-So-Good:**\n",
            "\n",
            "* **Domain Specificity:**  While pre-trained models are general purpose, they might not be optimized for the nuances of a specific domain. Fine-tuning is essential to adapt the model to domain-specific language, concepts, and terminology.\n",
            "* **Data Requirements:**  Fine-tuning still requires some domain-specific data to achieve optimal performance. The amount of data needed depends on the complexity of the task and the similarity between the pre-trained model's training data and the target domain.\n",
            "* **Bias:**  Pre-trained models can inherit biases from their training data, which might not be representative of the target domain.  Careful consideration of bias mitigation strategies is crucial.\n",
            "\n",
            "**In Conclusion:**\n",
            "\n",
            "Pre-trained models are a valuable tool for domain-specific tasks, but they require careful consideration and adaptation.  Fine-tuning, distillation, and bias mitigation are key factors to ensure optimal performance.  The success of using pre-trained models for domain-specific tasks depends on the specific domain, the available data, and the chosen fine-tuning techniques. \n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_block('Do Pre-trained models perform well for Domain-specific task?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "Wi6LF-vR3xWG",
        "outputId": "e02a8499-aa46-4a5e-f4d9-e350afb394d7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That's a great question! You're right, pre-trained models can be incredibly effective for domain-specific tasks, even surpassing traditional approaches in many cases. \n",
            "\n",
            "Here's why:\n",
            "\n",
            "* **Transfer Learning:** Pre-trained models have already learned general language representations from massive datasets. This allows them to quickly adapt to specific domains by fine-tuning on smaller, domain-specific datasets. This is a huge advantage over training models from scratch, which requires significantly more data and computational resources.\n",
            "* **Fine-tuning:**  Fine-tuning involves adjusting the pre-trained model's parameters to optimize its performance for the target domain. This process leverages the model's existing knowledge and allows it to specialize in the specific language patterns and concepts relevant to the domain.\n",
            "* **Distillation:** As you mentioned, pre-trained models can be distilled into smaller, more efficient models. This is crucial for deployment in resource-constrained environments or for serving at scale. Distillation techniques preserve the knowledge of the larger model while reducing its size and computational requirements.\n",
            "\n",
            "**However, it's important to note that there are some challenges:**\n",
            "\n",
            "* **Data Availability:** While fine-tuning is effective, it still requires domain-specific data. If sufficient data is not available, the model's performance may be limited.\n",
            "* **Domain Specificity:** Some domains may have unique language patterns or concepts that are not well captured by general pre-trained models. In such cases, specialized pre-training or domain-specific augmentation techniques may be necessary.\n",
            "\n",
            "**Overall, pre-trained models are a powerful tool for domain-specific tasks. Their ability to transfer knowledge and adapt quickly makes them highly efficient and effective. However, careful consideration should be given to data availability and the specific requirements of the domain.** \n",
            "\n",
            "Let me know if you have any other questions! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  run_block(input(\"Enter a query: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58tU_Etd3zLH",
        "outputId": "a33fa9f9-251e-44b4-d5d3-738f7094beb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Transformer's encoder is a crucial component of the Transformer architecture, which revolutionized natural language processing (NLP) in 2017. It's responsible for processing the input sequence and extracting meaningful representations, which are then used by the decoder to generate the output. \n",
            "\n",
            "Let's break down how the encoder works:\n",
            "\n",
            "1. **Input Embedding:** The input sequence is first embedded into a vector representation. This embedding captures the semantic meaning of each word in the sequence.\n",
            "\n",
            "2. **Positional Encoding:** Since the Transformer doesn't rely on recurrent connections, it needs to explicitly encode the positional information of each word in the sequence. This is done by adding positional encodings to the embedded vectors.\n",
            "\n",
            "3. **Multi-Head Attention:** The heart of the encoder lies in the multi-head attention mechanism. This allows the model to attend to different parts of the input sequence simultaneously, capturing complex relationships between words. It essentially allows the model to \"look\" at different parts of the sequence and understand how they relate to each other.\n",
            "\n",
            "4. **Feed-Forward Network:** After the multi-head attention layer, the encoder passes the information through a feed-forward neural network. This network further processes the information and learns non-linear relationships between the words.\n",
            "\n",
            "5. **Residual Connections:** To prevent vanishing gradients, the encoder uses residual connections. This allows information to flow directly from the input to the output, improving the model's ability to learn long-term dependencies.\n",
            "\n",
            "6. **Layer Normalization:** Layer normalization is applied after each sub-layer (multi-head attention and feed-forward network) to stabilize the training process and improve performance.\n",
            "\n",
            "In summary, the Transformer's encoder acts as a powerful feature extractor, capturing complex relationships and dependencies within the input sequence. This information is then passed to the decoder, which uses it to generate the final output. \n",
            "\n",
            "The encoder's ability to process sequences in parallel and its use of multi-head attention have made it a cornerstone of many successful NLP models, including the one you mentioned, which was published in 2019. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XKlVjCKt-L-u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}